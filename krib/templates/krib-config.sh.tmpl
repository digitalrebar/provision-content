#!/usr/bin/env bash
# Kubernetes Rebar Integrated Boot (KRIB) Kubeadm Installer
set -e

# Get access and who we are.
{{ template "setup.tmpl" .}}

{{ template "download-tools.tmpl" .}}

{{ template "krib-runtime-proxy.tmpl" .}}

{{ if .ParamExists "krib/cluster-profile" -}}
CLUSTER_PROFILE={{ .Param "krib/cluster-profile" }}
PROFILE_TOKEN={{ .GenerateProfileToken (.Param "krib/cluster-profile") 7200 }}
{{ else -}}
xiterr 1 "Missing krib/cluster-profile on the machine!"
{{ end -}}

{{ template "krib-lib.sh.tmpl" .}}

set_taint() {
  {{ if eq (.Param "krib/cluster-masters-untainted") true -}}
  echo "Removing taint from master nodes:"
  kubectl taint nodes -l node-role.kubernetes.io/master node-role.kubernetes.io/master- || true
  {{ else -}}
  echo "Ensure taint on master nodes:"
  kubectl taint nodes -l node-role.kubernetes.io/master node-role.kubernetes.io/master=:NoSchedule || true
  {{ end -}}
}

echo "Configure kubeadm master and minions..."

CLUSTER_NAME={{ .Param "krib/cluster-name" }}

echo "MAKE SURE SWAP IS OFF!- kubeadm requirement"
if free | grep -q Swap ; then
  swapoff -a
  if [ -f /etc/fstab ]; then
    sed -i /swap/d /etc/fstab
  fi
fi

echo "MAKE SURE kernel module br_netfilter is loaded - kubeadm requirement"
if ! lsmod | grep ^br_netfilter > /dev/null ; then
  modprobe br_netfilter
  echo "br_netfilter" > /etc/modules-load.d/br_netfilter.conf
fi

echo "MAKE SURE bridge-nf-call-iptables CONTAINS 1 - kubeadm requirement"
if [ ! -f /etc/sysctl.d/99-net.bridge.bridge-nf-call-iptables ]; then
  cat <<EOF >/etc/sysctl.d/99-net.bridge.bridge-nf-call-iptables
net.bridge.bridge-nf-call-iptables=1
EOF

  sysctl -p /etc/sysctl.d/99-net.bridge.bridge-nf-call-iptables
fi

VALUE=$(sysctl -n net.bridge.bridge-nf-call-iptables)
[ $VALUE != 1 ] && sysctl -w net.bridge.bridge-nf-call-iptables=1

mkdir -p /etc/kubernetes/manifests

{{ if .ParamExists "krib/ip" -}}
KRIB_IP={{ .Param "krib/ip" }}
{{ else -}}
KRIB_IP={{ .Machine.Address }}
{{ end -}}

# we need a random backoff to avoid races.
SLEEP=$[ ( $RANDOM % 20 ) ]
sleep $SLEEP

# Making sure I use the right cluster InternalIP
echo "KUBELET_EXTRA_ARGS=--node-ip=${KRIB_IP}" > /etc/default/kubelet

MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
echo "My Master index is $MASTER_INDEX"
if [[ $MASTER_INDEX != notme ]] ; then

  echo "I am master - run kubeadm"
  export KUBECONFIG=/etc/kubernetes/admin.conf

  # todo: why does this have a default value?
  {{ if .ParamExists "krib/cluster-bootstrap-token" -}}{{ if ne (.Param "krib/cluster-bootstrap-token") "fedcba.fedcba9876543210" -}}
  echo "looks like master recovery on existing cluster, just run kubeadm"
  if [[ $MASTER_INDEX == 0 ]] ; then
    MASTER_INDEX={{.Param "etcd/server-count"}}
  fi
  {{ end -}}{{ end -}}

  if [[ $MASTER_INDEX == 0 ]] ; then

    echo "Starting Master kubeadm init process."

    # generate a cluster bootstrap token, then store it on the Param
    # for the kubeadm.cfg file
    TOKEN=$(kubeadm token generate)
    drpcli -T $PROFILE_TOKEN profiles add $CLUSTER_PROFILE param $KRIB_BOOTSTRAP_TOKEN to "$TOKEN"

    {{ if .ParamExists "krib/kubeadm-cfg" -}}
    echo "Using specified 'kubeadm.cfg' instead of DRP template (krib-kubeadm.cfg.sh.tmpl)."
    KC={{ .Param "krib/kubeadm-cfg" }}
    echo "($KC)"
    echo "# externally defined template specified by 'krib/kubeadm-cfg'" > /tmp/kubeadm.cfg
    echo "# source: '$KC'" >> /tmp/kubeadm.cfg
    T=$(mktemp /tmp/kube.cfg.XXXXXXXXX)
    download -gfsSL -o $T $KC
    cat $T >> /tmp/kubeadm.cfg && rm -f $T
    {{ end -}}

    kubeadm init --config=/tmp/kubeadm.cfg --ignore-preflight-errors {{ .Param "krib/ignore-preflight-errors" }} 2>&1 | tee -a /tmp/kubeadm.init.log

    echo "Building certificate file."
    tar -zcvf /tmp/cert.tgz \
      /etc/kubernetes/pki/ca.crt \
      /etc/kubernetes/pki/ca.key \
      /etc/kubernetes/pki/sa.key \
      /etc/kubernetes/pki/sa.pub \
      /etc/kubernetes/pki/front-proxy-ca.*

    base64 -w 0 /tmp/cert.tgz > /tmp/cert.b64
    drpcli -T $PROFILE_TOKEN profiles add $CLUSTER_PROFILE param $KRIB_MASTER_CERTS_PARAM to /tmp/cert.b64
    rm /tmp/cert.tgz /tmp/cert.b64

    JOIN_CMD=$(kubeadm token create --print-join-command)

    NET_TYPE={{ .Param "krib/networking-provider" }}

    # example of appending a featureGate to cluster after initial init
    #kubeadm init --feature-gates=CoreDNS=true | tee -a ~/kubeadm_init.log

# Apply PSPs (if the user hasn't enabled the PSP admission controller, these will have no effect,
# but if he/she _has_, then the PSPs are required in order to proceed with CNI)
kubectl create -f /tmp/krib-default-psps.yaml

{{ if .ParamExists "krib/sign-kubelet-server-certs" -}}{{ if eq (.Param "krib/sign-kubelet-server-certs") true -}}
    # We are using serverTLSBootstrap for kublets, so enable auto-approval of kubelet server CSRs

  {{ if .ParamExists "krib/kubelet-rubber-stamp-container-image" }}
  # Replace default container image with the one specified in "krib/nginx-ingress-controller-container-image"
  sed -i 's|quay.io/kontena/kubelet-rubber-stamp-amd64|{{ .Param "krib/kubelet-rubber-stamp-container-image" }}|' /tmp/krib-kubelet-rubber-stamp.yaml
  {{ end -}}

    kubectl create -f /tmp/krib-kubelet-rubber-stamp.yaml
    rm /tmp/krib-kubelet-rubber-stamp.yaml
{{ end -}}{{ end -}}

    case $NET_TYPE in
      calico)
        echo "Starting calico networking..."
        # Disable NetworkManager interference per https://docs.projectcalico.org/v3.8/maintenance/troubleshooting#configure-networkmanager
        if [ -d "/etc/NetworkManager/conf.d" ]; then
          echo -e "[keyfile]\nunmanaged-devices=interface-name:cali*;interface-name:tunl*" > /etc/NetworkManager/conf.d/calico.conf
        fi

        # It is assumed here that the operator has pre-prepared their calico config
        download -gfsSL {{ .Param "provider/calico-config" }} > /tmp/calico.yaml
        kubectl apply -f /tmp/calico.yaml
        mkdir -p /tmp/cleanup
        mv /tmp/calico.yaml /tmp/cleanup/

        ;;
      flannel)
        echo "Starting flannel networking..."
        download -gfsSL {{ .Param "provider/flannel-config" }} | kubectl apply -f -
        ;;
      cilium)
        echo "Starting cilium networking..."
        # download -gfsSL {{ .Param "provider/flannel-config" }} | kubectl apply -f -
        curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
        helm repo add cilium https://funkypenguin.github.io/helm-cilium/
        helm repo update
        #kubectl create -f https://raw.githubusercontent.com/cilium/cilium/1.9.4/install/kubernetes/quick-install.yaml
        helm install cilium cilium/cilium --version 1.9.5 \
          --namespace kube-system \
          --set hubble.listenAddress=":4244" \
          --set hubble.relay.enabled=true \
          --set hubble.ui.enabled=true \
          --set kubeProxyReplacement=disabled \
          --set image.repository=gergorg/cilium \
          --set extraArgsList="{api-rate-limit=endpoint-create:rate-limit:2000/s\,rate-burst:5000\,parallel-requests:5000\,auto-adjust:false\,max-wait-duration:3000s,api-rate-limit=endpoint-delete=parallel-requests:5000\,auto-adjust:false}" \
          --set updateStrategy.rollingUpdate.maxUnavailable=1 \
          --set resourceQuotas.enabled=true \
          --set prometheus.enabled=true \
          --set prometheus.port=19090 \
          --set rollOutCiliumPods=true \
          --set operator.rollOutPods=true \
          --set operator.prometheus.enabled=true \
          --set podDisruptionBudget.maxUnavailable=1 \
          --set resources.requests.cpu=1 \
          --set resources.requests.memory=128Mi \
          --set containerRuntime.integration=containerd \
          --set containerRuntime.socketPath=/run/containerd/containerd.sock \
          --set hubble.metrics.enabled="{dns,drop,tcp,flow,icmp,http}" \
          --set etcd.enabled=true \
          --set etcd.managed=true \
          --set k8s.requireIPv4PodCIDR=true \
          --set ipam.mode=kubernetes
          # we need kubeproxyreplacement disabeld for istio support

          # --set hostServices.enabled=false \
          # --set k8sServiceHost={{ .Param "krib/cluster-master-vip" }} \
          # --set k8sServicePort={{ .Param "krib/cluster-api-port" }}         
        ;;        
      weave)
        echo "Starting weave networking..."
        kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
        ;;
    esac

  else
    echo "waiting for $KRIB_MASTER_CERTS_PARAM"
    wait_for_variable $KRIB_MASTER_CERTS_PARAM

    cd /
    get_param $KRIB_MASTER_CERTS_PARAM | jq -r . | base64 -d - | tar -zxvf -
    cd -

    kubeadm init --config=/tmp/kubeadm.cfg --ignore-preflight-errors {{ .Param "krib/ignore-preflight-errors" }}  --v=5 2>&1 > /tmp/kubeadm.init.log

  fi

  MAX_WAIT_SECONDS=300
  INCREMENT_SECONDS=10
  N=0
  READY=false

  while [ "$READY" != "true" ]; do
    READY=true

    if [ $N -gt ${MAX_WAIT_SECONDS} ]; then
      echo "All masters did not come up!"
      echo "DEBUG: review /tmp/kubeadm.init.log:"
      xiterr 1 "$(cat /tmp/kubeadm.init.log)"
    else
      N=$(expr $N + ${INCREMENT_SECONDS})
    fi

    OLD_IFS=$IFS
    IFS=" " ; while read name ; do
    # desensitize get nodes to case, if hostname has mix or upper case
    if kubectl get nodes | tr '[:upper:]' '[:lower:]' | egrep -qi "^$name[ \t]*ready"; then
      echo "Node $name is ready."
      HOSTNAME=$name
    else
      echo "Node $name is not yet ready."
      echo "$(kubectl get nodes)"
      READY=false
    fi
  done <<< $(get_param $KRIB_MASTERS_PARAM | jq -r '.[].Name' )
    IFS=$OLD_IFS

    if [ "$READY" != "true" ]; then
      sleep ${INCREMENT_SECONDS}s
    fi
  done

  if [[ $MASTER_INDEX == 0 ]] ; then
    set_taint

    echo "Recording 'kubeadm' bootstrap config ..."
    drpcli -T $PROFILE_TOKEN profiles add $CLUSTER_PROFILE param $KRIB_CLUSTER_KUBEADM_CFG_PARAM to - < /tmp/kubeadm.cfg

    echo "Recording cluster admin config ..."
    drpcli -T $PROFILE_TOKEN profiles add $CLUSTER_PROFILE param $KRIB_ADMIN_CONF_PARAM to - < /etc/kubernetes/admin.conf

    echo "Recording cluster join script ..."
    drpcli -T $PROFILE_TOKEN profiles add $CLUSTER_PROFILE param $KRIB_JOIN_PARAM to "$JOIN_CMD"

    drpcli machines tasks add {{ .Machine.UUID }} at 0 krib-settings krib-dashboard | jq .Tasks

    drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"purple\", \"icon\": \"anchor\" }}" | jq .Meta
  else
    {{ if .ParamExists "krib/cluster-bootstrap-token" -}}{{ if ne (.Param "krib/cluster-bootstrap-token") "fedcba.fedcba9876543210" -}}
    echo "remove taint during master recovery"
    set_taint
    {{ end -}}{{ end -}}
    drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"green\", \"icon\": \"anchor\" }}" | jq .Meta
  fi

else

  echo "I am a worker - run kubeadm join when defined"

  JC=$(wait_for_variable $KRIB_JOIN_PARAM)

  echo "Running: $JC"
  $JC

  # Set machine icon and color for KRIB cluster building
  drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"green\", \"icon\": \"ship\" }}" | jq .Meta

fi

while [ ! -f /etc/kubernetes/kubelet.conf ] ;
do
  sleep 2
done

# Wait for KRIB config to be updated from bootstrap node
echo "waiting for $KRIB_ADMIN_CONF_PARAM"
wait_for_variable $KRIB_ADMIN_CONF_PARAM > label.conf

# Set labels for nodes / runs on all nodes
drpcli -T $PROFILE_TOKEN profiles get $CLUSTER_PROFILE param $KRIB_ADMIN_CONF_PARAM > label.conf
# Re-set HOSTNAME to kubernetes node name
HOSTNAME=$(kubectl --kubeconfig=label.conf get nodes | cut -d" " -f1 | grep -i $(hostname -s))
echo "Adding env={{.Param "krib/label-env"}} label to machine $HOSTNAME"
kubectl --kubeconfig=label.conf label nodes $HOSTNAME env={{.Param "krib/label-env"}} --overwrite=true
# using adhoc labels
{{if .ParamExists "krib/labels" -}}
  {{range $key, $value := .Param "krib/labels" -}}
    echo "Adding {{$key}}=\"{{$value | toString | replace " " "_" -}}\" label to machine $HOSTNAME from krib/labels"
    kubectl --kubeconfig=label.conf label nodes $HOSTNAME {{$key}}="{{$value | toString | replace " " "_" -}}" --overwrite=true || true
  {{end -}}
{{else -}}
  echo "use of krib/labels to create adhoc labels"
{{end -}}
# using inventory
{{if .ParamExists "inventory/data" -}}
  {{range $key, $value := .Param "inventory/data" -}}
    echo "Adding {{$key}}=\"{{$value | toString | replace " " "_" -}}\" label to machine $HOSTNAME from inventory/data"
    kubectl --kubeconfig=label.conf label nodes $HOSTNAME {{$key}}="{{$value | toString | replace " " "_" -}}" --overwrite=true || true
  {{end }}
{{else -}}
  echo "use of inventory/data to create machine specific labels"
{{end -}}
kubectl --kubeconfig=label.conf get node $HOSTNAME --show-labels
rm label.conf

# Fix kubelet to use the vip/lb
{{ if .ParamExists "krib/cluster-master-vip" -}}
MASTER_VIP={{ .Param "krib/cluster-master-vip" }}
MASTER_COUNT={{ .Param "krib/cluster-master-count" }}
API_PORT={{ .Param "krib/cluster-api-port" }}
if [[ $MASTER_COUNT -gt 1 ]] ; then
  API_PORT={{ .Param "krib/cluster-api-vip-port" }}
fi
sed -i "s#server:.*#server: https://${MASTER_VIP}:${API_PORT}#g" /etc/kubernetes/kubelet.conf
systemctl restart kubelet
{{ else -}}
xiterr 1 "Missing required krib/cluster-master-vip"
{{ end -}}

# Clean up
rm -f /tmp/kubeadm.cfg

echo "Finished successfully"
exit 0
