meta:
  Author: RackN and Digital Rebar Community
  CodeSource: https://github.com/digitalrebar/provision-content
  Color: black
  Copyright: RackN
  Description: Kubernetes Rebar Integrated Bootstrap (KRIB)
  DisplayName: Kubernetes (KRIB)
  DocUrl: https://provision.readthedocs.io/en/latest/doc/content-packages/krib.html
  Documentation: ".. _component_krib:\n\nKRIB (Kubernetes Rebar Integrated Bootstrapping)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nLicense:
    KRIB is APLv2\n\nThis document provides information on how to use the Digital
    Rebar *KRIB* content add-on.  Use of this content will enable the operator to
    install Kubernetes in either a Live Boot (immutable infrastructure pattern) mode,
    or via installed to local hard disk OS mode.\n\nKRIB uses the `kubeadm <https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/>`_
    cluster deployment methodology coupled with Digital Rebar enhancements to help
    proctor the cluster Master election and secrets management.  With this content
    pack, you can install Kubernetes in a zero-touch manner.\n\nKRIB does also support
    production, highly available (HA) deployments, with multiple masters.  To enable
    this configuration, we've chosen to manage the TLS certificates and etcd installation
    in the Workflow instead of using the kubeadm process.\n\nThis document assumes
    you have your Digital Rebar Provisioning endpoint fully configured, tested, and
    working.  We assume that you are able to properly provision Machines in your environment
    as a base level requirement for use of the KRIB content add-on use.  See :ref:`_rs_krib`
    for step by step instructions.\n\n.. note:: documentation source: https://github.com/digitalrebar/provision-content/blob/master/krib/._Documentation.meta\n\nKRIB
    Video References\n---------------------\n\nThe following videos have been produced
    or presented by RackN related to the Digital Rebar KRIB solution.\n\n* `KRIB Zero
    Config Kubernetes Cluster channel <https://www.youtube.com/watch?v=SYOHI8DfRMo&list=PLXPBeIrpXjfhKqmTvxI5-0CmgUh82dztr&index=1>`_
    on YouTube.\n* `KubeCon: Zero Configuration Pattern on Bare Metal <https://youtu.be/Psm9aOWzfWk>`_
    on YouTube - RackN presentation at 2017 KubeCon/Cloud NativeCon in Austin TX\n\nOnline
    Requirements\n-------------------\n\nKRIB uses community `kubeadm` for installation.
    \ That process relies on internet connectivity to download containers and other
    components.\n\nImmutable -vs- Local Install Mode\n---------------------------------\n\nThe
    two primary deployment patterns that the Digital Rebar KRIB content pack supports
    are:\n\n#. Live Boot (immutable infrastructure pattern - references [#]_ [#]_)\n#.
    Local Install (standard install-to-disk pattern)\n\nThe *Live Boot* mode uses
    an in-memory Linux image based on the Digital Rebar Sledgehammer (CentOS based)
    image.  After each reboot of the Machine, the node is reloaded with the in-memory
    live boot image.  This enforces the concept of *immutable infrastructure* - every
    time a node is booted, deployed, or needs updating, simply reload the latest Live
    Boot image with appropriate fixes, patches, enhancements, etc.\n\nThe Local Install
    mode mimics the traditional \"install-to-my-disk\" method that most people are
    familiar with.\n\nKRIB Basics\n-----------\n\nKRIB is a Content Pack addition
    to Digital Rebar Provision.  It uses the :ref:`rs_cluster_pattern` which provides
    atomic guarantees.  This allows for Kubernetes master(s) to be dynamically elected,
    forcing all other nodes to wait until the kubeadm on the elected master to generate
    an installation token for the rest of the nodes.  Once the Kubernetes master is
    bootstrapped, the Digital Rebar system facilitates the security token hand-off
    to rest of the cluster so they can join without any operator intervention.\n\nElected
    -vs- Specified Master\n-----------------------------\n\nBy default, the KRIB process
    will dynamically elect a Master for the Kubernetes cluster.  This masters simply
    win the *race-to-master* election process and the rest of the cluster will coalesce
    around the elected master.\n\nIf you wish to specify a specific machines to be
    the designated masters, you can do so by setting a *Param* in the cluster *Profile*
    to the specific *Machine* that will be come the master.  To do so, set the ``krib/cluster-masters``
    \ *Param* to a JSON structure with the Name, UUID and IP of the machines to become
    masters.  You may add this *Param* to the *Profile* in the below specifications,
    as follows:\n\n  ::\n\n    # JSON reference to add to the Profile Params section\n
    \   \"krib/cluster-masters\": [{\"Name\":\"<NAME>\", \"Uuid\":\"<UUID>\", \"Address\":
    \"<ADDRESS>\"}]\n\n    # or drpcli command line option\n    drpcli profiles set
    my-k8s-cluster param krib/cluster-master to <JSON>\n\nThe Kubernetes Master will
    be built on this Machine specified by the *<UUID>* value.\n\n.. note:: This *MUST*
    be in the cluster profile because all machines in the cluster must be able to
    see this parameter.\n\nInstall KRIB\n------------\n\nKRIB is a Content Pack and
    is installed in the standard method as any other Contents.   We need the ``krib.json``
    content pack to fully support KRIB and install the helper utility contents for
    stage changes.\n\nPlease review :ref:`_rs_krib` for step by step instructions.\n\nCLI
    Install\n===========\n\nKRIB uses the Certs plugin to build TLS, you can install
    that from the RackN library\n\n  ::\n    # install required content and create
    the certs plugin\n    drpcli plugin_providers upload certs from catalog:certs-tip\n\n
    \   # verify it worked - should return true\n    drpcli plugins show certs | jq
    .Available\n\nUsing the Command Line (`drpcli`) utility configured to your endpoint,
    use this process:\n\n  ::\n\n    # Get code\n    drpcli contents upload catalog:krib-tip\n\nUX
    Install\n==========\n\nIn the UX, follow this process:\n\n#. Open your DRP Endpoint:
    (eg. https://127.0.0.1:8092/ )\n#. Authenticate to your Endpoint\n#. Login with
    your ```RackN Portal Login``` account (upper right)\n#. Go to the left panel \"Content
    Packages\" menu\n#. Select ``Kubernetes (KRIB: Kubernetes Rebar Immutable Bootstrapping)``
    from the right side panel (you may need to select *Browser for more Content* or
    use the *Catalog* button)\n#. Select the *Transfer* button for both content packs
    to add the content to your local Digital Rebar endpoint\n\n\nConfiguring KRIB\n----------------\n\nThe
    basic outline for configuring KRIB follows the below steps:\n\n#. create a *Profile*
    to hold the *Params* for the KRIB configuration (you can also clone the ``krib-example``
    profile)\n#. add a *Param* of name ``krib/cluster-profile`` to the *Profile* you
    created\n#. add a *Param* of name ``etcd/cluster-profile`` to the *Profile* you
    created\n#. apply the Profile to the Machines you are going to add to the KRIB
    cluster\n#. change the Workflow on the Machines to ``krib-live-cluster`` for memory
    booting or ``krib-install-cluster`` to install to Centos.  You may clone these
    reference workflows to build custom actions.\n#. installation will start as soon
    as the Workflow has been set.\n\nThere are many configuration options available,
    review the ``krib/*`` and ``etcd/*`` parameters to learn more.\n\nConfigure with
    the CLI\n======================\n\nThe configuration of the Cluster includes several
    reference *Workflow* that can be used for installation.  Depending on which Workflow
    you use, will determine if the cluster is built via install-to-local-disk or via
    an immutable pattern (live boot in-memory boot process).   Outside of the Workflow
    differences, all remaining configuration elements are the same.\n\nYou must writeable
    create a *Profile* from YAML (or JSON if you prefer) with the Params stagemap
    and param required information. Modify the *Name* or other fields as appropriate
    - be sure you rename all subsequent fields appropriately.\n\n  ::\n\n    echo
    '\n    ---\n    Name: \"my-k8s-cluster\"\n    Description: \"Kubernetes install-to-local-disk\"\n
    \   Params:\n      krib/cluster-profile: \"my-k8s-cluster\"\n      etcd/cluster-profile:
    \"my-k8s-cluster\"\n    Meta:\n      color: \"purple\"\n      icon: \"ship\"\n
    \     title: \"My Installed Kubernetes Cluster\"\n      render: \"krib\"\n      reset-keeps\":
    \"krib/cluster-profile,etcd/cluster-profile\"\n    ' > /tmp/krib-config.yaml\n\n
    \   drpcli profiles create - < /tmp/krib-config.yaml\n\n.. note:: The following
    commands should be applied to all of the Machines you wish to enroll in your KRIB
    cluster.  Each Machine needs to be referenced by the Digital Rebar Machine UUID.
    \ This example shows how to collect the UUIDs, then you will need to assign them
    to the ``UUIDS`` variable.  We re-use this variable throughout the below documentation
    within the shell function named *my_machines*.  We also show the correct ``drpcli``
    command that should be run for you by the helper function, for your reference.\n\nCreate
    our helper shell function *my_machines*\n  ::\n\n    function my_machines() {
    for U in $UUIDS; do set -x; drpcli machines $1 $U $2; set +x; done; }\n\nList
    your Machines to determine which to apply the Profile to\n  ::\n\n    drpcli machines
    list | jq -r '.[] | \"\\(.Name) : \\(.Uuid)\"'\n\nIF YOU WANT to make ALL Machines
    in your endpoint use KRIB, do:\n  ::\n\n    export UUIDS=`drpcli machines list
    | jq -r '.[].Uuid'`\n\nOtherwise - individually add them to the *UUIDS* variable,
    like:\n  ::\n\n    export UUIDS=\"UUID_1 UUID_2 ... UUID_n\"\n\nAdd the Profile
    to your machines that will be enrolled in the cluster\n\n  ::\n\n    my_machines
    addprofile my-k8s-cluster\n\n    # runs example command:\n    # drpcli machines
    addprofile <UUID> my-k8s-cluster\n\nChange stage on the Machines to initiate the
    Workflow transition.  YOU MUST select the correct stage, dependent on your install
    type (Immutable/Live Boot mode or install-to-local-disk mode).  For Live Boot
    mode, select the stage ``ssh-access`` and for the install-to-local-disk mode select
    the stage ``centos-7-install``.\n\n  ::\n\n    # for Live Boot/Immutable Kubernetes
    mode\n    my_machines workflow krib-live-cluster\n\n    # for intall-to-local-disk
    mode:\n    my_machines workflow krib-install-cluster\n\n    # runs example command:\n
    \   # drpcli machines workflow <UUID> krib-live-cluster\n    # or\n    # drpcli
    machines workflow <UUID> krib-install-cluster\n\nConfigure with the UX\n=====================\n\nThe
    below example outlines the process for the UX.\n\nRackN assumes the use of CentOS
    7 BootEnv during this process.  However, it should theoretically work on most
    of the BootEnvs.  We have not tested it, and your mileage will absolutely vary...\n\n1.
    create a *Profile* for the Kubernetes Cluster (e.g. ``my-k8s-cluster``) or clone
    the ``krib-example`` profile.\n2. add a *Param* to that *Profile*: ``krib/cluster-profile``
    = ``my-k8s-cluster``\n2. add a *Param* to that *Profile*: ``etcd/cluster-profile``
    = ``my-k8s-cluster``\n3. Add the *Profile* (eg ``my-k8s-cluster``) to all the
    machines you want in the cluster.\n4. Change workflow on all the machines to ``krib-install-cluster``
    for install-to-local-disk, or to ``krib-live-cluster`` for the Live Boot/Immutable
    Kubernetes mode\n\nThen wait for them to complete.  You can watch the Stage transitions
    via the Bulk Actions panel (which requires RackN Portal authentication to view).\n\n..
    note:: The reason the *Immutable Kubernetes/Live Boot* mode does not need a reboot
    is because they are already running *Sledgehammer* and will start installing upon
    the stage change.\n\nOperating KRIB\n--------------\n\nWho is my Master?\n=================\n\nIf
    you have not specified who the Kubernetes Master should be; and the master was
    chosen by election - you will need to determine which Machine is the cluster Master.\n
    \ ::\n\n    # returns the Kubernetes cluster Machine UUID\n    drpcli profiles
    show my-k8s-cluster | jq -r '.Params.\"krib/cluster-masters\"'\n\nUse ``kubectl``
    - on Master\n===========================\n\nYou can log in to the Master node
    as identified above, and execute ``kubectl`` commands as follows:\n  ::\n\n      export
    KUBECONFIG=/etc/kubernetes/admin.conf\n      kubectl get nodes\n\n\nUse ``kubectl``
    - from anywhere\n===============================\n\nOnce the Kubernetes cluster
    build has been completed, you may use the ``kubectl`` command to both verify and
    manage the cluster.  You will need to download the *conf* file with the appropriate
    tokens and information to connect to and authenticate your ``kubectl`` connections.
    Below is an example of doing this:\n  ::\n\n    # get the Admin configuration
    and tokens\n    drpcli profiles get my-k8s-cluster param krib/cluster-admin-conf
    > admin.conf\n\n    # set our KUBECONFIG variable and get nodes information\n
    \   export KUBECONFIG=`pwd`/admin.conf\n    kubectl get nodes\n\nAdvanced Stages
    - Helm and Sonobuoy\n===================================\n\nKRIB includes stages
    for advanced Kubernetes operating support.\n\nThe reference workflows already
    install Helm using the `krib-helm` stage.  To leverage this utility simply define
    the required JSON syntax for your charts as shown in :ref:`krib_helm`.\n\nSonobuoy
    can be used to validate that the cluster conforms to community specification.
    \ Adding the `krib-sonobuoy` stage will start a test run.  It can be rerun to
    collect the results or configured to wait for them.  Storing test results in the
    `files` path requires setting the `unsafe/password` parameter and is undesirable
    for production clusters.\n\n\nIngress/Egress Traffic, Dashboard Access, Istio\n===============================================\n\nThe
    Kubernetes dashboard is enabled within a default KRIB built cluster.  However
    no Ingress traffic rules are set up.  As such, you must access services from external
    connections by making changes to Kubernetes, or via the :ref:`rs_k8s_proxy`.\n\nThese
    are all issues relating to managing, operating, and running a Kubernetes cluster,
    and not restrictions that are imposed by Digital Rebar Provision.  Please see
    the appropriate Kubernetes documentation on questions regarding operating, running,
    and administering Kubernetes (https://kubernetes.io/docs/home/).\n\nFor Istio
    via Helm, please consult :ref:`krib_helm` for a reference install\n\n.. _rs_k8s_proxy:\n\nKubernetes
    Dashboard via Proxy\n==============================\n\nYou can get the admin-user
    security token with the following command:\n  ::\n  \n    kubectl -n kube-system
    describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print
    $1}')\n    \nNow copy the token from the token part printed on-screen so you can
    paste it into the ``Enter token`` field of the dashboard log in screen.    \n\nOnce
    you have obtained the ``admin.conf`` configuration file and security tokens, you
    may use ``kubectl`` in Proxy mode to the Master.  Simply open a separate terminal/console
    session to dedicate to the Proxy connection, and do:\n  ::\n\n    kubectl proxy\n\nNow,
    in a local web browser (on the same machine you executed the Proxy command) open
    the following URL:\n\n    http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/\n\nMetalLB
    Load Balancer\n==============================\n\nIf your cluster is running on
    bare metal you will most likely need a LoadBalancer provider.  You can easily
    add this to your cluster by adding the ``krib-metallb`` stage after the ``krib-config``
    stage in your workflow.  Currently only L2 mode is supported.  You will need to
    set the ``metallb/l2-ip-range`` param in your profile with the range of IP's you
    wish to use.  This ip range must not be within the configured DHCP scope.  See
    the MetalLB docs for more information (https://metallb.universe.tf/tutorial/layer2/).\n\n..
    _rs_nginx_ingress:\n\nNGINX Ingress\n==============================\n\nYou can
    add nginx-ingress to your cluster by adding the ``krib-ingress-nginx`` stage to
    your workflow.  This stage requires helm and tiller to be installed so should
    come after the ``krib-helm`` stage in your workflow.\n\nThis stage also requires
    a cloud provider LoadBalancer service or on bare metal you can add the ``krib-metallb``
    stage before this stage in your workflow.\n\nThis stage includes support for cert-manager
    if your profile is properly configured.  See ``example-cert-manager`` profile.\n\nKubernetes
    Dashboard via NGINX Ingress\n======================================\n\nIf your
    workflow includes the :ref:`rs_nginx_ingress` stage the kubernetes dashboard will
    be accessable via ``https://k8s-db.LOADBALANCER_IP.xip.io``.  The access url and
    cert-manager tls can also be configured by setting the appropriate params in your
    profile.  See ``example-k8s-db-ingress`` profile.\n\nPlease consult :ref:`rs_k8s_proxy`
    for information on getting the login token\n\nRook Ceph Manager Dashboard\n===========================\nIf
    you install the rook via the `krib-helm` chart template and have ``krib-ingress-nginx``
    stage in your workflow an ingress will be created so you can access the Ceph Manager
    Dashboard at ``https://rook-db.LOADBALANCER_IP.xip.io``.  The access url and cert-manager
    tls can also be configured by setting the appropriate params in your profile.
    \  See ``example-rook-db-ingress`` profile.\n\nThe default username is ``admin``
    and you can get the generated password with the with the following command:\n
    \ ::\n\n    kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o yaml
    | grep \"password:\" | awk '{print $2}' | base64 --decode\n\nMultiple Clusters\n-----------------\n\nIt
    is absolutely possible to build multiple Kubernetes KRIB clusters with this process.
    \ The only difference is each cluster should have a unique name and profile assigned
    to it.  A given Machine may only participate in a single Kubernetes cluster type
    at any one time.  You can install and operate both Live Boot/Immutable with install-to-disk
    cluster types in the same DRP Endpoint.\n\n\nFootnotes\n---------\n\n.. [#] Immutable
    Infrastructure Reference: `Making Server Deployment 10x Faster â€“ the ROI on Immutable
    Infrastructure <https://www.rackn.com/2017/10/11/making-server-deployment-10x-faster-roi-immutable-infrastructure/>`_\n\n..
    [#] Immutable Infrastructure Reference: `Go CI/CD and Immutable Infrastructure
    for Edge Computing Management <https://www.rackn.com/2017/09/15/go-cicd-immutable-infrastructure-edge-computing-management/>`_"
  Icon: ship
  License: APLv2
  Name: krib
  Order: "1000"
  Prerequisites: drp-community-content,task-library,certs
  RequiredFeatures: sane-exit-codes, job-exit-states, fsm-runner, workflows, default-workflow,
    http-range-header, roles, tenants, sprig, log-has-head-method
  Source: Community Content
  Tags: kubernetes,linux,rackn
  Type: dynamic
  Version: v0.0.0
sections:
  params:
    certmanager/acme-challenge-dns01-provider:
      Available: false
      Bundle: ""
      Description: certmanager acme challenge dns01 provider
      Documentation: |
        cert-manager DNS01 Challenge Provider Name
        Only route53, cloudflare, akamai, and rfc2136 are currently supported
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#supported-dns01-providers
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/acme-challenge-dns01-provider
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/cloudflare-api-key:
      Available: false
      Bundle: ""
      Description: certmanager cloudflare api key
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#cloudflare
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: certmanager/cloudflare-api-key
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/cloudflare-email:
      Available: false
      Bundle: ""
      Description: certmanager cloudflare email address
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#cloudflare
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/cloudflare-email
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/crds:
      Available: false
      Bundle: ""
      Description: Specifies the YAML config file to use for the cert-manager CRDs.
      Documentation: |
        Set this string to an HTTP or HTTPS reference for a YAML configuration to
        use for the cert-manager CRDs.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/crds
      ReadOnly: false
      Schema:
        default: https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8.1/deploy/manifests/00-crds.yaml
        type: string
      Secure: false
      Validated: false
    certmanager/default-issuer-name:
      Available: false
      Bundle: ""
      Description: certmanager default issuer name
      Documentation: |
        The default issuer to use when creating ingresses
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/default-issuer-name
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/dns-domain:
      Available: false
      Bundle: ""
      Description: certmanager dns domain
      Documentation: |
        cert-manager DNS domain - the suffix appended to hostnames on certificates
        signed with cert-manager. Used to auto-generate the ingress for rook ceph, for example
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/dns-domain
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/email:
      Available: false
      Bundle: ""
      Description: certmanager email address
      Documentation: |
        cert-manager ClusterIssuer configuration
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers.html#issuers
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/email
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/fastdns-access-token:
      Available: false
      Bundle: ""
      Description: certmanager fastdns access token
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#akamai-fastdns
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: certmanager/fastdns-access-token
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/fastdns-client-secret:
      Available: false
      Bundle: ""
      Description: certmanager fastdns client secret
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#akamai-fastdns
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: certmanager/fastdns-client-secret
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/fastdns-client-token:
      Available: false
      Bundle: ""
      Description: certmanager fastdns client token
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#akamai-fastdns
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: certmanager/fastdns-client-token
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/fastdns-service-consumer-domain:
      Available: false
      Bundle: ""
      Description: certmanager fastdns service consumer domain
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#akamai-fastdns
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/fastdns-service-consumer-domain
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/manifests:
      Available: false
      Bundle: ""
      Description: Specifies the YAML config file to use for an entire deploy of certmanager
        via manifests (vs helm)
      Documentation: |
        Set this string to an HTTP or HTTPS reference for a YAML configuration to
        use for the cert-manager deployment (https://github.com/jetstack/cert-manager/releases/download/v0.8.1/cert-manager-no-webhook.yaml is the non-validating one)
        https://github.com/jetstack/cert-manager/releases/download/v0.8.1/cert-manager.yaml is the validating one
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/manifests
      ReadOnly: false
      Schema:
        default: https://github.com/jetstack/cert-manager/releases/download/v0.8.1/cert-manager.yaml
        type: string
      Secure: false
      Validated: false
    certmanager/rfc2136-nameserver:
      Available: false
      Bundle: ""
      Description: certmanager rfc2136 nameserver
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#rfc2136
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/rfc2136-nameserver
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/rfc2136-tsig-alg:
      Available: false
      Bundle: ""
      Description: certmanager rfc2136 tsig algorithm
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#rfc2136
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/rfc2136-tsig-alg
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/rfc2136-tsig-key:
      Available: false
      Bundle: ""
      Description: certmanager rfc2136 tsig key
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#rfc2136
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: certmanager/rfc2136-tsig-key
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/rfc2136-tsig-key-name:
      Available: false
      Bundle: ""
      Description: certmanager rfc2136 tsig key name
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#rfc2136
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/rfc2136-tsig-key-name
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/route53-access-key:
      Available: false
      Bundle: ""
      Description: certmanager route53 access key
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#amazon-route53
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: certmanager/route53-access-key
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/route53-access-key-id:
      Available: false
      Bundle: ""
      Description: certmanager route53 access key id
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#amazon-route53
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: certmanager/route53-access-key-id
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/route53-hosted-zone-id:
      Available: false
      Bundle: ""
      Description: certmanager route53 hosted zone id
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#amazon-route53
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/route53-hosted-zone-id
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/route53-region:
      Available: false
      Bundle: ""
      Description: certmanager route53 region
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#amazon-route53
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: certmanager/route53-region
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    certmanager/route53-secret-access-key:
      Available: false
      Bundle: ""
      Description: certmanager route53 secret access key
      Documentation: |
        DNS01 Challenge Provider Configuration data
        See https://cert-manager.readthedocs.io/en/latest/reference/issuers/acme/dns01.html#amazon-route53
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: certmanager/route53-secret-access-key
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    consul/agent-count:
      Available: false
      Bundle: ""
      Description: Number of consul agents to expect
      Documentation: |
        Allows operators to set the number of machines required for the
        consul agents cluster.
        Machines will be automatically added until the number is met.
        NOTE: These machines will also be the vault members
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: consul/agent-count
      ReadOnly: false
      Schema:
        default: 1
        type: integer
      Secure: false
      Validated: false
    consul/agents:
      Available: false
      Bundle: ""
      Description: List of the agent UUIDs in the consul cluster
      Documentation: |
        Param is set (output) by the consul cluster building process
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: machines-map
        title: Community Content
      Name: consul/agents
      ReadOnly: false
      Schema:
        items:
          properties:
            Address:
              type: string
            Name:
              type: string
            Uuid:
              type: string
          type: object
        type: array
      Secure: false
      Validated: false
    consul/agents-done:
      Available: false
      Bundle: ""
      Description: List of the agents in the consul cluster that are complete
      Documentation: |
        Param is set (output) by the consul cluster building process
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: machines-map
        title: Community Content
      Name: consul/agents-done
      ReadOnly: false
      Schema:
        items:
          properties:
            Address:
              type: string
            Name:
              type: string
            Uuid:
              type: string
          type: object
        type: array
      Secure: false
      Validated: false
    consul/cluster-profile:
      Available: false
      Bundle: ""
      Description: Name of the profile for this consul cluster
      Documentation: |
        Part of the Digital Rebar Cluster pattern, this parameter is used
        to identify the machines used in the consul cluster
        This parameter is REQUIRED for KRIB and consul cluster contruction
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: profile
        title: Community Content
      Name: consul/cluster-profile
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    consul/controller-client-cert:
      Available: false
      Bundle: ""
      Description: Generated SSL cert for use by a controller, external to the KRIB
        cluster
      Documentation: |
        For use in configurations where the consul cluster is backed up from a machine
        external to the KRIB cluster. Will be generated for the value in consul/controller-ip.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: consul/controller-client-cert
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    consul/controller-client-key:
      Available: false
      Bundle: ""
      Description: Generated SSL key for use by a controller, external to the KRIB
        cluster
      Documentation: |
        For use in configurations where the consul cluster is backed up from a machine
        external to the KRIB cluster. Will be generated for the value in etcd/controller-client-cert.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: consul/controller-client-key
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    consul/controller-ip:
      Available: false
      Bundle: ""
      Description: An optional IP for a controller outside of KRIB used to interact
        with/backup consul
      Documentation: |
        An optional IP outside the cluster designated for a "controller" (can be the DRP host)
        Can be used in combination with consul/controller-client-cert and consul/controller/client-key
        to remotely backup a consul cluster

        If unset, will default to the DRP ProvisionerAddress.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: consul/controller-ip
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    consul/encryption-key:
      Available: false
      Bundle: ""
      Description: Encryption key used to secure Consul Gossip communications
      Documentation: |
        Enables gossip encryption between Consul nedes
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: consul/encryption-key
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    consul/name:
      Available: false
      Bundle: ""
      Description: Name of the consul cluster
      Documentation: |
        Allows operators to set a name for the consul cluster
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: consul/name
      ReadOnly: false
      Schema:
        default: consul-cluster
        type: string
      Secure: false
      Validated: false
    consul/server-ca-cert:
      Available: false
      Bundle: ""
      Description: The consul server CA cert
      Documentation: |
        Stores consul CA cert for use in non-DRP managed hosts (like a backup host)
        Requires Cert Plugin
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: consul/server-ca-cert
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    consul/server-ca-name:
      Available: false
      Bundle: ""
      Description: Name of the certificate root for the consul server certs
      Documentation: |
        Allows operators to set the CA name for the server certificate
        Requires Cert Plugin
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: consul/server-ca-name
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    consul/server-ca-pw:
      Available: false
      Bundle: ""
      Description: Password to access the consul server CA
      Documentation: |
        Allows operators to set the CA password for the consul server certificate
        Requires Cert Plugin
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: consul/server-ca-pw
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    consul/server-count:
      Available: false
      Bundle: ""
      Description: Number of consul servers to expect
      Documentation: |
        Allows operators to set the number of machines required for the
        consul cluster.
        Machines will be automatically added until the number is met.
        NOTE: should be an odd number
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: consul/server-count
      ReadOnly: false
      Schema:
        default: 1
        type: integer
      Secure: false
      Validated: false
    consul/servers:
      Available: false
      Bundle: ""
      Description: List of the server UUIDs in the consul cluster
      Documentation: |
        Param is set (output) by the consul cluster building process
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: machines-map
        title: Community Content
      Name: consul/servers
      ReadOnly: false
      Schema:
        items:
          properties:
            Address:
              type: string
            Name:
              type: string
            Uuid:
              type: string
          type: object
        type: array
      Secure: false
      Validated: false
    consul/servers-done:
      Available: false
      Bundle: ""
      Description: List of the servers in the consul cluster that are complete
      Documentation: |
        Param is set (output) by the consul cluster building process
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: machines-map
        title: Community Content
      Name: consul/servers-done
      ReadOnly: false
      Schema:
        items:
          properties:
            Address:
              type: string
            Name:
              type: string
            Uuid:
              type: string
          type: object
        type: array
      Secure: false
      Validated: false
    consul/version:
      Available: false
      Bundle: ""
      Description: Version of consul to use in cluster
      Documentation: |
        Allows operators to determine the version of consul to install
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: semver
        title: Community Content
      Name: consul/version
      ReadOnly: false
      Schema:
        default: 1.5.1
        type: string
      Secure: false
      Validated: false
    containerd/version:
      Available: false
      Bundle: ""
      Description: Version of containerd to use in cluster
      Documentation: |
        Allows operators to determine the version of containerd to install

        String should NOT include v as as a prefix
        Used to download from https://storage.googleapis.com/cri-containerd-release/cri-containerd-${VERSION}.linux-amd64.tar.gz path
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: cubes
        render: semver
        title: Community Content Copyright RackN 2019
      Name: containerd/version
      ReadOnly: false
      Schema:
        default: 1.2.7
        type: string
      Secure: false
      Validated: false
    docker/daemon:
      Available: false
      Bundle: ""
      Description: Special Docker Config Values
      Documentation: |
        Provide a custom /etc/docker/daemon.json
        See https://docs.docker.com/engine/reference/commandline/dockerd/
        For example:
          ::
            {"insecure-registries":["ci-repo.englab.juniper.net:5010"]}
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: boxes
        title: Community Content
      Name: docker/daemon
      ReadOnly: false
      Schema:
        properties:
          insecure-registries:
            type: array
        type: object
      Secure: false
      Validated: false
    docker/version:
      Available: false
      Bundle: ""
      Description: Docker Version to use for Kubernetes
      Documentation: |
        "Docker Version to use for Kubernetes"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: boxes
        title: Community Content
      Name: docker/version
      ReadOnly: false
      Schema:
        default: "18.09"
        type: string
      Secure: false
      Validated: false
    docker/working-dir:
      Available: false
      Bundle: ""
      Description: The docker working directory.
      Documentation: |
        Allows operators to change the Docker working directory
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: boxes
        title: Community Content
      Name: docker/working-dir
      ReadOnly: false
      Schema:
        default: /mnt/hdd/docker
        type: string
      Secure: false
      Validated: false
    etcd/client-ca-name:
      Available: false
      Bundle: ""
      Description: Name of the certificate root for the etcd client certs
      Documentation: |
        Allows operators to set the CA name for the client certificate
        Requires Cert Plugin
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: etcd/client-ca-name
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    etcd/client-ca-pw:
      Available: false
      Bundle: ""
      Description: Password to access the etcd client CA
      Documentation: |
        Allows operators to set the CA password for the client certificate
        Requires Cert Plugin
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: etcd/client-ca-pw
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    etcd/client-port:
      Available: false
      Bundle: ""
      Description: Port clients use to talk to etcd servers
      Documentation: |
        Allows operators to set the port used by etcd clients
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: etcd/client-port
      ReadOnly: false
      Schema:
        default: 2379
        type: integer
      Secure: false
      Validated: false
    etcd/cluster-client-vip-port:
      Available: false
      Bundle: ""
      Description: VIP etcd client port for multi-master etcd clusters (default 8379)
      Documentation: |
        The VIP client port to use for multi-master etcd clusters. Each
        etcd instance will bind to 'etcd/client-port' (2379 by default),
        but HA services will be serviced by this port number.

        Defaults to '8379'.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: etcd/cluster-client-vip-port
      ReadOnly: false
      Schema:
        default: 8379
        type: number
      Secure: false
      Validated: false
    etcd/cluster-profile:
      Available: false
      Bundle: ""
      Description: Name of the profile for this etcd cluster
      Documentation: |
        Part of the Digital Rebar Cluster pattern, this parameter is used
        to identify the machines used in the etcd cluster
        This parameter is REQUIRED for KRIB and etcd cluster contruction
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: profile
        title: Community Content
      Name: etcd/cluster-profile
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    etcd/controller-client-cert:
      Available: false
      Bundle: ""
      Description: Generated SSL cert for use by a controller, external to the KRIB
        cluster
      Documentation: |
        For configurations where the etcd cluster should communicate
        over a network other than the one that the machine was booted
        from. Will be generated for the value in etcd/controller-ip.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: etcd/controller-client-cert
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    etcd/controller-client-key:
      Available: false
      Bundle: ""
      Description: Generated SSL key for use by a controller, external to the KRIB
        cluster
      Documentation: |
        For configurations where the etcd cluster should communicate
        over a network other than the one that the machine was booted
        from. Will be generated for the value in etcd/controller-client-cert.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: etcd/controller-client-key
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    etcd/controller-ip:
      Available: false
      Bundle: ""
      Description: An optional IP for a controller outside of KRIB used to interact
        with/backup etcd
      Documentation: |
        An optional IP outside the cluster designated for a "controller" (can be the DRP host)
        Can be used in combination with etcd/controller-client-cert and etcd/controller/client-key
        to remotely backup an etcd cluster

        If unset, will default to the DRP ProvisionerAddress.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: etcd/controller-ip
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    etcd/ip:
      Available: false
      Bundle: ""
      Description: IP used by etcd for listen / advertise
      Documentation: |
        For configurations where the etcd cluster should communicate
        over a network other than the one that the machine was booted
        from.

        If unset, will default to the Machine Address.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: etcd/ip
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    etcd/name:
      Available: false
      Bundle: ""
      Description: Name of the etcd cluster
      Documentation: |
        Allows operators to set a name for the etcd cluster
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: etcd/name
      ReadOnly: false
      Schema:
        default: etcd-cluster
        type: string
      Secure: false
      Validated: false
    etcd/peer-ca-name:
      Available: false
      Bundle: ""
      Description: Name of the certificate root for the etcd peer certs
      Documentation: |
        Allows operators to set the CA name for the peer certificate
        Requires Cert Plugin
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: etcd/peer-ca-name
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    etcd/peer-ca-pw:
      Available: false
      Bundle: ""
      Description: Password to access the etcd peer CA
      Documentation: |
        Allows operators to set the CA password for the peer certificate
        If missing, will be generated
        Requires Cert Plugin
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: etcd/peer-ca-pw
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    etcd/peer-port:
      Available: false
      Bundle: ""
      Description: Port peers use to talk to each other
      Documentation: |
        Allows operators to set the port for the cluster peers
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: etcd/peer-port
      ReadOnly: false
      Schema:
        default: 2380
        type: integer
      Secure: false
      Validated: false
    etcd/server-ca-cert:
      Available: false
      Bundle: ""
      Description: The etcd server CA cert
      Documentation: |
        Stores etcd CA cert for use in non-DRP managed hosts (like a backup host)
        Requires Cert Plugin
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: etcd/server-ca-cert
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    etcd/server-ca-name:
      Available: false
      Bundle: ""
      Description: Name of the certificate root for the etcd server certs
      Documentation: |
        Allows operators to set the CA name for the server certificate
        Requires Cert Plugin
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: etcd/server-ca-name
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    etcd/server-ca-pw:
      Available: false
      Bundle: ""
      Description: Password to access the etcd server CA
      Documentation: |
        Allows operators to set the CA password for the server certificate
        Requires Cert Plugin
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: etcd/server-ca-pw
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    etcd/server-count:
      Available: false
      Bundle: ""
      Description: Number of servers to expected
      Documentation: |
        Allows operators to set the number of machines required for the
        etcd cluster.
        Machines will be automatically added until the number is met.
        NOTE: should be an odd number
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: etcd/server-count
      ReadOnly: false
      Schema:
        default: 1
        type: integer
      Secure: false
      Validated: false
    etcd/servers:
      Available: false
      Bundle: ""
      Description: List of the server UUIDs in the cluster
      Documentation: |
        Param is set (output) by the etcd cluster building process
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: machines-map
        title: Community Content
      Name: etcd/servers
      ReadOnly: false
      Schema:
        items:
          properties:
            Address:
              type: string
            Name:
              type: string
            Uuid:
              type: string
          type: object
        type: array
      Secure: false
      Validated: false
    etcd/servers-done:
      Available: false
      Bundle: ""
      Description: List of the servers in the cluster that are complete
      Documentation: |
        Param is set (output) by the etcd cluster building process
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: machines-map
        title: Community Content
      Name: etcd/servers-done
      ReadOnly: false
      Schema:
        items:
          properties:
            Address:
              type: string
            Name:
              type: string
            Uuid:
              type: string
          type: object
        type: array
      Secure: false
      Validated: false
    etcd/version:
      Available: false
      Bundle: ""
      Description: Version of the etcd to use in cluster
      Documentation: |
        Allows operators to determine the version of etcd to install
        Note: Changes should be coordinate with KRIB Kubernetes version
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: semver
        title: Community Content
      Name: etcd/version
      ReadOnly: false
      Schema:
        default: 3.3.13
        type: string
      Secure: false
      Validated: false
    helm/charts:
      Available: false
      Bundle: ""
      Description: Install Helm Charts
      Documentation: |
        Install Helm Charts
        -------------------

        .. _krib_helm:

        Array of charts to install via Helm.  The list will be followed in order.

        Work is idempotent: No action is taken if charts are already installed.

        Fields: chart and name are required.

        Options exist to inject additional control flags into helm install instructions:

        * name: name of the chart (required)
        * chart: reference of the chart (required) - may rely on repo, path or other helm install [chart] standard
        * namespace: kubernetes namespace to use for chart (defaults to none)
        * params: map of parameters to include in the helm install (optional).  Keys and values are converted to --[key] [value] in the install instruction.
        * sleep: time to wait after install (defaults to 10)
        * wait: wait for name (and namespace if provided) to be running before next action
        * prekubectl (optional) array of kubectl [request] commands to run before the helm install
        * postkubectl (optional) array of kubectl [request] commands to run after the helm install
        * targz (optional) provides a location for a tar.gz file containing charts to install. Path is relative.
        * templates (optional) map of DRP templates keyed to the desired names (must be uploaded!) to render before doing other work.
        * repos (optional) adds the requested repos to helm using `helm repo add` before installing helm.  syntax is `[repo name]: [repo path]`.
        * templatesbefore (optional) expands the provided template files inline before the helm install happens.
        * templatesafter (optional) expands the provided template files inline after the helm install happens

        example:

          ::

            [
              {
                "chart": "stable/mysql",
                "name": "mysql"
              }, {
                "chart": "istio-1.0.1/install/kubernetes/helm/istio",
                "name": "istio",
                "targz": "https://github.com/istio/istio/releases/download/1.0.1/istio-1.0.1-linux.tar.gz",
                "namespace": "istio-system",
                "params": {
                  "set": "sidecarInjectorWebhook.enabled=true"
                },
                "sleep": 10,
                "wait": true,
                "kubectlbefore": ["get nodes"],
                "kubectlafter": ["get nodes"]
              }, {
                "chart": "rook-stable/rook-ceph",
                "kubectlafter": [
                  "apply -f cluster.yaml"
                ],
                "name": "rook-ceph",
                "namespace": "rook-ceph-system",
                "repos": {
                  "rook-stable": "https://charts.rook.io/stable"
                },
                "templatesafter": [{
                  "name": "helm-rook.after.sh.tmpl"
                  "nodes": "leader",
                }],
                "templatesbefore": [{
                  "name": "helm-rook.before.sh.tmpl",
                  "nodes": "all",
                  "runIfInstalled": true
                }],
                "templates": {
                  "cluster": "helm-rook.cfg.tmpl"
                },
                "wait": true
             }
            ]
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: map
        title: Community Content
      Name: helm/charts
      ReadOnly: false
      Schema:
        default: []
        items:
          properties:
            chart:
              type: string
            kubectlafter:
              default: []
              items:
                type: string
              type: array
            kubectlbefore:
              default: []
              items:
                type: string
              type: array
            name:
              type: string
            namespace:
              type: string
            params:
              default: {}
              properties:
                repo:
                  type: string
                set:
                  type: string
                values:
                  type: string
              type: object
            repos:
              type: object
            sleep:
              default: 10
              type: integer
            targz:
              type: string
            templates:
              type: object
            templatesafter:
              default: []
              items:
                type: object
              type: array
            templatesbefore:
              default: []
              items:
                properties:
                  name:
                    type: string
                  nodes:
                    type: string
                    values:
                    - all
                    - masters
                    - leader
                  runIfInstalled:
                    default: false
                    type: boolean
                type: object
              type: array
            wait:
              default: false
              type: boolean
          required:
          - chart
          - name
          type: object
        type: array
      Secure: false
      Validated: false
    helm/version:
      Available: false
      Bundle: ""
      Description: Version of the helm to use in cluster
      Documentation: |
        Allows operators to determine the version of etcd to install
        Note: Changes should be coordinate with KRIB Kubernetes version
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: semver
        title: Community Content
      Name: helm/version
      ReadOnly: false
      Schema:
        default: latest
        type: string
      Secure: false
      Validated: false
    ingress/ip-address:
      Available: false
      Bundle: ""
      Description: IP Address assigned to ingress service via LoadBalancer
      Documentation: |
        IP Address assigned to ingress service via LoadBalancer
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: ingress/ip-address
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    ingress/k8s-dashboard-hostname:
      Available: false
      Bundle: ""
      Description: kubernetes dashboard hostname
      Documentation: |
        Hostname to use for the kubernetes dashboard. You
        will need to manually configure your DNS to point
        to the ingress ingress/ip-address.

        If no hostname is provided a {{"ingress/ip-address"}}.xip.io hostname will be assigned
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: ingress/k8s-dashboard-hostname
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    ingress/longhorn-dashboard-hostname:
      Available: false
      Bundle: ""
      Description: Rancher Longhorn Dashboard Hostname
      Documentation: |
        Hostname to use for the Rancher Longhorn Dashboard. You
        will need to manually configure your DNS to point
        to the ingress ingress/ip-address.

        If no hostname is provided a longhorn-db.$INGRESSIP.xip.io hostname will be assigned
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: ingress/longhorn-dashboard-hostname
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    ingress/rook-dashboard-hostname:
      Available: false
      Bundle: ""
      Description: Rook Ceph Manager Dashboard Hostname
      Documentation: |
        Hostname to use for the Rook Ceph Manager Dashboard. You
        will need to manually configure your DNS to point
        to the ingress ingress/ip-address.

        If no hostname is provided a rook-db.$INGRESSIP.xip.io hostname will be assigned
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: ingress/rook-dashboard-hostname
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/apiserver-extra-SANs:
      Available: false
      Bundle: ""
      Description: extra SANs used for the Kubernetes API Server
      Documentation: |
        List of additional SANs (IP addresses or FQDNs) used in the certificate used for the API Server
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        render: machines-map
        title: Community Content
      Name: krib/apiserver-extra-SANs
      ReadOnly: false
      Schema:
        items:
          properties:
            SAN:
              type: string
          type: object
        type: array
      Secure: false
      Validated: false
    krib/apiserver-extra-args:
      Available: false
      Bundle: ""
      Description: Kubeadm apiServerExtraArgs
      Documentation: |
        Array of apiServerExtraArgs that you want added to the kubeadm configuration.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/apiserver-extra-args
      ReadOnly: false
      Schema:
        default: {}
        type: object
      Secure: false
      Validated: false
    krib/calico-container-image-cni:
      Available: false
      Bundle: ""
      Description: Custom container image to use for Calico CNI
      Documentation: "Allows operators to optionally override the container image
        used in the\nCalico deployment. Possible use case would be pre-prepared images
        pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/calico-container-image-cni
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/calico-container-image-kube-controllers:
      Available: false
      Bundle: ""
      Description: Custom container image to use for Calico Kube Controllers
      Documentation: "Allows operators to optionally override the container image
        used in the\nCalico deployment. Possible use case would be pre-prepared images
        pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/calico-container-image-kube-controllers
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/calico-container-image-node:
      Available: false
      Bundle: ""
      Description: Custom container image to use for Calico node
      Documentation: "Allows operators to optionally override the container image
        used in the\nCalico deployment. Possible use case would be pre-prepared images
        pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/calico-container-image-node
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/calico-container-image-pod2daemon-flexvol:
      Available: false
      Bundle: ""
      Description: Custom container image to use for Calico pod2daemon flexvol
      Documentation: "Allows operators to optionally override the container image
        used in the\nCalico deployment. Possible use case would be pre-prepared images
        pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/calico-container-image-pod2daemon-flexvol
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/cert-manager-container-image-cainjector:
      Available: false
      Bundle: ""
      Description: Custom container image to use for cert-manager cainjector
      Documentation: "Allows operators to optionally override the container image
        used in the\ncert-manager deployment. Possible use case would be pre-prepared
        images pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/cert-manager-container-image-cainjector
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/cert-manager-container-image-controller:
      Available: false
      Bundle: ""
      Description: Custom container image to use for cert-manager controller
      Documentation: "Allows operators to optionally override the container image
        used in the\ncert-manager deployment. Possible use case would be pre-prepared
        images pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/cert-manager-container-image-controller
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/cert-manager-container-image-webhook:
      Available: false
      Bundle: ""
      Description: Custom container image to use for cert-manager webhook
      Documentation: "Allows operators to optionally override the container image
        used in the\ncert-manager deployment. Possible use case would be pre-prepared
        images pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/cert-manager-container-image-webhook
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/cluster-admin-conf:
      Available: false
      Bundle: ""
      Description: Admin config file
      Documentation: |
        Param is set (output) by the cluster building process
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/cluster-admin-conf
      ReadOnly: false
      Schema: null
      Secure: false
      Validated: false
    krib/cluster-api-port:
      Available: false
      Bundle: ""
      Description: API bindPort for Cluster (default 6443)
      Documentation: |
        The API bindPort number for the cluster masters.
        Defaults to '6443'.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/cluster-api-port
      ReadOnly: false
      Schema:
        default: 6443
        type: number
      Secure: false
      Validated: false
    krib/cluster-api-vip-port:
      Available: false
      Bundle: ""
      Description: VIP API port for multi-master clusters (default 8443)
      Documentation: |
        The VIP API port to use for multi-master clusters. Each
        master will bind to 'krib/cluster-api-port' (6443 by default),
        but HA services for the API will be services by this port
        number.

        Defaults to '8443'.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/cluster-api-vip-port
      ReadOnly: false
      Schema:
        default: 8443
        type: number
      Secure: false
      Validated: false
    krib/cluster-bootstrap-token:
      Available: false
      Bundle: ""
      Description: Define the cluster BootStrap Token value
      Documentation: |
        Defines the bootstrap token to use.
        Default is 'fedcba.fedcba9876543210'.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: krib/cluster-bootstrap-token
      ReadOnly: false
      Schema:
        default: fedcba.fedcba9876543210
        type: string
      Secure: false
      Validated: false
    krib/cluster-bootstrap-ttl:
      Available: false
      Bundle: ""
      Description: Define the cluster BootStrap Token TTL value
      Documentation: |
        How long BootStrap tokens for the cluster should live.
        Default is '24h0m0s'.

        Must use a format similar to the default.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/cluster-bootstrap-ttl
      ReadOnly: false
      Schema:
        default: 24h0m0s
        type: string
      Secure: false
      Validated: false
    krib/cluster-cni-version:
      Available: false
      Bundle: ""
      Description: Define what version of Kubernetes CNI utilities to install.
      Documentation: |
        Allows operators to specify the version of the Kubernetes CNI utilities to
        install.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/cluster-cni-version
      ReadOnly: false
      Schema:
        default: v0.8.0
        type: string
      Secure: false
      Validated: false
    krib/cluster-cri-socket:
      Available: false
      Bundle: ""
      Description: Specify the CRI Socket for Kubernetes.
      Documentation: "This Param defines which Socket to use for the Container Runtime\nInterface.
        \ By default KRIB content uses Docker as the CRI, however\nour goal is to
        support multiple container CRI formats. A viable \nalternative is /run/containerd/containerd.sock,
        assuming krib/container-runtime \nis set to \"containerd\"\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/cluster-cri-socket
      ReadOnly: false
      Schema:
        default: /var/run/dockershim.sock
        type: string
      Secure: false
      Validated: false
    krib/cluster-crictl-version:
      Available: false
      Bundle: ""
      Description: Define what version of Kubernetes CRICTL utility to install.
      Documentation: |
        Allows operators to specify the version of the Kubernetes CRICTL utility to
        install.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/cluster-crictl-version
      ReadOnly: false
      Schema:
        default: v1.15.0
        type: string
      Secure: false
      Validated: false
    krib/cluster-dns:
      Available: false
      Bundle: ""
      Description: Define the cluster DNS Address
      Documentation: |
        Allows operators to specify the DNS address for the cluster
        name resolution services.

        Set by default to "10.96.0.10".

        WARNING: This IP Address must be in the same range as the
                 "krib/cluster-service-cidr" specified addresses.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/cluster-dns
      ReadOnly: false
      Schema:
        default: 10.96.0.10
        type: string
      Secure: false
      Validated: false
    krib/cluster-domain:
      Available: false
      Bundle: ""
      Description: Define the cluster Domain
      Documentation: |
        Defines the cluster domain for kublets to operate
        in by default.
        Default is 'cluster.local'.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/cluster-domain
      ReadOnly: false
      Schema:
        default: cluster.local
        type: string
      Secure: false
      Validated: false
    krib/cluster-image-repository:
      Available: false
      Bundle: ""
      Description: Define where to pull Kubernetes container images from.
      Documentation: |
        Allows operators to specify the location to pull images from for
        Kubernetes.  Defaults to 'k8s.gcr.io'.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/cluster-image-repository
      ReadOnly: false
      Schema:
        default: k8s.gcr.io
        type: string
      Secure: false
      Validated: false
    krib/cluster-is-production:
      Available: false
      Bundle: ""
      Description: Set the KRIB cluster to production mode.
      Documentation: |
        By default the KRIB cluster mode will be set to dev/test/lab
        (whatever you wanna call it).  If you set this Param to true
        then the cluster will be tagged as in Production use.

        If the cluster is in Production mode, then the state of the
        various Params for new clusters will be preserved, preventing
        the cluster from being overwritten.

        If NOT in Production mode, the following Params will be wiped
        clean before building the cluster.  This is essentially a
        destructive pattern.

          krib/cluster-admin-conf - the admin.conf file Param will be wiped
          krib/cluster-join       - the Join token will be destroyed

        This allows for "fast reuse" patterns with building KRIB
        clusters, while also allowing a cluster to be marked Production
        and require manual intervention to wipe the Params to rebuild
        the cluster.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/cluster-is-production
      ReadOnly: false
      Schema:
        default: false
        type: boolean
      Secure: false
      Validated: false
    krib/cluster-join-command:
      Available: false
      Bundle: ""
      Description: kubeadm command to join the cluster
      Documentation: |
        Param is set (output) by the cluster building process
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        password: hideme
        title: Community Content
      Name: krib/cluster-join-command
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/cluster-kubeadm-cfg:
      Available: false
      Bundle: ""
      Description: The 'kubeadm.cfg' bootstrap config will be recorded on this param.
      Documentation: |
        Once the cluster initial master has completed startup, then the
        KRIB config task will record the bootstrap configuration used
        by 'kubeadm init'.  This provides a reference going forward on
        the cluster configurations when it was created.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/cluster-kubeadm-cfg
      ReadOnly: false
      Schema: null
      Secure: false
      Validated: false
    krib/cluster-kubernetes-version:
      Available: false
      Bundle: ""
      Description: Define what version of Kubernetes containers to install.
      Documentation: |
        Allows operators to specify the version of Kubernetes containers to
        pull from the 'krib/cluster-image-repository'.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/cluster-kubernetes-version
      ReadOnly: false
      Schema:
        default: v1.15.2
        type: string
      Secure: false
      Validated: false
    krib/cluster-master:
      Available: false
      Bundle: ""
      Description: Master of Kubernetes cluster
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        render: machines-map
        title: Community Content
      Name: krib/cluster-master
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/cluster-master-certs:
      Available: false
      Bundle: ""
      Description: Kubeadm requires the master to have the same ca that it generates.
        Base 64 tgz string
      Documentation: |
        Requires Cert Plugin
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        password: hideme
        title: Community Content
      Name: krib/cluster-master-certs
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/cluster-master-count:
      Available: false
      Bundle: ""
      Description: Number of masters to create
      Documentation: |
        Allows operators to set the number of machines required for the
        Kubernetes cluster.
        Machines will be automatically added until the number is met.
        NOTE: should be an odd number
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/cluster-master-count
      ReadOnly: false
      Schema:
        default: 1
        type: integer
      Secure: false
      Validated: false
    krib/cluster-master-vip:
      Available: false
      Bundle: ""
      Description: Virtual IP for the Master Servers
      Documentation: |
        For High Availability (HA) configurations, a floating IP is required
        by the load balancer.  This should be an available IP in the same
        subnet as the master nodes and not in the dhcp range.  If using MetalLB
        the ip should not be in the configured metallb/l2-ip-range.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/cluster-master-vip
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/cluster-masters:
      Available: false
      Bundle: ""
      Description: Masters of Kubernetes cluster
      Documentation: |
        List of the machine(s) assigned as cluster master(s).
        If not set, the automation will elect leaders and populate the list automatically.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        render: machines-map
        title: Community Content
      Name: krib/cluster-masters
      ReadOnly: false
      Schema:
        items:
          properties:
            Address:
              type: string
            Name:
              type: string
            Uuid:
              type: string
          type: object
        type: array
      Secure: false
      Validated: false
    krib/cluster-masters-on-etcds:
      Available: false
      Bundle: ""
      Description: Should the masters run on the etcd servers
      Documentation: |
        For development clusters, allows running etcd on the same
        machines as the Kubernetes masters
        RECOMMENDED: set to false for production clusters
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/cluster-masters-on-etcds
      ReadOnly: false
      Schema:
        default: true
        type: boolean
      Secure: false
      Validated: false
    krib/cluster-masters-untainted:
      Available: false
      Bundle: ""
      Description: Should the masters be untainted
      Documentation: |
        For development clusters, allows nodes to run
        on the same machines as the Kubernetes masters.
        NOTE: If you have only master nodes helm/tiller
        install will fail if set to false.
        RECOMMENDED: set to false for production clusters
        and have non-master nodes in the cluster
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/cluster-masters-untainted
      ReadOnly: false
      Schema:
        default: true
        type: boolean
      Secure: false
      Validated: false
    krib/cluster-name:
      Available: false
      Bundle: ""
      Description: Name of Kubernetes cluster
      Documentation: |
        Allows operators to set the Kubernetes cluster name
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/cluster-name
      ReadOnly: false
      Schema:
        default: k8s-cluster
        type: string
      Secure: false
      Validated: false
    krib/cluster-pod-subnet:
      Available: false
      Bundle: ""
      Description: Define the cluster podSugnetvia 'kubeadm init' process
      Documentation: |
        Allows operators to specify the podSubnet that will be
        used by CoreDNS during the 'kubeadm init' process of the cluster
        creation.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/cluster-pod-subnet
      ReadOnly: false
      Schema:
        default: 10.112.0.0/12
        type: string
      Secure: false
      Validated: false
    krib/cluster-profile:
      Available: false
      Bundle: ""
      Description: Name of the profile for this kubernetes cluster
      Documentation: |
        Part of the Digital Rebar Cluster pattern, this parameter is used
        to identify the machines used in the Kubernetes cluster
        This parameter is REQUIRED for KRIB and etcd cluster contruction
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        render: profile
        title: Community Content
      Name: krib/cluster-profile
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/cluster-service-dns-domain:
      Available: false
      Bundle: ""
      Description: Define the cluster Service DNS Domain via 'kubeadm init' process
      Documentation: |
        Allows operators to specify the Service DNS Domain that will be
        used by CoreDNS during the 'kubeadm init' process of the cluster
        creation.

        By default we do not override the setting from kubeadm default
        behavior.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/cluster-service-dns-domain
      ReadOnly: false
      Schema:
        default: ""
        type: string
      Secure: false
      Validated: false
    krib/cluster-service-subnet:
      Available: false
      Bundle: ""
      Description: Define the cluster Service subnet CIDR via 'kubeadm init' process
      Documentation: |
        Allows operators to specify the service subnet CIDR that will be
        used during the 'kubeadm init' process of the cluster creation.

        Defaults to "10.96.0.0/12".
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/cluster-service-subnet
      ReadOnly: false
      Schema:
        default: 10.96.0.0/12
        type: string
      Secure: false
      Validated: false
    krib/container-runtime:
      Available: false
      Bundle: ""
      Description: Container runtime employed in KRIB cluster
      Documentation: |
        The container runtime to be used for the KRIB cluster. This can be
        either docker (the default) or containerd.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/container-runtime
      ReadOnly: false
      Schema:
        default: docker
        enum:
        - docker
        - containerd
        type: string
      Secure: false
      Validated: false
    krib/dashboard-config:
      Available: false
      Bundle: ""
      Description: Specifies the YAML config file to use for the Dashboard.
      Documentation: |
        Set this string to an HTTP or HTTPS reference for a YAML configuration to
        use for the Kubernetes Dashboard.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/dashboard-config
      ReadOnly: false
      Schema:
        default: https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended.yaml
        type: string
      Secure: false
      Validated: false
    krib/dashboard-enabled:
      Available: false
      Bundle: ""
      Description: Enables install of Kubernetes dashboard
      Documentation: Boolean value that enables Kubernetes dashboard install
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: dashboard
        title: Digital Rebar Community Content
      Name: krib/dashboard-enabled
      ReadOnly: false
      Schema:
        default: true
        type: boolean
      Secure: false
      Validated: false
    krib/externaldns-container-image:
      Available: false
      Bundle: ""
      Description: Custom container image to use for ExternalDNS
      Documentation: "Allows operators to optionally override the container image
        used in the\nExteranlDNS deployment. Possible use case would be pre-prepared
        images pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/externaldns-container-image
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/fluent-bit-container-image:
      Available: false
      Bundle: ""
      Description: Custom container image to use for fluent-bit
      Documentation: "Allows operators to optionally override the container image
        used in the\nfluent-bit / logging daemonset. Possible use case would be pre-prepared
        images pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/fluent-bit-container-image
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/i-am-master:
      Available: false
      Bundle: ""
      Description: Cause member nodes to assume a GKC master role
      Documentation: |
        When this param is set to true AND the krib/selective-mastership param is set to true,
        then this node will participate in mastership election. On the other hard, if this param
        is set to false (the default), but krib/selective-mastership param is set to true, then
        this node will never become a master. This option is useful to prevent dedicated workers from
        assuming mastership.

        Defaults to 'false'.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/i-am-master
      ReadOnly: false
      Schema:
        default: false
        type: boolean
      Secure: false
      Validated: false
    krib/ignore-preflight-errors:
      Available: false
      Bundle: ""
      Description: Options for kubeadm init --ignore-preflight-errors
      Documentation: |
        Helpful for debug and test clusters.  This flag allows operators to select none, all or some
        preflight error checks to ignore during the kubeadm init.

        Use "all" to ignore all errors
        Use "none" to include all errors [default]

        More Info, see https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/
      Endpoint: ""
      Errors: []
      Meta:
        color: black
        icon: bug
        title: Community Content
      Name: krib/ignore-preflight-errors
      ReadOnly: false
      Schema:
        default: none
        type: string
      Secure: false
      Validated: false
    krib/ingress-external-enabled:
      Available: false
      Bundle: ""
      Description: Enables install of a second nginx controller intended for external
        access
      Documentation: |
        When enabled, deploys a second ingress controller (the first is default)
        Services are exposed to the second ingress using the class "nginx-external" instead of the
        "ingress" class. This can be helpful in environments where it may be helpful to expose
        some services only to the cluster, as an ingress, as opposed to the world.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: dashboard
        title: Digital Rebar Community Content
      Name: krib/ingress-external-enabled
      ReadOnly: false
      Schema:
        default: false
        type: boolean
      Secure: false
      Validated: false
    krib/ingress-nginx-config:
      Available: false
      Bundle: ""
      Description: Specifies the YAML config file url to use for nginx ingress service
        install.
      Documentation: "Set this string to an HTTP or HTTPS reference for a YAML configuration
        to\nuse for nginx service install.  \n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/ingress-nginx-config
      ReadOnly: false
      Schema:
        default: https://github.com/kubernetes/ingress-nginx/raw/nginx-0.24.1/deploy/provider/cloud-generic.yaml
        type: string
      Secure: false
      Validated: false
    krib/ingress-nginx-external-loadbalancer-ip:
      Available: false
      Bundle: ""
      Description: Optionally specifies a loadbalancer IP for the nginx external ingress
      Documentation: |
        If set, this param will set the LoadBalancer IP for the nginx external ingress service. Used
        in situations where you want to specifically choose the IP assigned to the ingress,
        rather than having it be applied by the cloud provider (or metallb, in the bare-metal case)
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/ingress-nginx-external-loadbalancer-ip
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/ingress-nginx-loadbalancer-ip:
      Available: false
      Bundle: ""
      Description: Optionally specifies a loadbalancer IP for the nginx ingress
      Documentation: |
        If set, this param will set the LoadBalancer IP for the nginx ingress service. Used
        in situations where you want to specifically choose the IP assigned to the ingress,
        rather than having it be applied by the cloud provider (or metallb, in the bare-metal case)
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/ingress-nginx-loadbalancer-ip
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/ingress-nginx-mandatory:
      Available: false
      Bundle: ""
      Description: Specifies the YAML config file url to use for nginx ingress install.
      Documentation: "Set this string to an HTTP or HTTPS reference for a YAML configuration
        to\nuse for nginx install.  \n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/ingress-nginx-mandatory
      ReadOnly: false
      Schema:
        default: https://github.com/kubernetes/ingress-nginx/raw/nginx-0.25.1/deploy/static/mandatory.yaml
        type: string
      Secure: false
      Validated: false
    krib/ingress-nginx-publish-ip:
      Available: false
      Bundle: ""
      Description: Optionally specifies a publish IP to the nginx external ingress
      Documentation: |
        If running an nginx ingress behind a NATing firewall, it may be required to explicitly specify
        the public IP assigned to ingresses, for example to make something like external-dns work
        If this value is set, then either the nginx ingress, or (if enabled) the _external_ nginx ingress
        will have the "--publish-status-address" argument set to this value.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/ingress-nginx-publish-ip
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/ip:
      Available: false
      Bundle: ""
      Description: InternalIP (--node-ip param to kubelet) used by Kubernetes hosts
      Documentation: |
        For configurations where kubelet does not correctly detect the IP
        over which nodes should communicate.

        If unset, will default to the Machine Address.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/ip
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/k3s:
      Available: false
      Bundle: ""
      Description: Use k3s for KRIB (instead of k8s)
      Documentation: |
        Informs tasks to use k3s instead of k8s
        No need to include etcd stages when k3s is true
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: cube
        title: Community Content
      Name: krib/k3s
      ReadOnly: false
      Schema:
        default: false
        type: boolean
      Secure: false
      Validated: false
    krib/kubeadm-cfg:
      Available: false
      Bundle: ""
      Description: Use an alternate 'kubeadm.cfg' for 'kubeadm init'
      Documentation: "Set this string to an HTTP or HTTPS reference for a YAML configuration
        to\nuse for the 'kubeadm.cfg' used during the 'kubeadm init' process.\n\nThe
        default behavior is to use the Parameterized 'kubeadm.cfg' from the\ntemplate
        named 'krib-kubeadm.cfg.tmpl'.  This config file is used in \nthe 'krib-config.sh.tmpl'
        which is the main template script that drives\nthe 'kubeadm' cluster init
        and configuration.\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/kubeadm-cfg
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/kubelet-rubber-stamp-container-image:
      Available: false
      Bundle: ""
      Description: Custom container image to use for kublet rubber stamp
      Documentation: "Allows operators to optionally override the container image
        used in the\nkubelt rubber stamp deployment. Possible use case would be pre-prepared
        images pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/kubelet-rubber-stamp-container-image
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/label-env:
      Available: false
      Bundle: ""
      Description: Value set for env label
      Documentation: |
        Used for node specification, labels should be set.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: tag
        title: Community Content
      Name: krib/label-env
      ReadOnly: false
      Schema:
        default: dev
        type: string
      Secure: false
      Validated: false
    krib/labels:
      Available: false
      Bundle: ""
      Description: 'Hash of labels: values to add to nodes'
      Documentation: |
        Used for adhoc node specification, labels should be set.

        NOTES:
        * Use krib/label-env to set the env label!
        * Use inventory/data to set physical characteristics
      Endpoint: ""
      Errors: []
      Meta: {}
      Name: krib/labels
      ReadOnly: false
      Schema:
        Meta: null
        color: blue
        default:
          builder: krib
        icon: tags
        items:
          type: string
        title: Community Content
        type: object
      Secure: false
      Validated: false
    krib/log-target-gelf:
      Available: false
      Bundle: ""
      Description: The target for GELF (Graylog) logs if running the krib-logging
        stage
      Documentation: |
        An IP outside the cluster designated configured to receive GELF (GrayLog) "The target for GELF (Graylog) "The target for GELF (Graylog) logs
        on UDP 2201
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/log-target-gelf
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/log-target-syslog:
      Available: false
      Bundle: ""
      Description: The target for remote syslog logs if running the krib-logging stage
      Documentation: |
        An IP outside the cluster designated configured to receive remote syslog logs
        on UDP 514
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/log-target-syslog
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/longhorn-config:
      Available: false
      Bundle: ""
      Description: Specifies the YAML config file to use for the Rancher Longhorn
        install.
      Documentation: |
        Set this string to an HTTP or HTTPS reference for a YAML configuration to
        use for the Rancher Longhorn install.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/longhorn-config
      ReadOnly: false
      Schema:
        default: https://raw.githubusercontent.com/rancher/longhorn/master/deploy/longhorn.yaml
        type: string
      Secure: false
      Validated: false
    krib/metallb-container-image-controller:
      Available: false
      Bundle: ""
      Description: Custom container image to use for MetalLB controller
      Documentation: "Allows operators to optionally override the container image
        used in the\nMetalLB controller deployment. Possible use case would be pre-prepared
        images pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/metallb-container-image-controller
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/metallb-container-image-speaker:
      Available: false
      Bundle: ""
      Description: Custom container image to use for MetalLB speaker
      Documentation: "Allows operators to optionally override the container image
        used in the\nMetalLB speaker daemonset. Possible use case would be pre-prepared
        images pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/metallb-container-image-speaker
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/metallb-version:
      Available: false
      Bundle: ""
      Description: Specifies the version of MetalLB to install.
      Documentation: |
        Set this string to the version of MetalLB to install. Defaults to "master", but cautious users may want to set
        this to an established MetalLB release version
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/metallb-version
      ReadOnly: false
      Schema:
        default: master
        type: string
      Secure: false
      Validated: false
    krib/networking-provider:
      Available: false
      Bundle: ""
      Description: Set the network provider types (eg 'flannel', 'calico').
      Documentation: "This Param can be used to specify either 'flannel', 'calico',
        or 'weave'\nnetwork providers for the Kubernetes cluster.  This is completed\nusing
        the provider specific YAML definition file. \n\nThe only supported providers
        are:\n\n  flannel\n  calico\n  weave\n\nThe default is 'flannel'.\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/networking-provider
      ReadOnly: false
      Schema:
        default: calico
        enum:
        - flannel
        - calico
        - weave
        type: string
      Secure: false
      Validated: false
    krib/nginx-external-udp-services:
      Available: false
      Bundle: ""
      Description: Nginx External Controller UDP services
      Documentation: |
        Array of optional UDP services you want to expose using Nginx Ingress Controller
        Example might be:
           9000: "default/example-go:8080"
        The services defined here will be inserted in a configmap named "udp-services" in the "ingress-nginx-external" namespace. The
        ConfigMap can be updated later if you want to change/update services

        See https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/ for details
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/nginx-external-udp-services
      ReadOnly: false
      Schema:
        default: {}
        type: object
      Secure: false
      Validated: false
    krib/nginx-ingress-controller-container-image:
      Available: false
      Bundle: ""
      Description: Custom container image to use for nginx ingress controller
      Documentation: "Allows operators to optionally override the container image
        used in the\nnginx-ingress-controller deployment. Possible use case would
        be pre-prepared images pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/nginx-ingress-controller-container-image
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/nginx-tcp-services:
      Available: false
      Bundle: ""
      Description: Nginx Controller TCP services
      Documentation: |
        Array of optional TCP services you want to expose using Nginx Ingress Controller
        Example might be:
           9000: "default/example-go:8080"
        The services defined here will be inserted in a configmap named "tcp-services" in the "ingress-nginx" namespace. The
        ConfigMap can be updated later if you want to change/update services

        See https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/ for details
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/nginx-tcp-services
      ReadOnly: false
      Schema:
        default: {}
        type: object
      Secure: false
      Validated: false
    krib/nginx-udp-services:
      Available: false
      Bundle: ""
      Description: Nginx Controller UDP services
      Documentation: |
        Array of optional UDP services you want to expose using Nginx Ingress Controller
        Example might be:
           9000: "default/example-go:8080"
        The services defined here will be inserted in a configmap named "udp-services" in the "ingress-nginx" namespace. The
        ConfigMap can be updated later if you want to change/update services

        See https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/ for details
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/nginx-udp-services
      ReadOnly: false
      Schema:
        default: {}
        type: object
      Secure: false
      Validated: false
    krib/operate-action:
      Available: false
      Bundle: ""
      Description: Operational action to take on KRIB cluster
      Documentation: |
        This Param can be used to:

          'drain', 'delete', 'cordon', or 'uncordon'

        a node in a KRIB built Kubernetes cluster. If this parameter is not
        defined on the Machine, the default action will be to 'drain' the node.

        Each action can be passed custom arguments via use of the 'krib/operate-options'
        Param.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/operate-action
      ReadOnly: false
      Schema:
        default: drain
        enum:
        - drain
        - delete
        - cordon
        - uncordon
        type: string
      Secure: false
      Validated: false
    krib/operate-on-node:
      Available: false
      Bundle: ""
      Description: Specify a Kubernetes node to drain from the cluster Master
      Documentation: |
        This Param specifies a Node in a Kubernetes cluster that should be
        operated on. Currently supported operations are 'drain' and 'uncordon'.

        The drain operation will by default maintain the contracts specified
        by PodDisruptionBudgets.

        Options can be specified to override the default actions by use
        of the 'krib/operate-options' Param.  This Param will be passed
        directly to the 'kubectl' command that has been specified by
        the 'krib/operate-action' Param setting (defaults to 'drain'
        operation if nothing specified).

        The Node name must be a valid cluster member name, which by default
        in a KRIB built cluster; the fully qualified value of the Machine
        object 'Name' value.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        render: machine
        title: Community Content
      Name: krib/operate-on-node
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/operate-options:
      Available: false
      Bundle: ""
      Description: Space separted list of command line flags to pass to 'kubectl'
        operations specified in 'krib/operate-action' ...'
      Documentation: |
        This Param can be used to pass additional flag options to the
        'kubectl' operation that is specified by the 'krib/operate-action'
        Param.  By default, the 'drain' operation will be called if no
        action is defined on the Machine.

        This Param provides some customization to how the operate operation
        functions.

        For 'kubectl drain' documentation, see the following URL:
          https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#drain
        For 'kubectl uncordin' doc, see the URL:
          https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#uncordon

        NOTE NOTE the following flags are set as default options in the
        Template for drain operations:

          --ignore-daemonsets --delete-local-data

        For 'drain' operations, if you override these defaults, you MOST LIKELY
        need to specify them for the drain operation to be successful.  You have
        been warned.

        No defaults provdided for 'uncordon' operations (you shouldn't need any).
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/operate-options
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/packages-to-prep:
      Available: false
      Bundle: ""
      Description: Space separated list of package prerequisites to install.
      Documentation: |
        List of packages to install for preparation of KRIB install
        process.  Designed to be used in some processes where pre-prep
        of the packages will accelerate the KRIB install process.  More
        specifically, in Sledgehammer Discover for Live Boot cluster
        install, the 'docker' install requires several minutes to run
        through selinux context changes.

        Simple space separated String list.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/packages-to-prep
      ReadOnly: false
      Schema:
        default: apt-transport-https keepalived psmisc rsync git wget haproxy docker
        type: string
      Secure: false
      Validated: false
    krib/repo:
      Available: false
      Bundle: ""
      Description: URL path to a pre-prepared collection of necessary KRIB install
        files
      Documentation: "Allows operators to pre-prepare a URL (i.e., a local repository)
        of the installation packages necessary for KRIB. \nIf this value is set, then
        tasks like containerd-install and etcd-config will source their installation
        files\nfrom this repository, rather than attempting to download them from
        the internet (which may take longer, given\nthe amount of machines to be installed
        plus the capacity of the internet service)\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: krib/repo
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/rook-ceph-container-image:
      Available: false
      Bundle: ""
      Description: Custom container image to use for rook ceph
      Documentation: "Allows operators to optionally override the container image
        used in the\nrook-ceph deployment. Possible use case would be pre-prepared
        images pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/rook-ceph-container-image
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/rook-ceph-container-image-ceph:
      Available: false
      Bundle: ""
      Description: Custom container image to use for rook ceph/ceph
      Documentation: "Allows operators to optionally override the container image
        used in the\nrook-ceph deployment. Possible use case would be pre-prepared
        images pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/rook-ceph-container-image-ceph
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/rook-ceph-container-image-daemon-base:
      Available: false
      Bundle: ""
      Description: Custom container image to use for rook ceph daemon-base
      Documentation: "Allows operators to optionally override the container image
        used in the\nrook-ceph deployment. Possible use case would be pre-prepared
        images pushed to a \nlocal trusted registry\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: krib/rook-ceph-container-image-daemon-base
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    krib/selective-mastership:
      Available: false
      Bundle: ""
      Description: Require an additional param (krib/i-am-master) to permit a machine
        to self-elect to master
      Documentation: "When this param is set to true, then in order for a machine
        to self-elect to a mastetr,\nthe, krib/i-am-master param must be set for that
        machine. This option is useful to prevent \ndedicated workers from assuming
        mastership.\n\nDefaults to 'false'.\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/selective-mastership
      ReadOnly: false
      Schema:
        default: false
        type: boolean
      Secure: false
      Validated: false
    krib/sign-kubelet-server-certs:
      Available: false
      Bundle: ""
      Description: Whether to sign the kubelet server certs using the CA, or leave
        them as self-signed (default)
      Documentation: |
        When this param is set to true, then the kubelets will be configured to request their certs from
        the cluster CA, using CSRs. The CSR approver won't natively sign server certs, so a custom operator,
        "https://github.com/kontena/kubelet-rubber-stamp", will be deployed to sign these.

        Defaults to 'false'.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib/sign-kubelet-server-certs
      ReadOnly: false
      Schema:
        default: false
        type: boolean
      Secure: false
      Validated: false
    kubectl/working-dir:
      Available: false
      Bundle: ""
      Description: The kubectl working directory.
      Documentation: |
        Allows operators to change the kubectl working directory
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: docker
        title: Community Content
      Name: kubectl/working-dir
      ReadOnly: false
      Schema:
        default: /mnt/hdd/kubectl
        type: string
      Secure: false
      Validated: false
    metallb/l2-ip-range:
      Available: false
      Bundle: ""
      Description: IP range for MetalLB L2 config
      Documentation: "This should be set to match the IP range \nyou have allocated
        to L2 MetalLB\nex: 192.168.1.240-192.168.1.250\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: metallb/l2-ip-range
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    metallb/l3-ip-range:
      Available: false
      Bundle: ""
      Description: IP range for MetalLB L3 config
      Documentation: "This should be set to match the CIDR route \nyou have allocated
        to L3 MetalLB\nex: 192.168.1.0/24 (currently only a single route supported)\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: metallb/l3-ip-range
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    metallb/l3-peer-address:
      Available: false
      Bundle: ""
      Description: IP range for MetalLB L3 peer address
      Documentation: |
        This should be set to match the IP of the
        BGP-enabled router you want MetalLB to peer with
        ex: 192.168.1.1 (currently only a single peer supported)
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: metallb/l3-peer-address
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    metallb/limits-cpu:
      Available: false
      Bundle: ""
      Description: CPU resource limits for MetalLB
      Documentation: "This should be set to match the cpu \nresource limits for MetalLB\n\nDefault:
        100m\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: metallb/limits-cpu
      ReadOnly: false
      Schema:
        default: 100m
        type: string
      Secure: false
      Validated: false
    metallb/limits-memory:
      Available: false
      Bundle: ""
      Description: Memory resource limits for MetalLB
      Documentation: "This should be set to match the memory \nresource limits for
        MetalLB\n\nDefault: 100Mi\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: metallb/limits-memory
      ReadOnly: false
      Schema:
        default: 100Mi
        type: string
      Secure: false
      Validated: false
    metallb/monitoring-port:
      Available: false
      Bundle: ""
      Description: Port for monitoring MetalLB with Prometheus
      Documentation: |
        This should be set to match the port you want to use
        for MetalLB Prometheus monitoring

        Default: 7472
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: setting
        title: Community Content
      Name: metallb/monitoring-port
      ReadOnly: false
      Schema:
        default: 7472
        type: number
      Secure: false
      Validated: false
    provider/calico-config:
      Available: false
      Bundle: ""
      Description: Calico config YAML to apply if using Calico Network Provider
      Documentation: |
        Set this string to an HTTP or HTTPS reference for a YAML configuration to
        use for the Calico network provider.  If Calico is not installed, this
        Param will have no effect on the cluster.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: provider/calico-config
      ReadOnly: false
      Schema:
        default: https://docs.projectcalico.org/v3.8/manifests/calico.yaml
        type: string
      Secure: false
      Validated: false
    provider/flannel-config:
      Available: false
      Bundle: ""
      Description: Flannel config YAML to apply if using Flannel Network Provider
      Documentation: |
        Set this string to an HTTP or HTTPS reference for a YAML configuration to
        use for the Flannel network provider.  If Flannel is not installed, this
        Param will have no effect on the cluster.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: provider/flannel-config
      ReadOnly: false
      Schema:
        default: https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml
        type: string
      Secure: false
      Validated: false
    rook/ceph-cluster-network:
      Available: false
      Bundle: ""
      Description: Network (in CIDR format) rook ceph will use when separating cluster
        traffic from public traffic
      Documentation: |
        This should be set to match a physical network on rook nodes to be used exclusively
        for cluster traffic
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: chess rock
        title: Community Content
      Name: rook/ceph-cluster-network
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    rook/ceph-public-network:
      Available: false
      Bundle: ""
      Description: Network (in CIDR format) rook ceph will use for public traffic
        - make this the same as the Kubernetes control-plane network
      Documentation: |
        This should be set to match the kubernetes "control plane" network on the nodes
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: chess rock
        title: Community Content
      Name: rook/ceph-public-network
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    rook/ceph-target-disk:
      Available: false
      Bundle: ""
      Description: Target disk used for ceph installation
      Documentation: "If using physical disks for Ceph OSDs, this value will be used
        as an explicit\ntarget for OSD installation. It will also be WIPED during
        dev-reset, so use with\ncare. The value of the param should be only the block
        device, don't prefix with /dev/\nExample value might be \"sda\", indicating
        that /dev/sda is to be used for rook ceph, and \nWIPED DURING RESET.\n\nAGAIN,
        IF YOU SET THIS VALUE, /dev/<this value> WILL BE WIPED DURING cluster dev-reset!!\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: chess rock
        title: Community Content
      Name: rook/ceph-target-disk
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    rook/ceph-version:
      Available: false
      Bundle: ""
      Description: The version of rook-ceph to deploy
      Documentation: |
        The version of rook-ceph to deploy
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: chess rock
        title: Community Content
      Name: rook/ceph-version
      ReadOnly: false
      Schema:
        default: 1.0.4
        type: string
      Secure: false
      Validated: false
    rook/data-dir-host-path:
      Available: false
      Bundle: ""
      Description: Location of Ceph Storage
      Documentation: |
        This should be set to match the desired location
        for Ceph storage.

        Default: /mnt/hdd/rook

        In future versions, this should be calculated or inferred based on the system inventory
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: chess rock
        title: Community Content
      Name: rook/data-dir-host-path
      ReadOnly: false
      Schema:
        default: /mnt/hdd/rook
        type: string
      Secure: false
      Validated: false
    sonobuoy/binary:
      Available: false
      Bundle: ""
      Description: Go binary with Sonobuoy tooling for test
      Documentation: |
        Downloads tgz with compiled sonobuoy executable.
        The full path is included so that operators can choose the correct version and archtiecture
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: sound
        title: Community Content
      Name: sonobuoy/binary
      ReadOnly: false
      Schema:
        default: https://github.com/heptio/sonobuoy/releases/download/v0.11.6/sonobuoy_0.11.6_linux_amd64.tar.gz
        type: string
      Secure: false
      Validated: false
    sonobuoy/wait-mins:
      Available: false
      Bundle: ""
      Description: Minutes to wait for Sonobuoy run to complete
      Documentation: |
        Default is -1 so that stages do not wait for complete.

        Typical runs may take 60 minutes.

        If <0 then code does wait and assumes you will run it again to retrieve the results.
        Task is idempotent so you can re-start a run after you have started to check on results.
      Endpoint: ""
      Errors: []
      Meta:
        color: black
        icon: time
        title: Community Content
      Name: sonobuoy/wait-mins
      ReadOnly: false
      Schema:
        default: -1
        type: number
      Secure: false
      Validated: false
    vault/awskms-access-key:
      Available: false
      Bundle: ""
      Description: AWS region for AWS KMS if used with Vault for automatic unsealing
      Documentation: |
        Allows operators to specify an AWS region to be used in the Vault "awskms" seal
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: vault/awskms-access-key
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    vault/awskms-kms-key-id:
      Available: false
      Bundle: ""
      Description: AWS KMS key id if used with Vault for automatic unsealing
      Documentation: |
        Allows operators to specify an AWS KMS key ID to be used in the Vault "awskms" seal
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: semver
        title: Community Content
      Name: vault/awskms-kms-key-id
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    vault/awskms-region:
      Available: false
      Bundle: ""
      Description: AWS region for AWS KMS if used with Vault for automatic unsealing
      Documentation: |
        Allows operators to specify an AWS region to be used in the Vault "awskms" seal
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: semver
        title: Community Content
      Name: vault/awskms-region
      ReadOnly: false
      Schema:
        default: us-east-1
        type: string
      Secure: false
      Validated: false
    vault/awskms-secret-key:
      Available: false
      Bundle: ""
      Description: AWS region for AWS KMS if used with Vault for automatic unsealing
      Documentation: |
        Allows operators to specify an AWS region to be used in the Vault "awskms" seal
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: vault/awskms-secret-key
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    vault/cluster-profile:
      Available: false
      Bundle: ""
      Description: Name of the profile for this vault cluster
      Documentation: |
        Part of the Digital Rebar Cluster pattern, this parameter is used
        to identify the machines used in the vault cluster
        This parameter is REQUIRED for KRIB and vault cluster contruction
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: profile
        title: Community Content
      Name: vault/cluster-profile
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    vault/kms-plugin-token:
      Available: false
      Bundle: ""
      Description: Token to use for vault KMS plugin for secret management in Kubernetes
      Documentation: |
        Authorizes the vault-kms-plugin to communicate with vault on behalf of Kubernetes API
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: vault/kms-plugin-token
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    vault/name:
      Available: false
      Bundle: ""
      Description: Name of the vault cluster
      Documentation: |
        Allows operators to set a name for the vault cluster
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: vault/name
      ReadOnly: false
      Schema:
        default: vault-cluster
        type: string
      Secure: false
      Validated: false
    vault/root-token:
      Available: false
      Bundle: ""
      Description: The root token generated when initializing vault
      Documentation: |
        The root token generated by initializing vault. Store this somewhere secure, and delete from DRP, for confidence
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: vault/root-token
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    vault/seal:
      Available: false
      Bundle: ""
      Description: The KMS (if any) to use with Vault auto-unseal
      Documentation: |
        Vault can optionally be configured to automatically unseal, using a cloud-based KMS. Currently
        the only configured option is "awskms", which necessitates you setting the following additional parameters
        - vault/awskms-region
        - vault/awskms-access-key
        - vault/awskms-secret-key
        - vault/awskms-kms-key-id
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: vault/seal
      ReadOnly: false
      Schema:
        default: undefined
        enum:
        - undefined
        - awskms
        type: string
      Secure: false
      Validated: false
    vault/server-count:
      Available: false
      Bundle: ""
      Description: Number of vault servers to expect
      Documentation: |
        Allows operators to set the number of machines required for the
        consul cluster.
        Machines will be automatically added until the number is met.
        NOTE: should be an odd number
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Community Content
      Name: vault/server-count
      ReadOnly: false
      Schema:
        default: 1
        type: integer
      Secure: false
      Validated: false
    vault/servers:
      Available: false
      Bundle: ""
      Description: List of the server UUIDs in the vault cluster
      Documentation: |
        Param is set (output) by the vault cluster building process
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: machines-map
        title: Community Content
      Name: vault/servers
      ReadOnly: false
      Schema:
        items:
          properties:
            Address:
              type: string
            Name:
              type: string
            Uuid:
              type: string
          type: object
        type: array
      Secure: false
      Validated: false
    vault/servers-done:
      Available: false
      Bundle: ""
      Description: List of the servers in the vault cluster that are complete
      Documentation: |
        Param is set (output) by the vault cluster building process
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: machines-map
        title: Community Content
      Name: vault/servers-done
      ReadOnly: false
      Schema:
        items:
          properties:
            Address:
              type: string
            Name:
              type: string
            Uuid:
              type: string
          type: object
        type: array
      Secure: false
      Validated: false
    vault/unseal-key:
      Available: false
      Bundle: ""
      Description: The key to unseal vault in the absense of the KMS
      Documentation: |
        The key generated by initializing vault in KMS mode. Use this to unseal vault if KMS
        becomes unavailable. Store this somewhere secure, and delete from DRP, for confidence
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        password: hideme
        title: Community Content
      Name: vault/unseal-key
      ReadOnly: false
      Schema:
        type: string
      Secure: false
      Validated: false
    vault/version:
      Available: false
      Bundle: ""
      Description: Version of vault to use in cluster
      Documentation: |
        Allows operators to determine the version of consul to install
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        render: semver
        title: Community Content
      Name: vault/version
      ReadOnly: false
      Schema:
        default: 1.1.3
        type: string
      Secure: false
      Validated: false
  profiles:
    example-k8s-db-ingress:
      Available: false
      Bundle: ""
      Description: Example Profile for custom K8S Dashboard ingress - CLONE ME!
      Documentation: |
        Example Profile for custom K8S ingress.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        reset-keeps: certmanager/email,certmanager/acme-challenge-dns01-provider,certmanager/cloudflare-api-key,certmanager/cloudflare-email,ingress/k8s-dashboard-hostname
        title: Community Content
      Name: example-k8s-db-ingress
      Params:
        certmanager/acme-challenge-dns01-provider: cloudflare
        certmanager/cloudflare-api-key: BASE64-encoded-cloudflare-api-key
        certmanager/cloudflare-email: me@here.com
        certmanager/email: me@here.com
        ingress/k8s-dashboard-hostname: k8s-db.cloud.mydomain.com
      ReadOnly: false
      Validated: false
    example-krib:
      Available: false
      Bundle: ""
      Description: Example Krib Profile - No HA - CLONE ME!
      Documentation: |
        Example of the absolute minimum required Params for a non-HA
        KRIB Kubernetes cluster.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        render: krib
        reset-keeps: krib/cluster-profile,etcd/cluster-profile
        title: Community Content
      Name: example-krib
      Params:
        etcd/cluster-profile: example-krib
        krib/cluster-profile: example-krib
      ReadOnly: false
      Validated: false
    example-krib-ha:
      Available: false
      Bundle: ""
      Description: Example HA Krib Profile - Clone Me!
      Documentation: |
        Minimum required Params to set on a KRIB Kubernetes
        cluster to define a Highly Available setup.

        Clone this profile as `krib-ha` and change the VIP to your needs.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        render: krib
        reset-keeps: etcd/cluster-profile,krib/cluster-profile,etcd/server-count,krib/cluster-master-count,krib/cluster-master-vip
        title: Community Content
      Name: example-krib-ha
      Params:
        etcd/cluster-profile: krib-ha
        etcd/server-count: 3
        krib/cluster-master-count: 3
        krib/cluster-master-vip: 10.10.10.10
        krib/cluster-profile: krib-ha
      ReadOnly: false
      Validated: false
    example-rook-db-ingress:
      Available: false
      Bundle: ""
      Description: Example Profile for custom Rook Dashboard ingress - CLONE ME!
      Documentation: |
        Example Profile for custom Rook Ceph Manager Dashboard ingress.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        reset-keeps: certmanager/email,certmanager/acme-challenge-dns01-provider,certmanager/cloudflare-api-key,certmanager/cloudflare-email,ingress/rook-dashboard-hostname
        title: Community Content
      Name: example-rook-db-ingress
      Params:
        certmanager/acme-challenge-dns01-provider: cloudflare
        certmanager/cloudflare-api-key: BASE64-encoded-cloudflare-api-key
        certmanager/cloudflare-email: me@here.com
        certmanager/email: me@here.com
        ingress/rook-dashboard-hostname: rook.cloud.mydomain.com
      ReadOnly: false
      Validated: false
    helm-reference:
      Available: false
      Bundle: ""
      Description: Reference Helm Profiles (Istio, Rook, etc)
      Documentation: |
        DO NOT USE THIS PROFILE!
        Copy the contents of the helm/charts param into the Cluster!
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: grid layout
        title: Community Content
      Name: helm-reference
      Params:
        helm/charts:
        - chart: stable/mysql
          name: mysql
        - chart: istio-1.0.1/install/kubernetes/helm/istio
          kubectlafter:
          - get nodes
          kubectlbefore:
          - get nodes
          name: istio
          namespace: istio-system
          params:
            set: sidecarInjectorWebhook.enabled=true
          sleep: 10
          targz: https://github.com/istio/istio/releases/download/1.0.1/istio-1.0.1-linux.tar.gz
          wait: true
        - chart: rook-stable/rook-ceph
          kubectlafter:
          - apply -f cluster.yaml
          name: rook-ceph
          namespace: rook-ceph-system
          repos:
            rook-stable: https://charts.rook.io/stable
          templates:
            cluster: helm-rook.cfg.tmpl
          templatesafter:
          - name: helm-rook.after.sh.tmpl
          - nodes: leader
          templatesbefore:
          - name: helm-rook.before.sh.tmpl
            nodes: all
            runIfInstalled: true
          wait: true
      ReadOnly: false
      Validated: false
    krib-kubeadm-settings-example:
      Available: false
      Bundle: ""
      Description: Example 'kubeadm' config file customizations settings.
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: book
        title: Digital Rebar Community Content
      Name: krib-kubeadm-settings-example
      Params:
        docker/working-dir: /mnt/hdd/docker
        etcd/client-port: 2379
        etcd/peer-ca-pw: f00b4r
        etcd/peer-port: 2380
        etcd/server-ca-pw: m0r3_f00b4r
        etcd/server-count: 3
        etcd/servers:
        - Address: 10.10.10.101
          Name: etcd01.unspecified.domain.local
          Uuid: d155b77f-00e9-456b-9b61-f5934b45f901
        - Address: 10.10.10.102
          Name: etcd02.unspecified.domain.local
          Uuid: d155b77f-00e9-456b-9b61-f5934b45f900
        - Address: 10.10.10.103
          Name: etcd03.unspecified.domain.local
          Uuid: d155b77f-00e9-456b-9b61-f5934b45f900
        etcd/version: 3.3.8
        krib/cluster-api-port: 6443
        krib/cluster-api-vip-port: 8443
        krib/cluster-bootstrap-token: fedcba.fedcba9876543210
        krib/cluster-bootstrap-ttl: 96h0m0s
        krib/cluster-cri-socket: /var/run/dockershim.sock
        krib/cluster-dns: 10.96.0.10
        krib/cluster-domain: cluster.local
        krib/cluster-image-repository: k8s.gcr.io
        krib/cluster-kubernetes-version: v1.11.2
        krib/cluster-master-vip: 10.10.10.100
        krib/cluster-masters:
        - Address: 10.10.10.201
          Name: master01.unspecified.domain.local
          Uuid: r153b77f-00e9-200b-9b61-f5942b45f901
        - Address: 10.10.10.202
          Name: master02.unspecified.domain.local
          Uuid: d155b77f-00e9-456b-9b61-f5934b45f723
        - Address: 10.10.10.203
          Name: master03.unspecified.domain.local
          Uuid: d155b72f-10e9-550c-7b62-c5932a45a659
        krib/cluster-masters-on-etcds: false
        krib/cluster-name: tiger
        krib/cluster-pod-subnet: 10.112.0.0/12
        krib/cluster-service-dns-domain: unspecified.domain.local
        krib/cluster-service-subnet: 10.96.0.0/12
        krib/dashboard-config: https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
        kubectl/working-dir: /mnt/hdd/kubectl
        provider/calico-config: https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml
        provider/flannel-config: https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml
      ReadOnly: true
      Validated: false
    krib-operate-cordon:
      Available: false
      Bundle: ""
      Description: Contains Params that set the krib-operate-cordon Stage values.
      Documentation: |
        This profile contians the default krib-operate task parameters to drive
        a 'cordon' operation.  This profile can be added to a node or stage to
        allow the krib-operate task to do a cordon operation.

        This profile is used by the krib-cordon stage to allow a machine to
        be cordoned without changing the parameters on the machine.
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: krib-operate-cordon
      Params:
        krib/operate-action: cordon
      ReadOnly: false
      Validated: false
    krib-operate-delete:
      Available: false
      Bundle: ""
      Description: Contains Params that set the krib-operate-delete Stage values to
        DESTROY a k8s Node.
      Documentation: "This profile contains the default krib-operate task parameters
        to drive\na 'delete' operation.  This profile can be added to a node or stage
        to\nallow the krib-operate task to do a delete operation.\n\nThis profile
        is used by the krib-delete stage to allow a machine to\nbe deleted without
        changing the parameters on the machine. \n\nWARNING: This pattern destroys
        a kubernetes node.\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: krib-operate-delete
      Params:
        krib/operate-action: delete
      ReadOnly: false
      Validated: false
    krib-operate-drain:
      Available: false
      Bundle: ""
      Description: Contains Params that set the krib-operate-drain Stage values.
      Documentation: |
        This profile contians the default krib-operate task parameters to drive
        a drain operation.  This profile can be added to a node or stage to
        allow the krib-operate task to do a drain operation.

        This profile is used by the krib-drain stage to allow a machine to
        be drained without altering the parameters on the machine.
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: krib-operate-drain
      Params:
        krib/operate-action: drain
        krib/operate-options-drain: --ignore-daemonsets --delete-local-data
      ReadOnly: false
      Validated: false
    krib-operate-uncordon:
      Available: false
      Bundle: ""
      Description: Contains Params that set the krib-operate-uncordon Stage values.
      Documentation: |
        This profile contians the default krib-operate task parameters to drive
        an uncordon operation.  This profile can be added to a node or stage to
        allow the krib-operate task to do an uncordon operation.

        This profile is used by the krib-uncordon stage to allow a machine to
        be uncordoned without altering the parameters on the machine.
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: krib-operate-uncordon
      Params:
        krib/operate-action: uncordon
        krib/operate-options-drain: ""
      ReadOnly: false
      Validated: false
  stages:
    consul-agent:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Configure consul agents.
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: consul-agent
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - consul-agent-install
      - consul-agent-config
      Templates: []
      Validated: false
    consul-server:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Configure a consul server cluster.
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: consul-server
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - consul-server-install
      - consul-server-config
      Templates: []
      Validated: false
    docker-install:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Install Docker from Internet Repos
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: docker
        title: Community Content
      Name: docker-install
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - docker-install
      Templates: []
      Validated: false
    etcd-config:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Configure an etcd cluster.
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: etcd-config
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - etcd-config
      Templates: []
      Validated: false
    k3s-config:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB configure a K3s cluster master and nodes
      Documentation: |
        Designed to substitute for Kubernetes with K3s
        Installs k3s using the KRIB process and params
        with the goal of being able to use the same downstream stages
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        copyright: RackN 2019
        icon: ship
        k3s: "true"
        title: Community Content
      Name: k3s-config
      OptionalParams: []
      Params:
        krib/container-runtime: containerd
        krib/k3s: true
        krib/repo: https://github.com/rancher/k3s/releases
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - containerd-install
      - krib-get-masters
      - k3s-config
      Templates: []
      Validated: false
    krib-config:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB configure a Kubernetes cluster master and/or worker nodes
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: krib-config
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-get-masters
      - krib-config
      Templates: []
      Validated: false
    krib-contrail:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB install Contrail on the cluster
      Documentation: |
        Installs and runs Contrail kubectl install

        CURRENTLY CENTOS ONLY
        see: https://github.com/Juniper/contrail-kubernetes-docs/blob/master/install/kubernetes/standalone-kubernetes-centos.md
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: plane
        title: Community Content
      Name: krib-contrail
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-contrail
      Templates: []
      Validated: false
    krib-dev-hard-reset:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: 'DEV: Clear the Profile to Reset before test run, with no regard
        for races'
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: red
        icon: ship
        title: Community Content
      Name: krib-dev-hard-reset
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-dev-hard-reset
      Templates: []
      Validated: false
    krib-dev-reset:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: 'DEV: Clear the Profile to Reset before test run'
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: red
        icon: ship
        title: Community Content
      Name: krib-dev-reset
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-dev-reset
      Templates: []
      Validated: false
    krib-external-dns:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB installs and configures ExternalDNS on the cluster
      Documentation: |
        Installs and runs ExternalDNS
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: sitemap
        title: Community Content
      Name: krib-external-dns
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-external-dns
      Templates: []
      Validated: false
    krib-helm:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB install helm and tiller on the cluster
      Documentation: |
        Installs and runs Helm Charts after a cluster has been constructed.
        This stage is idempotent and can be run multiple times.
        This allows operators to create workflows with multiple instances of this stage.
        The charts to run are determined by the helm/charts parameter.

        Due to helm downloads, this stage requires internet access.

        This stage also creats a tiller service account.  For advanced security, this
        configuration may not be desirable.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib-helm
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-helm
      Templates: []
      Validated: false
    krib-helm-charts:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB install helm charts on the cluster
      Documentation: |
        Installs and runs Helm Charts after a cluster has been constructed.
        This stage is idempotent and can be run multiple times.
        This allows operators to create workflows with multiple instances of this stage.
        The charts to run are determined by the helm/charts parameter.

        Due to helm downloads, this stage requires internet access.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib-helm-charts
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-helm-charts
      Templates: []
      Validated: false
    krib-helm-init:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB install helm and tiller on the cluster
      Documentation: |
        This stage is idempotent and can be run multiple times.
        This allows operators to create workflows with multiple instances of this stage.
        Due to helm downloads, this stage requires internet access.

        This stage also creats a tiller service account.  For advanced security, this
        configuration may not be desirable.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib-helm-init
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-helm-init
      Templates: []
      Validated: false
    krib-ingress-nginx:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB install/config ingress-nginx and optional cert-manager
      Documentation: |
        Install/config ingress-nginx and optional cert-manager
        Requires a cloud LoadBalancer or MetalLB to provide Service ingress.ip
        must run after krib-helm stage
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: sitemap
        title: Community Content
      Name: krib-ingress-nginx
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-ingress-nginx
      Templates: []
      Validated: false
    krib-ingress-nginx-tillerless:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB install/config ingress-nginx and optional cert-manager WITHOUT
        helm or tiller
      Documentation: |
        Install/config ingress-nginx and optional cert-manager
        Requires a cloud LoadBalancer or MetalLB to provide Service ingress.ip
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: sitemap
        title: Community Content
      Name: krib-ingress-nginx-tillerless
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-ingress-nginx-tillerless
      Templates: []
      Validated: false
    krib-install-complete:
      Available: false
      BootEnv: local
      Bundle: ""
      Description: KRIB cluster install-to-disk is complete.
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: green
        icon: check circle outline
        title: KRIB Install Complete
      Name: krib-install-complete
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: true
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks: []
      Templates: []
      Validated: false
    krib-kubevirt:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB install KubeVirt.io on the cluster
      Documentation: |
        Installs KubeVirt.io using the chosen release from the cluster leader.
        This stage is idempotent and can be run multiple times.
        This allows operators to create workflows with multiple instances of this stage.

        Due to yaml and container downloads, this stage requires internet access.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: cloud
        title: Community Content
      Name: krib-kubevirt
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-kubevirt
      Templates: []
      Validated: false
    krib-live-wait:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Mark KRIB Live Boot cluster completed and wait for jobs.
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: green
        icon: hand paper
        title: KRIB Demo Content - krib-live-wait
      Name: krib-live-wait
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: false
      Tasks: []
      Templates: []
      Validated: false
    krib-logging:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB install fluent-bit on the cluster for logging
      Documentation: |
        Installs and runs fluent-bit to aggregate container logs to a graylog server
        via GELF UDP input
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: ship
        title: Community Content
      Name: krib-logging
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-logging
      Templates: []
      Validated: false
    krib-longhorn:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB installs and configures Rancher Longhorn on the cluster
      Documentation: |
        Installs and runs Rancher Longhorn kubectl install

        see: https://github.com/rancher/longhorn
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: sitemap
        title: Community Content
      Name: krib-longhorn
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-longhorn
      Templates: []
      Validated: false
    krib-metallb:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB installs and configures MetalLB on the cluster
      Documentation: |
        Installs and runs MetalLB kubectl install

        see: https://metallb.netlify.com/tutorial/
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: sitemap
        title: Community Content
      Name: krib-metallb
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-metallb
      Templates: []
      Validated: false
    krib-operate:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Operate (drain|uncordon) on a given Kubernetes node
      Documentation: |
        This stage runs an Operation (drain|uncordon) on a given KRIB
        built Kubernetes node.  You must specify action you want taken
        via the 'krib/operate-action' Param.  If nothing specified, the
        default action will be to 'drain' the node.

        In addition - you may set the following Params to alter the
        behavior of this stage:

          krib/operate-action     - action to take (drain or uncordon)
          krib/operate-on-node    - a Kubernetes node name to operate on
          krib/operate-options    - command line arguments to pass to the
                                    'kubectl' command for the action

        DRAIN NOTES: this Stage does a few things that MAY BE VERY BAD !!

        1. service pods are ignored for the drain operation
        2. --delete-local-data is used to evict pods using local storage

        Default options are '--ignore-daemonsets --delete-local-data' to
        the drain operation.  If you override these values (by setting
        'krib/operate-options') you MAY NEED to re-specify these values,
        otherwise, the Node will NOT be drained properly.

        These options may mean your data might be nuked.

        UNCORDON NODES:  typically does not require additional options
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: krib-operate
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-operate
      Templates: []
      Validated: false
    krib-operate-cordon:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Cordon operation on a given KRIB built Kubernetes node
      Documentation: |
        This stage runs a Cordon operation on a given KRIB built Kubernetes
        node.  It uses the 'krib-operate-cordon' Profile.

        In addition - you may set the following Params on a Machine object
        to override the default behaviors of this stage:

          krib/operate-action     - action to take (cordon or uncordon)
          krib/operate-on-node    - a Kubernetes node name to operate on
          krib/operate-options    - command line arguments to pass to the
                                    'kubectl' command for the action

        If the 'krib/operate-on-node' Param is empty, the node that is
        currently running the Stage will be operated on.  Otherwise,
        specifying an alternate Node allows remote cordon a node.
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: krib-operate-cordon
      OptionalParams: []
      Params: {}
      Profiles:
      - krib-operate-cordon
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-operate
      Templates: []
      Validated: false
    krib-operate-delete:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Delete Node operation on a given KRIB built Kubernetes node
      Documentation: |
        This stage runs an Delete node operation on a given KRIB built Kubernetes
        node.  It uses the 'krib-operate-delete' Profile

        In addition - you may set the following Params on a Machine object
        to override the default behaviors of this stage:

          krib/operate-action     - action to take
          krib/operate-on-node    - a Kubernetes node name to operate on
          krib/operate-options    - command line arguments to pass to the
                                    'kubectl' command for the action

        If the 'krib/operate-on-node' Param is empty, the node that is
        currently running the Stage will be operated on.  Otherwise,
        specifying an alternate Node allows remote delete a node.

        WARNING: THIS OPERATE DESTROYS A KUBERNETES NODE!

        Presumably, you want to 'krib-operate-drain' the node first to
        remove it from the cluster and drain it's workload to other cluster
        workers prior to deleting the node.
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: krib-operate-delete
      OptionalParams: []
      Params: {}
      Profiles:
      - krib-operate-delete
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-operate
      Templates: []
      Validated: false
    krib-operate-drain:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Drain operation on a given KRIB built Kubernetes node
      Documentation: |
        This stage runs an Drain operation on a given KRIB built Kubernetes
        node.  It uses the 'krib-operate-drain' Profile

        In addition - you may set the following Params on a Machine object
        to override the default behaviors of this stage:

          krib/operate-action     - action to take (drain or uncordon)
          krib/operate-on-node    - a Kubernetes node name to operate on
          krib/operate-options    - command line arguments to pass to the
                                    'kubectl' command for the action

        If the 'krib/operate-on-node' Param is empty, the node that is
        currently running the Stage will be operated on.  Otherwise,
        specifying an alternate Node allows remote draining a node.

        DRAIN NOTES: this Stage does a few things that MAY BE VERY BAD !!

        1. service pods are ignored for the drain operation
        2. --delete-local-data is used to evict pods using local storage

        Default options are '--ignore-daemonsets --delete-local-data' to
        the drain operation.  If you override these values (by setting
        'krib/operate-options') you MAY NEED to re-specify these values,
        otherwise, the Node will NOT be drained properly.

        These options may mean your data might be nuked.
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: krib-operate-drain
      OptionalParams: []
      Params: {}
      Profiles:
      - krib-operate-drain
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-operate
      Templates: []
      Validated: false
    krib-operate-uncordon:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Uncordon operation on a given KRIB built Kubernetes node
      Documentation: |
        This stage runs an Uncordon operation on a given KRIB built Kubernetes
        node.  This returns a Node back to service in a Kubernetes cluster that
        has previously been drained.  It uses the 'krib-operate-uncordon' Profile

        In addition - you may set the following Params on a Machine object
        to override the default behaviors of this stage:

          krib/operate-action     - action to take (drain or uncordon)
          krib/operate-on-node    - a Kubernetes node name to operate on
          krib/operate-options    - command line arguments to pass to the
                                    'kubectl' command for the action

        If the 'krib/operate-on-node' Param is empty, the node that is
        currently running the Stage will be operated on.  Otherwise,
        specifying an alternate Node allows remote uncordon on a node.

        Default options are '' (empty) to the uncordon operation.
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: krib-operate-uncordon
      OptionalParams: []
      Params: {}
      Profiles:
      - krib-operate-uncordon
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-operate
      Templates: []
      Validated: false
    krib-pkg-prep:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Helper stage to install prereqs prior to Kubernetes install.
      Documentation: |
        Simple helper stage to install prereq packages prior to doing
        the kubernetes package installation.  This just helps us get a
        Live Boot set of hosts (eg Sledgehammer Discovered) prepped a
        little faster with packages in some use cases.
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: krib-pkg-prep
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: false
      Tasks:
      - krib-pkg-prep
      Templates: []
      Validated: false
    krib-rook-ceph:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB installs and configures Rook Ceph on the cluster
      Documentation: |
        Installs and runs Rook Ceph install
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: sitemap
        title: Community Content
      Name: krib-rook-ceph
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-rook-ceph
      Templates: []
      Validated: false
    krib-runtime-install:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Install a container runtime from Internet Repos
      Documentation: "This stage allows for the installation of multiple container
        runtimes. The single task (krib-runtime-install)\nwhich it executes, will
        launch further tasks based on the value of krib/container-runtime. Currently
        docker\nand containerd are supported, although the design is extensible. \n"
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: docker
        title: Community Content
      Name: krib-runtime-install
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: false
      Tasks:
      - krib-runtime-install
      Templates: []
      Validated: false
    krib-set-time:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Helper stage to set time on the machine - DEV
      Documentation: Helper stage to set time on the machine - DEV
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: krib-set-time
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: false
      Tasks:
      - krib-set-time
      Templates: []
      Validated: false
    krib-sonobuoy:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: KRIB install Sonobuoy and conformance test the cluster
      Documentation: |
        Installs and runs Sonobuoy after a cluster has been constructed.
        This stage is idempotent and can be run multiple times.
        The purpose is to ensure that the KRIB cluster is conformant with standards

        If credentials are required so that the results of the run are pushed back to DRP files.

        Roadmap items:
        * eliminate need for DRPCLI credentials
        * make "am I running" detection smarter
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        icon: sound
        title: Community Content
      Name: krib-sonobuoy
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - krib-sonobuoy
      Templates: []
      Validated: false
    kubernetes-install:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Install Kubernetes and Kubeadm Packages
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: kubernetes-install
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - kubernetes-install
      Templates: []
      Validated: false
    mount-local-disks:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Mount first local disk as /docker
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: disk outline
        title: Community Content
      Name: mount-local-disks
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - erase-hard-disks-for-os-install
      - mount-disks
      Templates: []
      Validated: false
    vault-config:
      Available: false
      BootEnv: ""
      Bundle: ""
      Description: Configure a vault cluster.
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: Community Content
      Name: vault-config
      OptionalParams: []
      Params: {}
      Profiles: []
      ReadOnly: false
      Reboot: false
      RequiredParams: []
      RunnerWait: true
      Tasks:
      - vault-install
      - vault-config
      - vault-kms-plugin
      Templates: []
      Validated: false
  tasks:
    consul-agent-config:
      Available: false
      Bundle: ""
      Description: A task to configure consul servers
      Documentation: |
        Configures consul agents, to be used by Vault against a consul server cluster
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: book
        title: Community Content
      Name: consul-agent-config
      OptionalParams:
      - consul/agent-count
      - consul/version
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: consul-agent.json.tmpl
        Meta: null
        Name: Consul agent configuration
        Path: /etc/consul.d/consul-agent.json
      - Contents: ""
        ID: consul-agent-config.sh.tmpl
        Meta: null
        Name: Configure consul agent
        Path: ""
      Validated: false
    consul-agent-install:
      Available: false
      Bundle: ""
      Description: A task to install consul agent
      Documentation: |
        Installs (but not configures) consul in agent mode, to be used as an HA backend to Vault
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: book
        title: Community Content
      Name: consul-agent-install
      OptionalParams:
      - consul/agent-count
      - consul/version
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: consul-agent.service.tmpl
        Meta: null
        Name: Prepare systemd file for consul-agent
        Path: /etc/systemd/system/consul-agent.service
      - Contents: ""
        ID: consul-agent-install.sh.tmpl
        Meta: null
        Name: Configure consul agent
        Path: ""
      Validated: false
    consul-server-config:
      Available: false
      Bundle: ""
      Description: A task to configure consul servers
      Documentation: |
        Configures a consul server cluster, to be used as an HA backend to Vault
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: book
        title: Community Content
      Name: consul-server-config
      OptionalParams:
      - consul/server-count
      - consul/version
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: consul-server.json.tmpl
        Meta: null
        Name: Consul configuration
        Path: /etc/consul.d/consul-server.json
      - Contents: ""
        ID: consul-server-config.sh.tmpl
        Meta: null
        Name: Configure consul server
        Path: ""
      Validated: false
    consul-server-install:
      Available: false
      Bundle: ""
      Description: A task to install consul server
      Documentation: |
        Installs (but not configures) consul in server mode, to be used as an HA backend to Vault
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: book
        title: Community Content
      Name: consul-server-install
      OptionalParams:
      - consul/server-count
      - consul/version
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: consul-server.service.tmpl
        Meta: null
        Name: Prepare systemd file for consul-server
        Path: /etc/systemd/system/consul-server.service
      - Contents: ""
        ID: consul-server-env.sh.tmpl
        Meta: null
        Name: Prepare env file for CLI consul usage
        Path: /etc/consul/consul-server-env.sh
      - Contents: ""
        ID: consul-server-install.sh.tmpl
        Meta: null
        Name: Configure consul
        Path: ""
      Validated: false
    containerd-install:
      Available: false
      Bundle: ""
      Description: A task to install containerd
      Documentation: |
        Installs containerd using O/S packages
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: docker
        title: Community Content
      Name: containerd-install
      OptionalParams:
      - docker/working-dir
      - kubectl/working-dir
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: containerd-install.sh.tmpl
        Meta: null
        Name: Install containerd from internet repo
        Path: ""
      Validated: false
    docker-install:
      Available: false
      Bundle: ""
      Description: A task to install docker
      Documentation: |
        Installs Docker using O/S packages
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: docker
        title: Community Content
      Name: docker-install
      OptionalParams:
      - docker/working-dir
      - kubectl/working-dir
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: docker-install.sh.tmpl
        Meta: null
        Name: Install Docker from internet repo
        Path: ""
      Validated: false
    etcd-config:
      Available: false
      Bundle: ""
      Description: A task to configure etcd
      Documentation: |
        Sets Param: etcd/servers
        If installing Kubernetes via Kubeadm, make sure you install a supported version!
        This uses the Digital Rebar Cluster pattern so etcd/cluster-profile must be set
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: book
        title: Community Content
      Name: etcd-config
      OptionalParams:
      - etcd/client-ca-name
      - etcd/client-ca-pw
      - etcd/client-ca-name
      - etcd/cluster-client-vip-port
      - etcd/name
      - etcd/peer-ca-name
      - etcd/peer-ca-pw
      - etcd/peer-port
      - etcd/server-ca-name
      - etcd/server-ca-pw
      - etcd/server-count
      - etcd/version
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: etcd-config.sh.tmpl
        Meta: null
        Name: Config Etcd
        Path: ""
      Validated: false
    k3s-config:
      Available: false
      Bundle: ""
      Description: A task to configure k3s
      Documentation: |
        Sets Param: krib/cluster-join, krib/cluster-admin-conf
        Configure K3s using built-in commands
        This uses the Digital Rebar Cluster pattern so krib/cluster-profile must be set

        Server is setup to also be an agent - all machines have workload

        WARNING: Must NOT set etcd/cluster-profile when install k3s1
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        copyright: RackN 2019
        feature-flags: sane-exit-codes
        icon: ship
        k3s: "true"
        title: Community Content
      Name: k3s-config
      OptionalParams: []
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      Templates:
      - Contents: ""
        ID: k3s-config.sh.tmpl
        Meta: null
        Name: Config K3s
        Path: ""
      Validated: false
    krib-config:
      Available: false
      Bundle: ""
      Description: A task to configure kubernetes via kubeadm
      Documentation: |
        Sets Param: krib/cluster-join, krib/cluster-admin-conf
        Configure Kubernetes using Kubeadm
        This uses the Digital Rebar Cluster pattern so krib/cluster-profile must be set
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-config
      OptionalParams:
      - krib/cluster-name
      - krib/cluster-is-production
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      Templates:
      - Contents: ""
        ID: krib-kubeadm.cfg.tmpl
        Meta: null
        Name: Kubeadm config file
        Path: /tmp/kubeadm.cfg
      - Contents: ""
        ID: krib-kubelet-rubber-stamp.yaml.tmpl
        Meta: null
        Name: Manifests for kubelet rubber stamp operator
        Path: /tmp/krib-kubelet-rubber-stamp.yaml
      - Contents: ""
        ID: krib-config.sh.tmpl
        Meta: null
        Name: Config Kubernetes via Kubeadm
        Path: ""
      Validated: false
    krib-contrail:
      Available: false
      Bundle: ""
      Description: A task to install Contrail
      Documentation: |
        Installs Contrail via kubectl from the contrail.cfg template.
        Runs on the master only.
        Template replies on the cluster VIP as the master IP address
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: plane
        title: Community Content
      Name: krib-contrail
      OptionalParams: null
      Prerequisites: []
      ReadOnly: false
      RequiredParams: null
      Templates:
      - Contents: ""
        ID: contrail.cfg.tmpl
        Meta: null
        Name: Contrail Config
        Path: contrail-single-step-cni-install-centos.yaml
      - Contents: ""
        ID: krib-contrail.sh.tmpl
        Meta: null
        Name: kubectl Contrail
        Path: ""
      Validated: false
    krib-dashboard:
      Available: false
      Bundle: ""
      Description: A task to setup dashboard
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-dashboard
      OptionalParams: []
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: krib-dashboard.sh.tmpl
        Meta: null
        Name: Setup Dashboard
        Path: ""
      Validated: false
    krib-dev-hard-reset:
      Available: false
      Bundle: ""
      Description: 'DEV: Reset Profile values between Test Runs, ignoring races'
      Documentation: |
        Clears Created Params: krib/*, etcd/*
      Endpoint: ""
      Errors: []
      Meta:
        color: orange
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-dev-hard-reset
      OptionalParams:
      - krib/cluster-is-production
      - unsafe/rs-username
      - unsafe/rs-password
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      - etcd/cluster-profile
      Templates:
      - Contents: "#!/bin/bash\n# Reset etcd CA params\nset -e\n\ndeclare -a WIPE_PARAMS=('etcd/client-ca-name'
          'etcd/client-ca-pw' 'etcd/peer-ca-name' 'etcd/peer-ca-pw' 'etcd/server-ca-name'
          'etcd/server-ca-pw' 'etcd/server-ca-cert' 'etcd/controller-client-cert'
          'etcd/controller-client-key')\n{{if .ParamExists \"etcd/cluster-profile\"
          -}}\nCLUSTER_PROFILE={{.Param \"etcd/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken
          (.Param \"etcd/cluster-profile\") 7200}}\n{{else -}}\necho \"Missing etcd/cluster-profile
          on the machine!\"\nexit 1\n{{end -}}\n{{template \"setup.tmpl\" .}}\necho
          \"Checking CA\"\necho \"drpcli machines runaction $RS_UUID getca certs/root
          {{.Param \"etcd/name\"}}-server-ca\"\nCA_TEST=$(drpcli machines runaction
          $RS_UUID getca certs/root {{.Param \"etcd/name\"}}-server-ca) || true\nif
          [[ $CA_TEST != \"\" && $CA_TEST != null ]] ; then\n  echo \"CA contains
          $CA_TEST\"\n  echo \"HALTING!!! YOU MUST RESET THE CA BY HAND!\"  Run:\n
          \ echo \"===========================================================================\"\n
          \ echo \"drpcli plugins runaction certs deleteroot certs/root {{.Param \"etcd/name\"}}-client-ca\"\n
          \ echo \"drpcli plugins runaction certs deleteroot certs/root {{.Param \"etcd/name\"}}-server-ca\"\n
          \ echo \"drpcli plugins runaction certs deleteroot certs/root {{.Param \"etcd/name\"}}-peer-ca\"
          \    \n  echo \"===========================================================================\"\n
          \ echo \"Try to add unsafe/rs-username with value: <your-non-default-username>,
          and unsafe/rs-password with value: <your-non-default-password>, on your
          profile and re-run the krib-hard-reset-cluster workflow\"\n  \n  sleep 1\n
          \ {{if .ParamExists \"unsafe/rs-username\"}}\n    USERNAME=\"{{.Param \"unsafe/rs-username\"}}\"\n
          \ {{else}}\n    USERNAME=\"rocketskates\"\n  {{end}}          \n  {{if .ParamExists
          \"unsafe/rs-password\"}}\n    PASSWORD=\"{{.Param \"unsafe/rs-password\"}}\"\n
          \ {{else}}\n    PASSWORD=\"r0cketsk8ts\"\n  {{end}}\n\n  HOLD_TOKEN=$RS_TOKEN\n
          \ unset RS_TOKEN\n  drpcli -U \"$USERNAME\" -P \"$PASSWORD\" plugins runaction
          certs deleteroot certs/root {{.Param \"etcd/name\"}}-client-ca\n  drpcli
          -U \"$USERNAME\" -P \"$PASSWORD\" plugins runaction certs deleteroot certs/root
          {{.Param \"etcd/name\"}}-server-ca\n  drpcli -U \"$USERNAME\" -P \"$PASSWORD\"
          plugins runaction certs deleteroot certs/root {{.Param \"etcd/name\"}}-peer-ca\n
          \ export RS_TOKEN=$HOLD_TOKEN\nelse\n  echo \"  No CA root detected - no
          reset required\"\nfi\n{{template \"krib-dev-reset.sh.tmpl\" .}}\necho \"done
          CA params reset\""
        ID: ""
        Meta: null
        Name: etcd CA clear
        Path: ""
      - Contents: |-
          #!/bin/bash
          # Reset etcd params
          set -e

          declare -a WIPE_PARAMS=('etcd/servers' 'etcd/servers-done')
          declare CLUSTER_TYPE="etcd"
          {{template "setup.tmpl" .}}
          {{if .ParamExists "etcd/cluster-profile" -}}
          CLUSTER_PROFILE={{.Param "etcd/cluster-profile"}}
          PROFILE_TOKEN={{.GenerateProfileToken (.Param "etcd/cluster-profile") 7200}}
          {{else -}}
          echo "Missing etcd/cluster-profile on the machine!"
          exit 1
          {{end -}}
          {{template "krib-dev-reset.sh.tmpl" .}}
          echo "done etcd params reset"
        ID: ""
        Meta: null
        Name: etcd-reset
        Path: ""
      - Contents: "#!/bin/bash\n# Reset consul CA params\nset -e\n\ndeclare -a WIPE_PARAMS=('consul/server-ca-name'
          'consul/server-ca-pw' 'consul/server-ca-cert' 'consul/controller-client-cert'
          'consul/controller-client-key')\n{{if .ParamExists \"consul/cluster-profile\"
          -}}\nCLUSTER_PROFILE={{.Param \"consul/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken
          (.Param \"consul/cluster-profile\") 7200}}\n{{else -}}\necho \"Missing consul/cluster-profile
          on the machine!\"\nexit 1\n{{end -}}\n{{template \"setup.tmpl\" .}}\necho
          \"Checking CA\"\necho \"drpcli machines runaction $RS_UUID getca certs/root
          {{.Param \"consul/name\"}}-ca\"\nCA_TEST=$(drpcli machines runaction $RS_UUID
          getca certs/root {{.Param \"consul/name\"}}-ca) || true\nif [[ $CA_TEST
          != \"\" && $CA_TEST != null ]] ; then\n  echo \"CA contains $CA_TEST\"\n
          \ echo \"HALTING!!! YOU MUST RESET THE CA BY HAND!\"  Run:\n  echo \"===========================================================================\"\n
          \ echo \"drpcli plugins runaction certs deleteroot certs/root {{.Param \"consul/name\"}}-ca\"
          \           \n  echo \"===========================================================================\"\n
          \ echo \"Try to add unsafe/rs-username with value: <your-non-default-username>,
          and unsafe/rs-password with value: <your-non-default-password>, on your
          profile and re-run the krib-hard-reset-cluster workflow\"\n  \n  sleep 1\n
          \ {{if .ParamExists \"unsafe/rs-username\"}}\n    USERNAME=\"{{.Param \"unsafe/rs-username\"}}\"\n
          \ {{else}}\n    USERNAME=\"rocketskates\"\n  {{end}}          \n  {{if .ParamExists
          \"unsafe/rs-password\"}}\n    PASSWORD=\"{{.Param \"unsafe/rs-password\"}}\"\n
          \ {{else}}\n    PASSWORD=\"r0cketsk8ts\"\n  {{end}}\n\n  HOLD_TOKEN=$RS_TOKEN\n
          \ unset RS_TOKEN\n  drpcli -U \"$USERNAME\" -P \"$PASSWORD\" plugins runaction
          certs deleteroot certs/root {{.Param \"consul/name\"}}-ca            \n
          \ export RS_TOKEN=$HOLD_TOKEN\nelse\n  echo \"  No CA root detected - no
          reset required\"\nfi\n{{template \"krib-dev-reset.sh.tmpl\" .}}\necho \"done
          consul CA params reset\""
        ID: ""
        Meta: null
        Name: consul CA clear
        Path: ""
      - Contents: |-
          #!/bin/bash
          # Reset consul params
          set -e

          declare -a WIPE_PARAMS=('consul/servers' 'consul/servers-done' 'consul/agents' 'consul/agents-done' 'consul/encryption-key')
          declare CLUSTER_TYPE="consul"
          {{template "setup.tmpl" .}}
          {{if .ParamExists "consul/cluster-profile" -}}
          CLUSTER_PROFILE={{.Param "consul/cluster-profile"}}
          PROFILE_TOKEN={{.GenerateProfileToken (.Param "consul/cluster-profile") 7200}}
          {{else -}}
          echo "Missing consul/cluster-profile on the machine!"
          exit 1
          {{end -}}
          {{template "krib-dev-reset.sh.tmpl" .}}
          echo "done consul params reset"
        ID: ""
        Meta: null
        Name: consul-reset
        Path: ""
      - Contents: "#!/bin/bash\n# Reset vault CA params\nset -e\n\ndeclare -a WIPE_PARAMS=('vault/server-ca-name'
          'vault/server-ca-pw' 'vault/server-ca-cert')\n{{if .ParamExists \"vault/cluster-profile\"
          -}}\nCLUSTER_PROFILE={{.Param \"vault/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken
          (.Param \"vault/cluster-profile\") 7200}}\n{{else -}}\necho \"Missing vault/cluster-profile
          on the machine!\"\nexit 1\n{{end -}}\n{{template \"setup.tmpl\" .}}\necho
          \"Checking CA\"\necho \"drpcli machines runaction $RS_UUID getca certs/root
          {{.Param \"vault/name\"}}-ca\"\nCA_TEST=$(drpcli machines runaction $RS_UUID
          getca certs/root {{.Param \"vault/name\"}}-ca) || true\nif [[ $CA_TEST !=
          \"\" && $CA_TEST != null ]] ; then\n  echo \"CA contains $CA_TEST\"\n  echo
          \"HALTING!!! YOU MUST RESET THE CA BY HAND!\"  Run:\n  echo \"===========================================================================\"\n
          \ echo \"drpcli plugins runaction certs deleteroot certs/root {{.Param \"vault/name\"}}-ca\"
          \           \n  echo \"===========================================================================\"\n
          \ echo \"Try to add unsafe/rs-username with value: <your-non-default-username>,
          and unsafe/rs-password with value: <your-non-default-password>, on your
          profile and re-run the krib-hard-reset-cluster workflow\"\n  \n  sleep 1\n
          \ {{if .ParamExists \"unsafe/rs-username\"}}\n    USERNAME=\"{{.Param \"unsafe/rs-username\"}}\"\n
          \ {{else}}\n    USERNAME=\"rocketskates\"\n  {{end}}          \n  {{if .ParamExists
          \"unsafe/rs-password\"}}\n    PASSWORD=\"{{.Param \"unsafe/rs-password\"}}\"\n
          \ {{else}}\n    PASSWORD=\"r0cketsk8ts\"\n  {{end}}\n\n  HOLD_TOKEN=$RS_TOKEN\n
          \ unset RS_TOKEN\n  drpcli -U \"$USERNAME\" -P \"$PASSWORD\" plugins runaction
          certs deleteroot certs/root {{.Param \"vault/name\"}}-ca            \n  export
          RS_TOKEN=$HOLD_TOKEN\nelse\n  echo \"  No CA root detected - no reset required\"\nfi\n{{template
          \"krib-dev-reset.sh.tmpl\" .}}\necho \"done CA params reset\""
        ID: ""
        Meta: null
        Name: vault CA clear
        Path: ""
      - Contents: |-
          #!/bin/bash
          # Reset vault params
          set -e

          declare -a WIPE_PARAMS=('vault/unseal-key' 'vault/root-token' 'vault/kms-plugin-token' 'vault/servers' 'vault/servers-done' 'vault/server-ca-pw' 'vault/server-ca-name')
          declare CLUSTER_TYPE="vault"
          {{template "setup.tmpl" .}}
          {{if .ParamExists "krib/cluster-profile" -}}
          CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
          PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
          {{else -}}
          echo "Missing krib/cluster-profile on the machine!"
          exit 1
          {{end -}}
          {{template "krib-dev-reset.sh.tmpl" .}}
          echo "done vault params reset"
        ID: ""
        Meta: null
        Name: vault-reset
        Path: ""
      - Contents: |-
          #!/bin/bash
          # Reset KRIB params
          set -e

          declare -a WIPE_PARAMS=('krib/cluster-masters' 'krib/cluster-join-command' 'krib/cluster-admin-conf' 'krib/cluster-master-certs' 'krib/cluster-bootstrap-token' 'krib/cluster-kubeadm-cfg')
          {{template "setup.tmpl" .}}
          {{if .ParamExists "krib/cluster-profile" -}}
          CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
          PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
          {{else -}}
          echo "Missing krib/cluster-profile on the machine!"
          exit 1
          {{end -}}
          {{template "krib-dev-reset.sh.tmpl" .}}
          drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"black\", \"icon\": \"server\"}}" | jq .Meta
          echo "done KRIB params reset"
        ID: ""
        Meta: null
        Name: KRIB clear
        Path: ""
      - Contents: "#!/bin/bash\n\n# We might want to re-initialize a cluster using
          this machine, but without any\n# remanants of the old cluster. This script
          brutally removes any cluster-specific content,\n# and it's expected that
          you'll reinstall docker/containerd and kubernetes before rebuilding\n\nkubeadm
          reset -f\nsystemctl stop etcd || echo \"I'm not a master, don't need to
          stop etcd\"\nsystemctl stop vault-kms-plugin || echo \"vault-kms-plugin
          not running, no need to stop\"\nsystemctl stop vault || echo \"vault not
          running, no need to stop\"\nsystemctl stop consul-server || echo \"consul-server
          not running, no need to stop\"\nsystemctl stop consul-agent || echo \"consul-agent
          not running, no need to stop\"\n\n# Whether we're using containerd or dockerd,
          kill either\npkill containerd || pkill dockerd\n\n# Wipe out any persistent
          content\nrm -rf /var/lib/cni/\nrm -rf /var/lib/kubelet/*\nrm -rf /etc/cni/\nrm
          -rf /docker/etcd/*\nrm -rf /var/lib/containerd/*\nrm -rf /var/lib/docker/*
          \  \nrm -rf /var/lib/rook/*    \nrm -rf /var/lib/consul/client\nrm -rf /var/lib/consul/server
          \   \nrm -rf /var/log/containers/* || echo \"/var/log/containers doesn't
          exist, possibly we weren't using containerd\"\niptables -F && iptables -t
          nat -F && iptables -t mangle -F && iptables -X\n\n{{ if .ParamExists \"rook/ceph-target-disk\"}}\n{{if
          .ParamExists \"krib/selective-mastership\" -}}\nSELECTIVE_MASTERSHIP={{.Param
          \"krib/selective-mastership\" }}\n{{end -}}\n{{if .ParamExists \"krib/i-am-master\"
          -}}\nI_AM_MASTER={{.Param \"krib/i-am-master\" }}\n{{end -}}\nif [[ \"$SELECTIVE_MASTERSHIP\"
          == true ]] ; then\n  if [[ ! \"$I_AM_MASTER\" == true ]] ; then\n    # I'm
          a worker, so wipe disk in preparation for next ceph installation\n    yum
          install -y gdisk\n    sgdisk --zap-all /dev/{{ .Param \"rook/ceph-target-disk\"
          }}\n    # Now remove devicemapper details\n    for i in `ls /dev/mapper/ceph*`;
          do dmsetup remove $i || echo \"Failed to remove but failing gracefully to
          continue script\"; done\n  fi\nfi\n{{ end -}}"
        ID: ""
        Meta: null
        Name: KRIB wipe machine for cluster reinstall
        Path: ""
      - Contents: |-
          #!/bin/bash

          {{ if .ParamExists "rook/ceph-target-disk"}}
          {{if .ParamExists "krib/selective-mastership" -}}
          SELECTIVE_MASTERSHIP={{.Param "krib/selective-mastership" }}
          {{end -}}
          {{if .ParamExists "krib/i-am-master" -}}
          I_AM_MASTER={{.Param "krib/i-am-master" }}
          {{end -}}
          if [[ "$SELECTIVE_MASTERSHIP" == true ]] ; then
            if [[ ! "$I_AM_MASTER" == true ]] ; then
              # I'm a worker, so wipe disk in preparation for next ceph installation
              yum install -y gdisk
              sgdisk --zap-all /dev/{{ .Param "rook/ceph-target-disk" }}
              # Now remove devicemapper details
              for i in `ls /dev/mapper/ceph*`; do dmsetup remove $i || echo "Failed to remove but failing gracefully to continue script"; done
            fi
          fi
          {{ end -}}
        ID: ""
        Meta: null
        Name: KRIB wipe disks used for rook-ceph (if configured)
        Path: ""
      Validated: false
    krib-dev-reset:
      Available: false
      Bundle: ""
      Description: 'DEV: Reset Profile values between Test Runs'
      Documentation: |
        Clears Created Params: krib/*, etcd/*
      Endpoint: ""
      Errors: []
      Meta:
        color: orange
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-dev-reset
      OptionalParams:
      - krib/cluster-is-production
      - unsafe/rs-username
      - unsafe/rs-password
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      - etcd/cluster-profile
      Templates:
      - Contents: "#!/bin/bash\n# Reset etcd CA params\nset -e\n\n{{template \"setup.tmpl\"
          .}}\nexport RS_IP=\"{{.Machine.Address}}\"\n\n{{if .ParamExists \"etcd/cluster-profile\"
          -}}\nCLUSTER_PROFILE={{.Param \"etcd/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken
          (.Param \"etcd/cluster-profile\") 7200}}\n# This only needs to happen on
          the cluster master, so skip for non master[0]s\n{{template \"krib-lib.sh.tmpl\"
          .}}\nMASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM \"Uuid\" $RS_UUID)\nif [[
          ! $MASTER_INDEX == 0 ]] ; then\n  echo \"I am not master[0], so skipping
          DRP interaction (to avoid unnecessary races)\"\n  exit 0\nfi\n{{else -}}\necho
          \"Missing etcd/cluster-profile on the machine! Nothing to clear!\"\nexit
          0\n{{end -}}\n\necho \"Checking CA\"\necho \"drpcli machines runaction $RS_UUID
          getca certs/root {{.Param \"etcd/name\"}}-server-ca\"\nCA_TEST=$(drpcli
          machines runaction $RS_UUID getca certs/root {{.Param \"etcd/name\"}}-server-ca)
          || true\nif [[ $CA_TEST != \"\" && $CA_TEST != null ]] ; then\n  echo \"CA
          contains $CA_TEST\"\n  echo \"HALTING!!! YOU MUST RESET THE CA BY HAND!\"
          \ Run:\n  echo \"===========================================================================\"\n
          \ echo \"drpcli plugins runaction certs deleteroot certs/root {{.Param \"etcd/name\"}}-client-ca\"\n
          \ echo \"drpcli plugins runaction certs deleteroot certs/root {{.Param \"etcd/name\"}}-server-ca\"\n
          \ echo \"drpcli plugins runaction certs deleteroot certs/root {{.Param \"etcd/name\"}}-peer-ca\"\n
          \ echo \"===========================================================================\"\n
          \ echo \"Try to add unsafe/rs-username with value: <your-non-default-username>,
          and unsafe/rs-password with value: <your-non-default-password>, on your
          profile and re-run the krib-reset-cluster workflow\"\n\n  sleep 1\n  {{if
          .ParamExists \"unsafe/rs-username\"}}\n    USERNAME=\"{{.Param \"unsafe/rs-username\"}}\"\n
          \ {{else}}\n    USERNAME=\"rocketskates\"\n  {{end}}          \n  {{if .ParamExists
          \"unsafe/rs-password\"}}\n    PASSWORD=\"{{.Param \"unsafe/rs-password\"}}\"\n
          \ {{else}}\n    PASSWORD=\"r0cketsk8ts\"\n  {{end}}\n  HOLD_TOKEN=$RS_TOKEN\n
          \ unset RS_TOKEN\n  drpcli -U \"$USERNAME\" -P \"$PASSWORD\" plugins runaction
          certs deleteroot certs/root {{.Param \"etcd/name\"}}-client-ca\n  drpcli
          -U \"$USERNAME\" -P \"$PASSWORD\" plugins runaction certs deleteroot certs/root
          {{.Param \"etcd/name\"}}-server-ca\n  drpcli -U \"$USERNAME\" -P \"$PASSWORD\"
          plugins runaction certs deleteroot certs/root {{.Param \"etcd/name\"}}-peer-ca\n
          \ export RS_TOKEN=$HOLD_TOKEN\nelse\n  echo \"  No CA root detected - no
          reset required\"\nfi\ndeclare -a WIPE_PARAMS=('etcd/client-ca-name' 'etcd/client-ca-pw'
          'etcd/peer-ca-name' 'etcd/peer-ca-pw' 'etcd/server-ca-name' 'etcd/server-ca-pw'
          'etcd/server-ca-cert' 'etcd/controller-client-cert' 'etcd/controller-client-key')\n{{template
          \"krib-dev-reset.sh.tmpl\" .}}\necho \"done CA params reset\""
        ID: ""
        Meta: null
        Name: etcd CA clear
        Path: ""
      - Contents: |-
          #!/bin/bash
          # Reset etcd params
          set -e

          declare CLUSTER_TYPE="etcd"
          {{template "setup.tmpl" .}}
          {{if .ParamExists "etcd/cluster-profile" -}}
          CLUSTER_PROFILE={{.Param "etcd/cluster-profile"}}
          PROFILE_TOKEN={{.GenerateProfileToken (.Param "etcd/cluster-profile") 7200}}
          # This only needs to happen on the cluster master, so skip for non master[0]s
          {{template "krib-lib.sh.tmpl" .}}
          MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
          if [[ ! $MASTER_INDEX == 0 ]] ; then
            echo "I am not master[0], so skipping DRP interaction (to avoid unnecessary races)"
            exit 0
          fi
          {{else -}}
          echo "Missing etcd/cluster-profile on the machine! Nothing to clear!"
          exit 0
          {{end -}}
          declare -a WIPE_PARAMS=('etcd/servers' 'etcd/servers-done')
          {{template "krib-dev-reset.sh.tmpl" .}}
          echo "done etcd params reset"
        ID: ""
        Meta: null
        Name: etcd-reset
        Path: ""
      - Contents: "#!/bin/bash\n# Reset consul CA params\nset -e\n\n{{template \"setup.tmpl\"
          .}}\nexport RS_IP=\"{{.Machine.Address}}\"\n{{if .ParamExists \"consul/cluster-profile\"
          -}}\nCLUSTER_PROFILE={{.Param \"consul/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken
          (.Param \"consul/cluster-profile\") 7200}}\n# This only needs to happen
          on the cluster master, so skip for non master[0]s\n{{template \"krib-lib.sh.tmpl\"
          .}}\nMASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM \"Uuid\" $RS_UUID)\nif [[
          ! $MASTER_INDEX == 0 ]] ; then\n  echo \"I am not master[0], so skipping
          DRP interaction (to avoid unnecessary races)\"\n  exit 0\nfi\n{{else -}}\necho
          \"Missing consul/cluster-profile on the machine! Nothing to clear.\"\nexit
          0\n{{end -}}\necho \"Checking CA\"\necho \"drpcli machines runaction $RS_UUID
          getca certs/root {{.Param \"consul/name\"}}-ca\"\nCA_TEST=$(drpcli machines
          runaction $RS_UUID getca certs/root {{.Param \"consul/name\"}}-ca) || true\nif
          [[ $CA_TEST != \"\" && $CA_TEST != null ]] ; then\n  echo \"CA contains
          $CA_TEST\"\n  echo \"HALTING!!! YOU MUST RESET THE CA BY HAND!\"  Run:\n
          \ echo \"===========================================================================\"\n
          \ echo \"drpcli plugins runaction certs deleteroot certs/root {{.Param \"consul/name\"}}-ca\"\n
          \ echo \"===========================================================================\"\n
          \ echo \"Try to add unsafe/rs-username with value: <your-non-default-username>,
          and unsafe/rs-password with value: <your-non-default-password>, on your
          profile and re-run the krib-reset-cluster workflow\"\n\n  sleep 1\n  {{if
          .ParamExists \"unsafe/rs-username\"}}\n    USERNAME=\"{{.Param \"unsafe/rs-username\"}}\"\n
          \ {{else}}\n    USERNAME=\"rocketskates\"\n  {{end}}          \n  {{if .ParamExists
          \"unsafe/rs-password\"}}\n    PASSWORD=\"{{.Param \"unsafe/rs-password\"}}\"\n
          \ {{else}}\n    PASSWORD=\"r0cketsk8ts\"\n  {{end}}\n\n  HOLD_TOKEN=$RS_TOKEN\n
          \ unset RS_TOKEN\n  drpcli -U \"$USERNAME\" -P \"$PASSWORD\" plugins runaction
          certs deleteroot certs/root {{.Param \"consul/name\"}}-ca\n  export RS_TOKEN=$HOLD_TOKEN\nelse\n
          \ echo \"  No CA root detected - no reset required\"\nfi\ndeclare -a WIPE_PARAMS=('consul/server-ca-name'
          'consul/server-ca-pw' 'consul/server-ca-cert' 'consul/controller-client-cert'
          'consul/controller-client-key')\n{{template \"krib-dev-reset.sh.tmpl\" .}}\necho
          \"done consul CA params reset\""
        ID: ""
        Meta: null
        Name: consul CA clear
        Path: ""
      - Contents: |-
          #!/bin/bash
          # Reset consul params
          set -e

          {{template "setup.tmpl" .}}
          export RS_IP="{{.Machine.Address}}"
          {{if .ParamExists "consul/cluster-profile" -}}
          CLUSTER_PROFILE={{.Param "consul/cluster-profile"}}
          PROFILE_TOKEN={{.GenerateProfileToken (.Param "consul/cluster-profile") 7200}}
          # This only needs to happen on the cluster master, so skip for non master[0]s
          {{template "krib-lib.sh.tmpl" .}}
          MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
          if [[ ! $MASTER_INDEX == 0 ]] ; then
            echo "I am not master[0], so skipping DRP interaction (to avoid unnecessary races)"
            exit 0
          fi
          declare CLUSTER_TYPE="consul"
          {{else -}}
          echo "Missing consul/cluster-profile on the machine! Nothing to clear!"
          exit 0
          {{end -}}
          declare -a WIPE_PARAMS=('consul/servers' 'consul/servers-done' 'consul/agents' 'consul/agents-done' 'consul/encryption-key')
          {{template "krib-dev-reset.sh.tmpl" .}}
          echo "done consul params reset"
        ID: ""
        Meta: null
        Name: consul-reset
        Path: ""
      - Contents: "#!/bin/bash\n# Reset vault CA params\nset -e\n\n{{template \"setup.tmpl\"
          .}}\nexport RS_IP=\"{{.Machine.Address}}\"\n{{if .ParamExists \"vault/cluster-profile\"
          -}}\nCLUSTER_PROFILE={{.Param \"vault/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken
          (.Param \"vault/cluster-profile\") 7200}}\n# This only needs to happen on
          the cluster master, so skip for non master[0]s\n{{template \"krib-lib.sh.tmpl\"
          .}}\nMASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM \"Uuid\" $RS_UUID)\nif [[
          ! $MASTER_INDEX == 0 ]] ; then\n  echo \"I am not master[0], so skipping
          DRP interaction (to avoid unnecessary races)\"\n  exit 0\nfi\n\n{{else -}}\necho
          \"Missing vault/cluster-profile on the machine! Nothing to clear!\"\nexit
          0\n{{end -}}\n{{template \"setup.tmpl\" .}}\necho \"Checking CA\"\necho
          \"drpcli machines runaction $RS_UUID getca certs/root {{.Param \"vault/name\"}}-ca\"\nCA_TEST=$(drpcli
          machines runaction $RS_UUID getca certs/root {{.Param \"vault/name\"}}-ca)
          || true\nif [[ $CA_TEST != \"\" && $CA_TEST != null ]] ; then\n  echo \"CA
          contains $CA_TEST\"\n  echo \"HALTING!!! YOU MUST RESET THE CA BY HAND!\"
          \ Run:\n  echo \"===========================================================================\"\n
          \ echo \"drpcli plugins runaction certs deleteroot certs/root {{.Param \"vault/name\"}}-ca\"\n
          \ echo \"===========================================================================\"\n
          \ echo \"Try to add unsafe/rs-username with value: <your-non-default-username>,
          and unsafe/rs-password with value: <your-non-default-password>, on your
          profile and re-run the krib-reset-cluster workflow\"\n\n  sleep 1\n  {{if
          .ParamExists \"unsafe/rs-username\"}}\n    USERNAME=\"{{.Param \"unsafe/rs-username\"}}\"\n
          \ {{else}}\n    USERNAME=\"rocketskates\"\n  {{end}}          \n  {{if .ParamExists
          \"unsafe/rs-password\"}}\n    PASSWORD=\"{{.Param \"unsafe/rs-password\"}}\"\n
          \ {{else}}\n    PASSWORD=\"r0cketsk8ts\"\n  {{end}}\n  HOLD_TOKEN=$RS_TOKEN\n
          \ unset RS_TOKEN\n  drpcli -U \"$USERNAME\" -P \"$PASSWORD\" plugins runaction
          certs deleteroot certs/root {{.Param \"vault/name\"}}-ca\n  export RS_TOKEN=$HOLD_TOKEN\nelse\n
          \ echo \"  No CA root detected - no reset required\"\nfi\ndeclare -a WIPE_PARAMS=('vault/server-ca-name'
          'vault/server-ca-pw' 'vault/server-ca-cert')\n{{template \"krib-dev-reset.sh.tmpl\"
          .}}\necho \"done CA params reset\""
        ID: ""
        Meta: null
        Name: vault CA clear
        Path: ""
      - Contents: |-
          #!/bin/bash
          # Reset vault params
          set -e

          # This only needs to happen on the cluster master, so skip for non master[0]s
          {{template "setup.tmpl" .}}
          export RS_IP="{{.Machine.Address}}"
          declare CLUSTER_TYPE="vault"
          {{if .ParamExists "krib/cluster-profile" -}}
          CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
          PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
          {{template "krib-lib.sh.tmpl" .}}
          MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
          if [[ ! $MASTER_INDEX == 0 ]] ; then
            echo "I am not master[0], so skipping DRP interaction (to avoid unnecessary races)"
            exit 0
          fi

          {{else -}}
          echo "Missing krib/cluster-profile on the machine! Nothing to clear!"
          exit 0
          {{end -}}
          declare -a WIPE_PARAMS=('vault/unseal-key' 'vault/root-token' 'vault/kms-plugin-token' 'vault/servers' 'vault/servers-done' 'vault/server-ca-pw' 'vault/server-ca-name')
          {{template "krib-dev-reset.sh.tmpl" .}}
          echo "done vault params reset"
        ID: ""
        Meta: null
        Name: vault-reset
        Path: ""
      - Contents: |-
          #!/bin/bash
          # Reset KRIB params
          set -e

          {{template "setup.tmpl" .}}
          export RS_IP="{{.Machine.Address}}"
          {{if .ParamExists "krib/cluster-profile" -}}
          CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
          PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
          # This only needs to happen on the cluster master, so skip for non master[0]s
          {{template "krib-lib.sh.tmpl" .}}
          MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
          if [[ ! $MASTER_INDEX == 0 ]] ; then
            echo "I am not master[0], so skipping DRP interaction (to avoid unnecessary races)"
            exit 0
          fi

          {{else -}}
          echo "Missing krib/cluster-profile on the machine! Nothing to clear!"
          exit 0
          {{end -}}
          declare -a WIPE_PARAMS=('krib/cluster-masters' 'krib/cluster-join-command' 'krib/cluster-admin-conf' 'krib/cluster-master-certs' 'krib/cluster-bootstrap-token' 'krib/cluster-kubeadm-cfg')
          {{template "krib-dev-reset.sh.tmpl" .}}
          drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"black\", \"icon\": \"server\"}}" | jq .Meta
          echo "done KRIB params reset"
        ID: ""
        Meta: null
        Name: KRIB clear
        Path: ""
      - Contents: |-
          #!/bin/bash

          # We might want to re-initialize a cluster using this machine, but without any
          # remanants of the old cluster. This script brutally removes any cluster-specific content,
          # and it's expected that you'll reinstall docker/containerd and kubernetes before rebuilding

          kubeadm reset -f
          systemctl stop etcd || echo "I'm not a master, don't need to stop etcd"
          systemctl stop vault-kms-plugin || echo "vault-kms-plugin not running, no need to stop"
          systemctl stop vault || echo "vault not running, no need to stop"
          systemctl stop consul-server || echo "consul-server not running, no need to stop"
          systemctl stop consul-agent || echo "consul-agent not running, no need to stop"

          # Whether we're using containerd or dockerd, kill either
          pkill containerd || pkill dockerd

          # Wipe out any persistent content
          rm -rf /var/lib/cni/
          rm -rf /var/lib/kubelet/*
          rm -rf /etc/cni/
          rm -rf /docker/etcd/*
          rm -rf /var/lib/containerd/*
          rm -rf /var/lib/docker/*
          rm -rf /var/lib/rook/*
          rm -rf /var/lib/consul/client
          rm -rf /var/lib/consul/server
          rm -rf /var/log/containers/* || echo "/var/log/containers doesn't exist, possibly we weren't using containerd"
          iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X

          {{ if .ParamExists "rook/ceph-target-disk"}}
          {{if .ParamExists "krib/selective-mastership" -}}
          SELECTIVE_MASTERSHIP={{.Param "krib/selective-mastership" }}
          {{end -}}
          {{if .ParamExists "krib/i-am-master" -}}
          I_AM_MASTER={{.Param "krib/i-am-master" }}
          {{end -}}
          if [[ "$SELECTIVE_MASTERSHIP" == true ]] ; then
            if [[ ! "$I_AM_MASTER" == true ]] ; then
              # I'm a worker, so wipe disk in preparation for next ceph installation
              yum install -y gdisk
              sgdisk --zap-all /dev/{{ .Param "rook/ceph-target-disk" }}
              # Now remove devicemapper details
              for i in `ls /dev/mapper/ceph*`; do dmsetup remove $i || echo "Failed to remove but failing gracefully to continue script"; done
            fi
          fi
          {{ end -}}
        ID: ""
        Meta: null
        Name: KRIB wipe machine for cluster reinstall
        Path: ""
      - Contents: |-
          #!/bin/bash

          {{ if .ParamExists "rook/ceph-target-disk"}}
          {{if .ParamExists "krib/selective-mastership" -}}
          SELECTIVE_MASTERSHIP={{.Param "krib/selective-mastership" }}
          {{end -}}
          {{if .ParamExists "krib/i-am-master" -}}
          I_AM_MASTER={{.Param "krib/i-am-master" }}
          {{end -}}
          if [[ "$SELECTIVE_MASTERSHIP" == true ]] ; then
            if [[ ! "$I_AM_MASTER" == true ]] ; then
              # I'm a worker, so wipe disk in preparation for next ceph installation
              yum install -y gdisk
              for disk in `ls /dev/ | grep {{ .Param "rook/ceph-target-disk" }}`
              do
                # Don't wipe OS filesystems
                echo {{ .Param "operating-system-disk" }} | grep $disk
                if [[ $? -ne 0 ]]; then
                  echo "DESTRUCTIVELY zapping /dev/$disk"
                  sgdisk --zap-all /dev/$disk
                else
                  echo "/dev/$disk matches {{ .Param "operating-system-disk" }}, so NOT zapping"
                fi

              done
              # Now remove devicemapper details
              for i in `ls /dev/mapper/ceph*`; do dmsetup remove $i || echo "Failed to remove but failing gracefully to continue script"; done
            fi
          fi
          {{ end -}}
        ID: ""
        Meta: null
        Name: KRIB wipe disks used for rook-ceph (if configured)
        Path: ""
      Validated: false
    krib-external-dns:
      Available: false
      Bundle: ""
      Description: A task to install and setup ExternalDNS
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: sitemap
        title: Community Content
      Name: krib-external-dns
      OptionalParams:
      - certmanager/dns-domain
      - certmanager/route53-access-key-id
      - certmanager/route53-secret-access-key
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      - krib/external-dns-provider
      Templates:
      - Contents: ""
        ID: krib-external-dns.yaml.tmpl
        Meta: null
        Name: ExternalDNS manifests
        Path: /tmp/krib-external-dns.yaml
      - Contents: ""
        ID: krib-external-dns.sh.tmpl
        Meta: null
        Name: ExternalDNS script execution
        Path: ""
      Validated: false
    krib-get-masters:
      Available: false
      Bundle: ""
      Description: A task to collect kubernetes masters
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-get-masters
      OptionalParams: []
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: krib-get-masters.sh.tmpl
        Meta: null
        Name: Collect Kubernetes Masters
        Path: ""
      Validated: false
    krib-haproxy:
      Available: false
      Bundle: ""
      Description: A task to setup haproxy
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-haproxy
      OptionalParams: []
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: krib-haproxy.cfg.tmpl
        Meta: null
        Name: HA Proxy Config file
        Path: /etc/haproxy/haproxy.cfg
      - Contents: ""
        ID: krib-haproxy.sh.tmpl
        Meta: null
        Name: Setup HA Proxy
        Path: ""
      Validated: false
    krib-helm:
      Available: false
      Bundle: ""
      Description: A task to install Helm and Tiller and any defined Charts
      Documentation: |
        Installs Helm and runs helm init (which installs Tiller) on the leader.
        Installs Charts defined in helm/charts.
        This uses the Digital Rebar Cluster pattern so krib/cluster-profile must be set.

        The install checks to see if tiller is running and may skip initialization.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-helm
      OptionalParams:
      - helm/charts
      - helm/version
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      Templates:
      - Contents: ""
        ID: krib-helm.cfg.tmpl
        Meta: null
        Name: Service Account for Tiller
        Path: tiller-rbac.yaml
      - Contents: ""
        ID: krib-helm-init.sh.tmpl
        Meta: null
        Name: Bringing up Helm and Tiller on Master
        Path: ""
      - Contents: ""
        ID: krib-helm.sh.tmpl
        Meta: null
        Name: Running Helm Charts
        Path: ""
      Validated: false
    krib-helm-charts:
      Available: false
      Bundle: ""
      Description: A task to install Helm charts
      Documentation: |
        Installs Charts defined in helm/charts.
        This uses the Digital Rebar Cluster pattern so krib/cluster-profile must be set.
        The install checks to see if tiller is running and may skip initialization.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-helm-charts
      OptionalParams:
      - helm/charts
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      Templates:
      - Contents: ""
        ID: krib-helm.sh.tmpl
        Meta: null
        Name: Running Helm Charts on nodes
        Path: ""
      Validated: false
    krib-helm-init:
      Available: false
      Bundle: ""
      Description: A task to install Helm and Tiller
      Documentation: "Installs Helm and runs helm init (which installs Tiller) on
        the leader.\nThis uses the Digital Rebar Cluster pattern so krib/cluster-profile
        must be set.\n\nThe install checks to see if tiller is running and may skip
        initialization. \n\nThe tasks only run on the leader so it must be included
        in the workflow.  All other\nmachines will be skipped so it is acceptable
        to run the task on all machines\nin the cluster.\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-helm-init
      OptionalParams:
      - helm/version
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      Templates:
      - Contents: ""
        ID: krib-helm.cfg.tmpl
        Meta: null
        Name: Service Account for Tiller
        Path: tiller-rbac.yaml
      - Contents: ""
        ID: krib-helm-init.sh.tmpl
        Meta: null
        Name: Bringing up Helm and Tiller on Master
        Path: ""
      Validated: false
    krib-ingress-nginx:
      Available: false
      Bundle: ""
      Description: A task to install/config ingress-nginx and optional cert-manager
      Documentation: |
        Sets Param: ingress/ip-address
        Install/config ingress-nginx and optional cert-manager
        This uses the Digital Rebar Cluster pattern so krib/cluster-profile must be set
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-ingress-nginx
      OptionalParams:
      - ingress/k8s-dashboard-host.yaml
      - certmanager/acme-challenge-dns01-provider
      - certmanager/cloudflare-api-key
      - certmanager/cloudflare-email
      - certmanager/email
      - certmanager/fastdns-access-token
      - certmanager/fastdns-client-secret
      - certmanager/fastdns-client-token
      - certmanager/fastdns-service-consumer-domain
      - certmanager/rfc2136-nameserver
      - certmanager/rfc2136-tsig-alg
      - certmanager/rfc2136-tsig-key
      - certmanager/rfc2136-tsig-key-name
      - certmanager/route53-access-key
      - certmanager/route53-region
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      Templates:
      - Contents: ""
        ID: certmanager-clusterissuer.cfg.tmpl
        Meta: null
        Name: cert-manager ClusterIssuer config file
        Path: /tmp/certmanager-clusterissuer.yaml
      - Contents: ""
        ID: certmanager-provider-secret.cfg.tmpl
        Meta: null
        Name: cert-manager Provider Secret config file
        Path: /tmp/certmanager-provider-secret.yaml
      - Contents: ""
        ID: krib-ingress-nginx.sh.tmpl
        Meta: null
        Name: Install/config ingress-nginx and optional cert-manager
        Path: ""
      Validated: false
    krib-ingress-nginx-tillerless:
      Available: false
      Bundle: ""
      Description: A task to install/config ingress-nginx and optional cert-manager
        WITHOUT helm or tiller
      Documentation: |
        Sets Param: ingress/ip-address
        Install/config ingress-nginx and optional cert-manager
        This uses the Digital Rebar Cluster pattern so krib/cluster-profile must be set
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-ingress-nginx-tillerless
      OptionalParams:
      - ingress/k8s-dashboard-host.yaml
      - certmanager/acme-challenge-dns01-provider
      - certmanager/cloudflare-api-key
      - certmanager/cloudflare-email
      - certmanager/email
      - certmanager/fastdns-access-token
      - certmanager/fastdns-client-secret
      - certmanager/fastdns-client-token
      - certmanager/fastdns-service-consumer-domain
      - certmanager/rfc2136-nameserver
      - certmanager/rfc2136-tsig-alg
      - certmanager/rfc2136-tsig-key
      - certmanager/rfc2136-tsig-key-name
      - certmanager/route53-access-key
      - certmanager/route53-region
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      Templates:
      - Contents: ""
        ID: certmanager-clusterissuer.cfg.tmpl
        Meta: null
        Name: cert-manager ClusterIssuer config file
        Path: /tmp/certmanager-clusterissuer.yaml
      - Contents: ""
        ID: certmanager-provider-secret.cfg.tmpl
        Meta: null
        Name: cert-manager Provider Secret config file
        Path: /tmp/certmanager-provider-secret.yaml
      - Contents: ""
        ID: krib-nginx-tcp-services.yaml.tmpl
        Meta: null
        Name: ConfigMap for exposing TCP services
        Path: /tmp/krib-nginx-tcp-services.yaml
      - Contents: ""
        ID: krib-nginx-udp-services.yaml.tmpl
        Meta: null
        Name: ConfigMap for exposing UDP services
        Path: /tmp/krib-nginx-udp-services.yaml
      - Contents: ""
        ID: krib-nginx-external-tcp-services.yaml.tmpl
        Meta: null
        Name: ConfigMap for exposing external TCP services
        Path: /tmp/krib-nginx-external-tcp-services.yaml
      - Contents: ""
        ID: krib-nginx-external-udp-services.yaml.tmpl
        Meta: null
        Name: ConfigMap for exposing external UDP services
        Path: /tmp/krib-nginx-external-udp-services.yaml
      - Contents: ""
        ID: krib-ingress-nginx-tillerless.sh.tmpl
        Meta: null
        Name: Install/config ingress-nginx and optional cert-manager, without relying
          on helm or tiller
        Path: ""
      Validated: false
    krib-keepalived:
      Available: false
      Bundle: ""
      Description: A task to setup keepalived
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-keepalived
      OptionalParams: []
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: krib-keepalived.sh.tmpl
        Meta: null
        Name: Setup Keepalived
        Path: ""
      Validated: false
    krib-kubevirt:
      Available: false
      Bundle: ""
      Description: A task to install KubeVirt
      Documentation: "Installs KubeVirt on the leader.\nThis uses the Digital Rebar
        Cluster pattern so krib/cluster-profile must be set.\n\nThe install checks
        to see if KubeVirt is running and may skip initialization.\n\nRecommend: you
        may want to add `intel_iommu=on` to the kernel-console param\n\nThe Config
        is provided from the kubevirt-configmap.cfg.tmpl template \ninstead of being
        downloaded from github.  Version updates should be reflected in the template.\nThis
        approach allows for parameterization of the configuration map.\n\nThe kubectl
        tasks only run on the leader so it must be included in the workflow.  All
        other\nmachines will run virt-host-validate so it is important to run the
        task on all machines\nin the cluster.\n\nAt this time, virtctl is NOT installed
        on cluster\n"
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: cloud
        title: Community Content
      Name: krib-kubevirt
      OptionalParams: null
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      Templates:
      - Contents: ""
        ID: kubevirt.cfg.tmpl
        Meta: null
        Name: Config for KubeVirt
        Path: kubevirt.yaml
      - Contents: |-
          #!/bin/bash
          set -e

          # Get access and who we are.
          {{template "setup.tmpl" .}}

          echo "Attempting to Virt Validate Host"
          echo "YOU MAY NEED TO add `intel_iommu=on` to your kernel-console param"
          # this is for information only, we do NOT fail if the tests fail
          virt-host-validate qemu && true

          # create the pods required
          echo "Configure KubeVirt on the leader (skip for workers)..."

          {{if .ParamExists "krib/cluster-profile" -}}
          CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
          PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
          {{else -}}
          echo "Missing krib/cluster-profile on the machine!"
          exit 1
          {{end -}}

          {{template "krib-lib.sh.tmpl" .}}

          MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
          echo "My Master index is $MASTER_INDEX"
          if [[ $MASTER_INDEX != notme ]] ; then

            if [[ $MASTER_INDEX == 0 ]] ; then

              echo "I am the elected leader - install KubeVirt"
              export KUBECONFIG="/etc/kubernetes/admin.conf"

              # is KubeVirt running?
              STAT=$(kubectl get pods -n kube-system -l=kubevirt.io=virt-handler --field-selector=status.phase=Running)
              ESCAPE=0

              # install KubeVirt
              while [[ $ESCAPE -lt 30 && -z $STAT ]] ; do
                kubectl apply -f kubevirt.yaml
                STAT=$(kubectl get pods -n kube-system -l=kubevirt.io=virt-handler --field-selector=status.phase=Running)
                echo "...waiting 10s for KubeVirt.io Handler to be running ($ESCAPE of 30)"
                kubectl get pods -n kube-system -l=kubevirt.io=virt-handler
                sleep 10
                ((ESCAPE=ESCAPE+1))
              done

              echo "KubeVirt installed"

            fi

          fi
        ID: ""
        Meta: null
        Name: krib-kubevirt
        Path: ""
      Validated: false
    krib-logging:
      Available: false
      Bundle: ""
      Description: A task to install logging
      Documentation: |
        Installs fluent-bit for aggregation of cluster logging to a graylog server
        This uses the Digital Rebar Cluster pattern so krib/cluster-profile must be set.

        The install checks to see if tiller is running and may skip initialization.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-logging
      OptionalParams: []
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      - krib/log-target-gelf
      Templates:
      - Contents: ""
        ID: logging-fluent-bit.yaml.tmpl
        Meta: null
        Name: YAML manifents for fluent-bit
        Path: /tmp/logging-fluent-bit.yaml
      - Contents: ""
        ID: logging-fluent-bit.sh.tmpl
        Meta: null
        Name: Apply the YAML manifests
        Path: ""
      Validated: false
    krib-longhorn:
      Available: false
      Bundle: ""
      Description: A task to install and setup Rancher Longhorn
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: sitemap
        title: Community Content
      Name: krib-longhorn
      OptionalParams:
      - ingress/longhorn-dashboard-hostname
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      Templates:
      - Contents: ""
        ID: krib-longhorn.sh.tmpl
        Meta: null
        Name: Install Rancher Longhorn
        Path: ""
      Validated: false
    krib-metallb:
      Available: false
      Bundle: ""
      Description: A task to install and setup MetalLB
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: sitemap
        title: Community Content
      Name: krib-metallb
      OptionalParams:
      - krib/metallb-config
      - metallb/monitoring-port
      - metallb/limits-cpu
      - metallb/limits-memory
      - metallb/l2-ip-range
      - metallb/l3-ip-range
      - metallb/l3-peer-address
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      Templates:
      - Contents: ""
        ID: krib-metallb.sh.tmpl
        Meta: null
        Name: Setup MetalLB
        Path: ""
      Validated: false
    krib-operate:
      Available: false
      Bundle: ""
      Description: A task to 'kubectl' operate on KRIB built Kubernetes node.
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-operate
      OptionalParams: []
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: krib-operate.sh.tmpl
        Meta: null
        Name: Operate on a KRIB built Kubernetes node
        Path: ""
      Validated: false
    krib-pkg-prep:
      Available: false
      Bundle: ""
      Description: Prep a cluster with prerequisite packages.
      Documentation: |
        Installs prerequisite OS packages prior to starting KRIB
        install process.  In some use cases this may be a faster
        pattern than performing the steps in the standard templates.

        For example - Sledgehammer Discover nodes, add 'krib-prep-pkgs'
        stage.  As machine is finishing prep - you can move to setting
        up other things, before kicking off the KRIB workflow.

        Uses packages listed in the 'default' Schema section of the
        Param 'krib/packages-to-prep'.  You can override this list
        by setting the Param in a Profile or directly on the Machines
        to apply this to.

        Packages MUST exist in the repositories on the Machines already.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-pkg-prep
      OptionalParams:
      - krib/packages-to-prep
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: |-
          #!/bin/bash
          # Install prerequisite OS packages prior to KRIB workflow run
          set -e

          {{template "setup.tmpl"}}

          # only execute the package install if we are have a KRIB
          # cluster profile set - this is a weak check to verify it
          # we're going to becmoe a KRIB cluster
          {{if .ParamExists "krib/cluster-profile"}}
          PKGS="{{.Param "krib/packages-to-prep"}}"
          if [[ -z "$PKGS" ]]; then
            echo "'krib/packates-to-prep' empty - no prep completed."
            exit 0
          else
            echo "Installing prereq packages:  $PKGS"
            install $PKGS
          fi
          {{else}}
          echo "The 'krib/cluster-profile' not found on Machine. Skipping prep."
          exit 0
          {{end}}

          exit 0
        ID: ""
        Meta: null
        Name: krib-pkg-prep
        Path: ""
      Validated: false
    krib-rook-ceph:
      Available: false
      Bundle: ""
      Description: A task to install and setup Rook Ceph
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: sitemap
        title: Community Content
      Name: krib-rook-ceph
      OptionalParams: []
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      Templates:
      - Contents: ""
        ID: rook-ceph-dashboard-ingress.yaml.tmpl
        Meta: null
        Name: Ceph dashboard ingress
        Path: /tmp/rook-ceph-dashboard-ingress.yaml
      - Contents: ""
        ID: rook-ceph-override.yaml.tmpl
        Meta: null
        Name: Ceph override configmap
        Path: /tmp/rook-ceph-override.yaml
      - Contents: ""
        ID: rook-ceph-toolbox.yaml.tmpl
        Meta: null
        Name: Ceph toolbox deployment
        Path: /tmp/rook-ceph-toolbox.yaml
      - Contents: ""
        ID: krib-rook-ceph.sh.tmpl
        Meta: null
        Name: Setup Rook Ceph
        Path: ""
      Validated: false
    krib-runtime-install:
      Available: false
      Bundle: ""
      Description: Installs container runtime
      Documentation: |
        Installs a container runtime
      Endpoint: ""
      Errors: []
      Meta: {}
      Name: krib-runtime-install
      OptionalParams: []
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/container-runtime
      Templates:
      - Contents: ""
        ID: krib-runtime-install.sh.tmpl
        Meta: null
        Name: Install a container runtime on a KRIB built Kubernetes node
        Path: ""
      Validated: false
    krib-set-time:
      Available: false
      Bundle: ""
      Description: A task to force time setting for now
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-set-time
      OptionalParams: []
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: |
          #!/bin/bash
          {{ template "setup.tmpl" }}
          set -e
          test ! $(which ntpdate > /dev/null 2>&1) && install ntpdate
          ntpdate -s time.apple.com
        ID: ""
        Meta: null
        Name: Set Time
        Path: ""
      Validated: false
    krib-settings:
      Available: false
      Bundle: ""
      Description: A task to setup general HA Settings
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: krib-settings
      OptionalParams: []
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: krib-settings.sh.tmpl
        Meta: null
        Name: Setup Settings
        Path: ""
      Validated: false
    krib-sonobuoy:
      Available: false
      Bundle: ""
      Description: A task to install Sonobuoy
      Documentation: |
        Installs Sonobuoy and runs it against the cluster on the leader.
        This uses the Digital Rebar Cluster pattern so krib/cluster-profile must be set.

        NOTE: Sonobuoy may take over an HOUR to complete.  The task will be in process during this time.

        The tasks only run on the leader so it must be included in the workflow.  All other
        machines will be skipped so it is acceptable to run the task on all machines
        in the cluster.
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: sound
        title: Community Content
      Name: krib-sonobuoy
      OptionalParams:
      - sonobuoy/binary
      Prerequisites: []
      ReadOnly: false
      RequiredParams:
      - krib/cluster-profile
      Templates:
      - Contents: ""
        ID: krib-sonobuoy.sh.tmpl
        Meta: null
        Name: Running Sonobuoy on Master
        Path: ""
      Validated: false
    kubernetes-install:
      Available: false
      Bundle: ""
      Description: A task to install kubernetes and kubeadm
      Documentation: |
        Downloads Kubernetes installation components from repos
        This task relies on the O/S packages being updated
        and accessible.
        NOTE: Access to update repos is required!
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: ship
        title: Community Content
      Name: kubernetes-install
      OptionalParams:
      - krib/cluster-name
      - krib/cluster-is-production
      - krib/cluster-join
      - krib/cluster-master-certs
      - krib/cluster-master-count
      - krib/cluster-master-vip
      - krib/cluster-master-on-etcds
      - krib/cluster-masters
      - krib/operate-action
      - krib/operate-on-node
      - krib/operate-options
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: kubernetes-install.sh.tmpl
        Meta: null
        Name: Install Kubernetes and Kubeadm
        Path: ""
      Validated: false
    mount-disks:
      Available: false
      Bundle: ""
      Description: Create and Mount Docker filesystem
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: disk outline
        title: Community Content
      Name: mount-disks
      OptionalParams: []
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: mount-disks.sh.tmpl
        Meta: null
        Name: Create and mount docker filesystem
        Path: ""
      Validated: false
    vault-config:
      Available: false
      Bundle: ""
      Description: A task to configure vault
      Documentation: |
        Configures a vault backend (using consul for storage) for secret encryption
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: book
        title: Community Content
      Name: vault-config
      OptionalParams:
      - vault/server-count
      - vault/version
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: vault.hcl.tmpl
        Meta: null
        Name: Vault configuration
        Path: /etc/vault/vault.hcl
      - Contents: ""
        ID: vault-policy-transit-only.hcl.tmpl
        Meta: null
        Name: Vault policy for transit backend
        Path: /etc/vault/policy-transit-only.hcl
      - Contents: ""
        ID: vault-config.sh.tmpl
        Meta: null
        Name: Configure vault
        Path: ""
      Validated: false
    vault-install:
      Available: false
      Bundle: ""
      Description: A task to install vault
      Documentation: |
        Installs (but not configures) consul, to be used as an HA backend to Vault
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: book
        title: Community Content
      Name: vault-install
      OptionalParams:
      - vault/server-count
      - vault/version
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: vault.service.tmpl
        Meta: null
        Name: Prepare systemd file for vault
        Path: /etc/systemd/system/vault.service
      - Contents: ""
        ID: vault.hcl.tmpl
        Meta: null
        Name: Vault configuration
        Path: /etc/vault/vault.hcl
      - Contents: ""
        ID: vault-env.sh.tmpl
        Meta: null
        Name: Prepare env file for CLI vault usage
        Path: /etc/vault/vault-env.sh
      - Contents: ""
        ID: vault-install.sh.tmpl
        Meta: null
        Name: Configure consul
        Path: ""
      Validated: false
    vault-kms-plugin:
      Available: false
      Bundle: ""
      Description: A task to configure vault kms plugin
      Documentation: |
        Configures a vault plugin for secret encryption
      Endpoint: ""
      Errors: []
      Meta:
        color: blue
        feature-flags: sane-exit-codes
        icon: book
        title: Community Content
      Name: vault-kms-plugin
      OptionalParams:
      - vault/server-count
      - vault/version
      Prerequisites: []
      ReadOnly: false
      RequiredParams: []
      Templates:
      - Contents: ""
        ID: krib-encryption-config.yaml.tmpl
        Meta: null
        Name: Setup kube secret encryption
        Path: /etc/kubernetes/pki/encryption.yaml
      - Contents: ""
        ID: vault-kms-plugin.service.tmpl
        Meta: null
        Name: Setup systemd service for kms plugin
        Path: /etc/systemd/system/vault-kms-plugin.service
      - Contents: ""
        ID: krib-vault-kms-plugin.sh.tmpl
        Meta: null
        Name: Configure vault kms plugin
        Path: ""
      Validated: false
  templates:
    certmanager-clusterissuer.cfg.tmpl:
      Available: false
      Bundle: ""
      Contents: "apiVersion: certmanager.k8s.io/v1alpha1\nkind: ClusterIssuer\nmetadata:\n
        \ name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n{{-
        if .ParamExists \"certmanager/email\"}}\n    email: \"{{.Param \"certmanager/email\"}}\"\n{{-
        end}}\n    privateKeySecretRef:\n      name: letsencrypt-prod\n{{- if .ParamExists
        \"certmanager/acme-challenge-dns01-provider\"}}\n    dns01:\n      providers:\n
        \       - name: {{.Param \"certmanager/acme-challenge-dns01-provider\"}}\n
        \         {{.Param \"certmanager/acme-challenge-dns01-provider\"}}:\n{{- if
        .ParamExists \"certmanager/cloudflare-email\"}}\n            email: \"{{.Param
        \"certmanager/cloudflare-email\"}}\"\n{{- end}}\n{{- if .ParamExists \"certmanager/fastdns-service-consumer-domain\"}}\n
        \           serviceConsumerDomain: \"{{.Param \"certmanager/fastdns-service-consumer-domain\"}}\"\n{{-
        end}}\n{{- if .ParamExists \"certmanager/rfc2136-nameserver\"}}\n            nameserver:
        \"{{.Param \"certmanager/rfc2136-nameserver\"}}\"\n{{- end}}\n{{- if .ParamExists
        \"certmanager/rfc2136-tsig-key-name\"}}\n            tsigKeyName: \"{{.Param
        \"certmanager/rfc2136-tsig-key-name\"}}\"\n{{- end}}\n{{- if .ParamExists
        \"certmanager/rfc2136-tsig-alg\"}}\n            tsigAlgorithm: \"{{.Param
        \"certmanager/rfc2136-tsig-alg\"}}\"\n{{- end}}\n{{- if .ParamExists \"certmanager/route53-secret-access-key\"}}\n{{-
        if .ParamExists \"certmanager/route53-region\"}}\n            region: \"{{.Param
        \"certmanager/route53-region\"}}\"\n{{- end}}\n{{- if .ParamExists \"certmanager/route53-access-key-id\"}}\n
        \           accessKeyID: {{.Param \"certmanager/route53-access-key-id\" }}\n{{-
        end}}\n{{- if .ParamExists \"certmanager/route53-hosted-zone-id\"}}\n            hostedZoneID:
        {{.Param \"certmanager/route53-hosted-zone-id\" }}\n{{- end}}\n            secretAccessKeySecretRef:\n
        \             name: certmanager-provider\n              namespace: kube-system\n
        \             key: accessKey\n{{- end}}\n{{- if .ParamExists \"certmanager/cloudflare-api-key\"}}\n
        \           apiKeySecretRef:\n              name: certmanager-provider\n              namespace:
        kube-system\n              key: apiKey\n{{- end}}\n{{- if .ParamExists \"certmanager/fastdns-client-token\"}}\n
        \           clientTokenSecretRef:\n              name: certmanager-provider\n
        \             namespace: kube-system\n              key: clientToken\n{{-
        end}}\n{{- if .ParamExists \"certmanager/fastdns-client-secret\"}}\n            clientSecretSecretRef:\n
        \             name: certmanager-provider\n              namespace: kube-system\n
        \             key: clientSecret\n{{- end}}\n{{- if .ParamExists \"certmanager/fastdns-access-token\"}}\n
        \           accessTokenSecretRef:\n              name: certmanager-provider\n
        \             namespace: kube-system\n              key: accessToken\n{{-
        end}}\n{{- if .ParamExists \"certmanager/rfc2136-tsig-key\"}}\n            tsigSecretSecretRef:\n
        \             name: certmanager-provider\n              namespace: kube-system\n
        \             key: tsigKey\n{{- end}}\n    solvers:\n    - dns01:\n{{- if
        .ParamExists \"certmanager/route53-secret-access-key\"}}\n        route53:\n{{-
        if .ParamExists \"certmanager/route53-region\"}}\n          region: \"{{.Param
        \"certmanager/route53-region\"}}\"\n{{- end}}\n{{- if .ParamExists \"certmanager/route53-access-key-id\"}}\n
        \         accessKeyID: {{.Param \"certmanager/route53-access-key-id\" }}\n{{-
        end}}\n{{- if .ParamExists \"certmanager/route53-hosted-zone-id\"}}\n          hostedZoneID:
        {{.Param \"certmanager/route53-hosted-zone-id\" }}\n{{- end}}\n          secretAccessKeySecretRef:\n
        \           name: certmanager-provider\n            namespace: kube-system\n
        \           key: accessKey\n        selector:\n          matchLabels:\n            use-route53-solver:
        \"true\"            \n{{- end}}\n{{- else}}\n    http01: {}\n{{- end}}\n\n---\n\napiVersion:
        certmanager.k8s.io/v1alpha1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n
        \ acme:\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n{{-
        if .ParamExists \"certmanager/email\"}}\n    email: \"{{.Param \"certmanager/email\"}}\"\n{{-
        end}}\n    privateKeySecretRef:\n      name: letsencrypt-staging\n{{- if .ParamExists
        \"certmanager/acme-challenge-dns01-provider\"}}\n    dns01:\n      providers:\n
        \       - name: {{.Param \"certmanager/acme-challenge-dns01-provider\"}}\n
        \         {{.Param \"certmanager/acme-challenge-dns01-provider\"}}:\n{{- if
        .ParamExists \"certmanager/cloudflare-email\"}}\n            email: \"{{.Param
        \"certmanager/cloudflare-email\"}}\"\n{{- end}}\n{{- if .ParamExists \"certmanager/route53-region\"}}\n
        \           region: \"{{.Param \"certmanager/route53-region\"}}\"\n{{- end}}\n{{-
        if .ParamExists \"certmanager/fastdns-service-consumer-domain\"}}\n            serviceConsumerDomain:
        \"{{.Param \"certmanager/fastdns-service-consumer-domain\"}}\"\n{{- end}}\n{{-
        if .ParamExists \"certmanager/rfc2136-nameserver\"}}\n            nameserver:
        \"{{.Param \"certmanager/rfc2136-nameserver\"}}\"\n{{- end}}\n{{- if .ParamExists
        \"certmanager/rfc2136-tsig-key-name\"}}\n            tsigKeyName: \"{{.Param
        \"certmanager/rfc2136-tsig-key-name\"}}\"\n{{- end}}\n{{- if .ParamExists
        \"certmanager/rfc2136-tsig-alg\"}}\n            tsigAlgorithm: \"{{.Param
        \"certmanager/rfc2136-tsig-alg\"}}\"\n{{- end}}\n{{- if .ParamExists \"certmanager/route53-secret-access-key\"}}\n{{-
        if .ParamExists \"certmanager/route53-region\"}}\n            region: \"{{.Param
        \"certmanager/route53-region\"}}\"\n{{- end}}\n{{- if .ParamExists \"certmanager/route53-access-key-id\"}}\n
        \           accessKeyID: {{.Param \"certmanager/route53-access-key-id\" }}\n{{-
        end}}\n{{- if .ParamExists \"certmanager/route53-hosted-zone-id\"}}\n            hostedZoneID:
        {{.Param \"certmanager/route53-hosted-zone-id\" }}\n{{- end}}\n            secretAccessKeySecretRef:\n
        \             name: certmanager-provider\n              namespace: kube-system\n
        \             key: accessKey\n{{- end}}\n{{- if .ParamExists \"certmanager/cloudflare-api-key\"}}\n
        \           apiKeySecretRef:\n              name: certmanager-provider\n              namespace:
        kube-system\n              key: apiKey\n{{- end}}\n{{- if .ParamExists \"certmanager/fastdns-client-token\"}}\n
        \           clientTokenSecretRef:\n              name: certmanager-provider\n
        \             namespace: kube-system\n              key: clientToken\n{{-
        end}}\n{{- if .ParamExists \"certmanager/fastdns-client-secret\"}}\n            clientSecretSecretRef:\n
        \             name: certmanager-provider\n              namespace: kube-system\n
        \             key: clientSecret\n{{- end}}\n{{- if .ParamExists \"certmanager/fastdns-access-token\"}}\n
        \           accessTokenSecretRef:\n              name: certmanager-provider\n
        \             namespace: kube-system\n              key: accessToken\n{{-
        end}}\n{{- if .ParamExists \"certmanager/rfc2136-tsig-key\"}}\n            tsigSecretSecretRef:\n
        \             name: certmanager-provider\n              namespace: kube-system\n
        \             key: tsigKey\n{{- end}}\n    solvers:\n    - dns01:\n{{- if
        .ParamExists \"certmanager/route53-secret-access-key\"}}\n        route53:\n{{-
        if .ParamExists \"certmanager/route53-region\"}}\n          region: \"{{.Param
        \"certmanager/route53-region\"}}\"\n{{- end}}\n{{- if .ParamExists \"certmanager/route53-access-key-id\"}}\n
        \         accessKeyID: {{.Param \"certmanager/route53-access-key-id\" }}\n{{-
        end}}\n{{- if .ParamExists \"certmanager/route53-hosted-zone-id\"}}\n          hostedZoneID:
        {{.Param \"certmanager/route53-hosted-zone-id\" }}\n{{- end}}\n          secretAccessKeySecretRef:\n
        \           name: certmanager-provider\n            namespace: kube-system\n
        \           key: accessKey\n        selector:\n          matchLabels:\n            use-route53-solver:
        \"true\"            \n{{- end}}\n{{- else}}\n    http01: {}\n{{- end}}\n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: certmanager-clusterissuer.cfg.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    certmanager-provider-secret.cfg.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        apiVersion: v1
        kind: Secret
        metadata:
          name: certmanager-provider
          namespace: cert-manager
        data:
        {{- if .ParamExists "certmanager/route53-secret-access-key"}}
           accessKey: "{{.Param "certmanager/route53-secret-access-key"}}"
        {{- end}}
        {{- if .ParamExists "certmanager/cloudflare-api-key"}}
           apiKey: "{{.Param "certmanager/cloudflare-api-key"}}"
        {{- end}}
        {{- if .ParamExists "certmanager/fastdns-client-token"}}
           clientToken: "{{.Param "certmanager/fastdns-client-token"}}"
        {{- end}}
        {{- if .ParamExists "certmanager/fastdns-client-secret"}}
           clientSecret: "{{.Param "certmanager/fastdns-client-secret"}}"
        {{- end}}
        {{- if .ParamExists "certmanager/fastdns-access-token"}}
           accessToken: "{{.Param "certmanager/fastdns-access-token"}}"
        {{- end}}
        {{- if .ParamExists "certmanager/rfc2136-tsig-key"}}
           tsigKey: "{{.Param "certmanager/rfc2136-tsig-key"}}"
        {{- end}}
      Description: ""
      Endpoint: ""
      Errors: []
      ID: certmanager-provider-secret.cfg.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    consul-agent-config.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash


        # Build a consul cluster
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        # Skip the remainder of this template if this host is not a master in a selective-master deployment
        {{template "krib-skip-if-not-master.tmpl" .}}

        export RS_UUID="{{.Machine.UUID}}"
        export RS_IP="{{.Machine.Address}}"

        CONSUL_VERSION="{{ .Param "consul/version" }}"

        # these need to be before krib-lib template
        {{if .ParamExists "consul/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "consul/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "consul/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing consul/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        {{if .ParamExists "consul/servers" -}}
        # Ensure that the consul systemd service specifies all consul nodes using retry-join
        setup_retry_join() {
          echo "Setting up retry_join..."
          {{- range $elem := .Param "consul/servers"}}
          PEER={{ $elem.Address }}
          if [[ ! `grep $PEER /etc/systemd/system/consul-agent.service` ]]; then
            sed -i "/ExecStart=/ a \    -retry-join=$PEER:8301 \\\\" /etc/systemd/system/consul-agent.service
          fi
          {{ end -}}
        }

        {{ end -}}

        echo "Configure the consul cluster"

        setup_retry_join

        systemctl daemon-reload
        systemctl restart consul-agent
        systemctl status consul-agent

        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: consul-agent-config.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    consul-agent-install.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # This script installs consul, but doesn't configure it

        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        # Skip the remainder of this template if this host is not a master in a selective-master deployment
        {{template "krib-skip-if-not-master.tmpl" .}}

        export RS_UUID="{{.Machine.UUID}}"
        export RS_IP="{{.Machine.Address}}"

        CONSUL_VERSION="{{ .Param "consul/version" }}"

        # these need to be before krib-lib template
        {{if .ParamExists "consul/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "consul/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "consul/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing consul/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        echo "Prepare the consul cluster"

        CONSUL_CLUSTER_NAME={{.Param "consul/name"}}
        {{ if .ParamExists "consul/ip" -}}
        CONSUL_IP={{ .Param "consul/ip" }}
        {{ else -}}
        CONSUL_IP={{ .Machine.Address }}
        {{ end -}}

        {{if eq (.ParamExists "consul/agents") false -}}
        # add server management params if missing
        echo "Add initial variables to track members."
        drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "consul/agents" to "[]" || true
        {{ end -}}

        {{if eq (.ParamExists "consul/agents-done") false -}}
        drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "consul/agents-done" to "[]" || true
        {{ end -}}

        # Get the number of servers to create
        CONSUL_SERVER_COUNT={{.Param "consul/server-count"}}
        echo "Creating $CONSUL_SERVER_COUNT servers"

        echo "Electing consul members to cluster profile: $CLUSTER_PROFILE"
        CONSUL_INDEX=$(add_me_if_not_count "consul/agents" $CONSUL_SERVER_COUNT $CONSUL_IP)

        echo "Added myself to cluster profile, my index is ${CONSUL_INDEX}"

        if [[ $CONSUL_INDEX == notme ]] ; then
          echo "I am not a CONSUL server.  Move on."
          wait_for_count "consul/agents-done" $CONSUL_SERVER_COUNT
          exit 0
        fi

        SERVER_CA=${CONSUL_CLUSTER_NAME}-server-ca

        EXISTING_INDEX=$(find_me "consul/agents-done" "Uuid" $RS_UUID)


        echo "Waiting for ${CONSUL_SERVER_COUNT} servers to be ready"
        wait_for_count "consul/agents" $CONSUL_SERVER_COUNT

        echo "${CONSUL_SERVER_COUNT} servers ready!"

        SERVER_CA_PW=$(wait_for_variable "consul/server-ca-pw")

        # Add the consul user if it doesn't exist
        id -u consul &>/dev/null || ( echo "Creating consul user"; useradd consul -d /var/lib/consul )

        TMP_DIR=/tmp/consul-tmp
        INSTALL_DIR=/usr/local/bin
        if [[ $OS_FAMILY == coreos ]] ; then
          INSTALL_DIR=/opt/bin
        fi

        mkdir -p ${TMP_DIR}

        echo "Download consul version: v${CONSUL_VERSION}"
        # Allow for a local repository for installation files
        {{if .ParamExists "krib/package-repository" -}}
        KRIB_REPO={{.Param "krib/package-repository"}}
        {{end -}}

        if [[ ! -z "$KRIB_REPO" ]] ; then
          download -L ${KRIB_REPO}/consul_${CONSUL_VERSION}_linux_amd64.zip  -o ${TMP_DIR}/consul_${CONSUL_VERSION}_linux_amd64.zip
        else
          download -L https://releases.hashicorp.com/consul/${CONSUL_VERSION}/consul_${CONSUL_VERSION}_linux_amd64.zip -o ${TMP_DIR}/consul_${CONSUL_VERSION}_linux_amd64.zip
        fi

        echo "Install consul version: ${CONSUL_VERSION}"
        yum -y install unzip || echo "Unzip already installed"
        unzip -o ${TMP_DIR}/consul_${CONSUL_VERSION}_linux_amd64.zip -d ${INSTALL_DIR}

        systemctl daemon-reload
        systemctl enable consul-agent

        echo "Consul installed, ready to start"
        add_me_if_not_count "consul/agents-done" $CONSUL_SERVER_COUNT $CONSUL_IP

        echo "Waiting for ${CONSUL_SERVER_COUNT} servers to complete consul install"
        wait_for_count "consul/agents-done" $CONSUL_SERVER_COUNT

        rm -rf ${TMP_DIR}

        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: consul-agent-install.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    consul-agent.json.tmpl:
      Available: false
      Bundle: ""
      Contents: "{\n  \"server\": false,\n  \"node_name\": \"{{.Machine.ShortName}}-agent\",\n
        \ \"datacenter\": \"dc1\",\n  \"data_dir\": \"/var/lib/consul/client\",\n
        \ \"bind_addr\": \"0.0.0.0\",\n  \"client_addr\": \"127.0.0.1\",\n  \"advertise_addr\":
        \"{{.Machine.Address}}\",  \n  \"ui\": false,\n  \"log_level\": \"DEBUG\",\n
        \ \"enable_syslog\": true,\n  \"encrypt\": \"{{ .Param \"consul/encryption-key\"
        }}\",\n  \"acl_enforce_version_8\": false,\n  \"verify_incoming\": false,\n
        \ \"verify_outgoing\": true,\n  \"verify_server_hostname\": true,\n  \"ca_file\":
        \"/etc/consul/pki/server-ca.pem\",\n  \"cert_file\": \"/etc/consul/pki/server.pem\",\n
        \ \"key_file\": \"/etc/consul/pki/server-key.pem\",\n  \"ports\": {\n    \"http\":
        8500,\n    \"https\": -1,\n    \"serf_wan\": -1,\n    \"serf_lan\": 8311,\n
        \   \"dns\": 8601\n  }  \n}"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: consul-agent.json.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    consul-agent.service.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        ### BEGIN INIT INFO
        # Provides:          consul
        # Required-Start:    $local_fs $remote_fs
        # Required-Stop:     $local_fs $remote_fs
        # Default-Start:     2 3 4 5
        # Default-Stop:      0 1 6
        # Short-Description: Consul agent
        # Description:       Consul service discovery framework (agent)
        ### END INIT INFO

        [Unit]
        Description=Consul agent
        Requires=network-online.target
        After=network-online.target

        [Service]
        User=consul
        Group=consul
        PIDFile=/var/run/consul/consul-agent.pid
        PermissionsStartOnly=true
        ExecStartPre=-/bin/mkdir -p /var/run/consul
        ExecStartPre=/bin/chown -R consul:consul /var/run/consul
        ExecStart=/usr/local/bin/consul agent \
            -config-file=/etc/consul.d/consul-agent.json \
            -pid-file=/var/run/consul/consul-agent.pid
        ExecReload=/bin/kill -HUP $MAINPID
        KillMode=process
        KillSignal=SIGTERM
        Restart=on-failure
        RestartSec=42s

        [Install]
        WantedBy=multi-user.target
      Description: ""
      Endpoint: ""
      Errors: []
      ID: consul-agent.service.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    consul-server-config.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash


        # Build a consul cluster
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        # Skip the remainder of this template if this host is not a master in a selective-master deployment
        {{template "krib-skip-if-not-master.tmpl" .}}

        export RS_UUID="{{.Machine.UUID}}"
        export RS_IP="{{.Machine.Address}}"

        CONSUL_VERSION="{{ .Param "consul/version" }}"

        # these need to be before krib-lib template
        {{if .ParamExists "consul/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "consul/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "consul/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing consul/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        {{if .ParamExists "consul/servers" -}}
        # Ensure that the consul systemd service specifies all consul nodes using retry-join
        setup_retry_join() {
          echo "Setting up retry_join..."
          {{- range $elem := .Param "consul/servers"}}
          PEER={{ $elem.Address }}
          if [[ ! `grep $PEER /etc/systemd/system/consul-server.service` ]]; then
            sed -i "/ExecStart=/ a \    -retry-join=$PEER \\\\" /etc/systemd/system/consul-server.service
          fi
          {{ end -}}
        }

        {{ end -}}

        echo "Configure the consul cluster"

        setup_retry_join

        systemctl daemon-reload
        systemctl restart consul-server
        systemctl status consul-server

        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: consul-server-config.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    consul-server-env.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        #!/bin/bash
        # Set defaults for Consul CLI.
        export CONSUL_HTTP_ADDR=https://localhost:8501
        export CONSUL_CACERT=/etc/consul/pki/server-ca.pem
        export CONSUL_CLIENT_CERT=/etc/consul/pki/server.pem
        export CONSUL_CLIENT_KEY=/etc/consul/pki/server-key.pem
      Description: ""
      Endpoint: ""
      Errors: []
      ID: consul-server-env.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    consul-server-install.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: "#!/usr/bin/env bash\n# This script installs consul, but doesn't configure
        it\n\nset -e\n\n# Get access and who we are.\n{{template \"setup.tmpl\" .}}\n\n#
        Skip the remainder of this template if this host is not a master in a selective-master
        deployment\n{{template \"krib-skip-if-not-master.tmpl\" .}}\n\nexport RS_UUID=\"{{.Machine.UUID}}\"\nexport
        RS_IP=\"{{.Machine.Address}}\"\n\nCONSUL_VERSION=\"{{ .Param \"consul/version\"
        }}\"\n\n# these need to be before krib-lib template\n{{if .ParamExists \"consul/cluster-profile\"
        -}}\nCLUSTER_PROFILE={{.Param \"consul/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken
        (.Param \"consul/cluster-profile\") 7200}}\n{{else -}}\nxiterr 1 \"Missing
        consul/cluster-profile on the machine!\"\n{{end -}}\n\n{{template \"krib-lib.sh.tmpl\"
        .}}\n\nbuild_cert() {\n  local profile=$1\n  local ca_name=$2\n  local ca_pw=$3\n
        \ local myname=$4\n  local myip=$5\n\n  echo \"Generating certificate for
        ${profile} with my name ${myname} and my IP ${myip}\"\n  drpcli machines runaction
        $RS_UUID getca certs/root $ca_name | jq -r . > /etc/consul/pki/${profile}-ca.pem\n
        \ drpcli certs csr $ca_name $myname $RS_IP $myip $(hostname) server.dc1.consul
        localhost {{if .ParamExists \"krib/cluster-master-vip\" }}{{ .Param \"krib/cluster-master-vip\"
        }}{{end}} > tmp.csr\n  drpcli machines runaction $RS_UUID signcert certs/root
        $ca_name certs/root-pw $ca_pw certs/csr \"$(jq .CSR tmp.csr)\" certs/profile
        $profile | jq -r . > /etc/consul/pki/$profile.pem\n  jq -r .Key tmp.csr >
        /etc/consul/pki/$profile-key.pem\n  rm tmp.csr\n}\n\necho \"Prepare the consul
        cluster\"\n\nCONSUL_CLUSTER_NAME={{.Param \"consul/name\"}}\n{{ if .ParamExists
        \"consul/ip\" -}}\nCONSUL_IP={{ .Param \"consul/ip\" }}\n{{ else -}}\nCONSUL_IP={{
        .Machine.Address }}\n{{ end -}}\n\n# Either set controller IP to a param,
        or default to the DRP IP\n{{ if .ParamExists \"consul/controller-ip\" -}}\nCONSUL_CONTROLLER_IP={{
        .Param \"consul/controller-ip\" }}\n{{ else -}}\nCONSUL_CONTROLLER_IP={{ .ProvisionerAddress
        }}\n{{ end -}}\n\n\n{{if eq (.ParamExists \"consul/servers\") false -}}\n#
        add server management params if missing\necho \"Add initial variables to track
        members.\"\ndrpcli -T \"$PROFILE_TOKEN\" profiles add \"$CLUSTER_PROFILE\"
        param \"consul/servers\" to \"[]\" || true\n{{ end -}}\n\n{{if eq (.ParamExists
        \"consul/servers-done\") false -}}\ndrpcli -T \"$PROFILE_TOKEN\" profiles
        add \"$CLUSTER_PROFILE\" param \"consul/servers-done\" to \"[]\" || true\n{{
        end -}}\n\n# Get the number of servers to create\nCONSUL_SERVER_COUNT={{.Param
        \"consul/server-count\"}}\necho \"Creating $CONSUL_SERVER_COUNT servers\"\n\necho
        \"Electing consul members to cluster profile: $CLUSTER_PROFILE\"\nCONSUL_INDEX=$(add_me_if_not_count
        \"consul/servers\" $CONSUL_SERVER_COUNT $CONSUL_IP)\n\necho \"Added myself
        to cluster profile, my index is ${CONSUL_INDEX}\"\n\nif [[ $CONSUL_INDEX ==
        notme ]] ; then\n  echo \"I am not a CONSUL server.  Move on.\"\n  wait_for_count
        \"consul/servers-done\" $CONSUL_SERVER_COUNT\n  exit 0\nfi\n\nTMP_DIR=/tmp/consul-tmp\nINSTALL_DIR=/usr/local/bin\nif
        [[ $OS_FAMILY == coreos ]] ; then\n  INSTALL_DIR=/opt/bin\nfi\n\nmkdir -p
        ${TMP_DIR}\n\necho \"Download consul version: v${CONSUL_VERSION}\"\n# Allow
        for a local repository for installation files\n{{if .ParamExists \"krib/package-repository\"
        -}}\nKRIB_REPO={{.Param \"krib/package-repository\"}}\n{{end -}}\n\nif [[
        ! -z \"$KRIB_REPO\" ]] ; then\n  download -L ${KRIB_REPO}/consul_${CONSUL_VERSION}_linux_amd64.zip
        \ -o ${TMP_DIR}/consul_${CONSUL_VERSION}_linux_amd64.zip\nelse\n  download
        -L https://releases.hashicorp.com/consul/${CONSUL_VERSION}/consul_${CONSUL_VERSION}_linux_amd64.zip
        -o ${TMP_DIR}/consul_${CONSUL_VERSION}_linux_amd64.zip\nfi\n\necho \"Install
        consul version: ${CONSUL_VERSION}\"\nyum -y install unzip || echo \"Unzip
        already installed\"\nunzip -o ${TMP_DIR}/consul_${CONSUL_VERSION}_linux_amd64.zip
        -d ${INSTALL_DIR}\n\n\n\nSERVER_CA=${CONSUL_CLUSTER_NAME}-ca\n\nEXISTING_INDEX=$(find_me
        \"consul/servers-done\" \"Uuid\" $RS_UUID)\nif [[ $EXISTING_INDEX == notme
        ]] ; then\n  {{if .ParamExists \"certs/root\" -}}\n\n  echo \"Certs plugin
        detected....setting up CA\"\n  # If we are INDEX=0, let's setup the root certs
        for building keys\n  if [[ $CONSUL_INDEX == \"0\" ]] ; then\n    echo \"We
        are first machine in cluster, setting up the root certs...\"\n    # Are certs
        built yet?\n    if ! drpcli machines runaction $RS_UUID getca certs/root $SERVER_CA
        2>/dev/null >/dev/null ; then\n      SERVER_CA_PW=$(drpcli machines runaction
        $RS_UUID makeroot certs/root $SERVER_CA | jq -r .)\n      drpcli -T \"$PROFILE_TOKEN\"
        profiles add \"$CLUSTER_PROFILE\" param \"consul/server-ca-name\" to \"$SERVER_CA\"
        || true\n      drpcli -T \"$PROFILE_TOKEN\" profiles add \"$CLUSTER_PROFILE\"
        param \"consul/server-ca-pw\" to \"$SERVER_CA_PW\" || true\n    else\n      if
        [[ $(get_param \"consul/server-ca-pw\") == null ]] ; then\n        xiterr
        1 \"SERVER CA ($SERVER_CA) Exists, but we did not set password.  Need to reset
        data in certs-data profile!!\"\n      fi\n    fi\n    echo \"Setting up encryption
        key for gossip...\"\n{{if eq (.ParamExists \"consul/encryption-key\") false
        -}}\n    CONSUL_ENCRYPTION_KEY=`consul keygen`\n    # add server management
        params if missing\n    echo \"Add initial encryption key ...\"\n    drpcli
        -T \"$PROFILE_TOKEN\" profiles add \"$CLUSTER_PROFILE\" param \"consul/encryption-key\"
        to \"$CONSUL_ENCRYPTION_KEY\" || true\n{{ end -}}    \n\n    # On the assumption
        that we'll want to backup the consul cluster externally (from DRP host or
        a \"controller\")\n    # Generate a client cert for the controller\n    echo
        \"Generating controller client certificate with IP ${CONSUL_CONTROLLER_IP}\"\n
        \   # First, get the CA cert into a param\n    drpcli machines runaction $RS_UUID
        getca certs/root $SERVER_CA | jq -r . > tmp.ca\n    drpcli -T \"$PROFILE_TOKEN\"
        profiles add \"$CLUSTER_PROFILE\" param \"consul/server-ca-cert\" to \"\\\"$(awk
        '{printf \"%s\\\\n\", $0}' tmp.ca)\\\"\"\n\n    SERVER_CA_PW=$(wait_for_variable
        \"consul/server-ca-pw\")\n    # Next, generate a client cert and key for the
        controller's IP\n    drpcli certs csr client $CONSUL_CONTROLLER_IP > tmp.csr\n
        \   drpcli machines runaction $RS_UUID signcert certs/root $SERVER_CA certs/root-pw
        $SERVER_CA_PW certs/csr \"$(jq .CSR tmp.csr)\" certs/profile server | jq -r
        . > controller.pem\n    jq -r .Key tmp.csr > controller.key\n\n    # Now put
        the etcd controller cert details _back_ into params, to be extracted on the
        controller\n    drpcli -T \"$PROFILE_TOKEN\" profiles add \"$CLUSTER_PROFILE\"
        param \"consul/controller-client-cert\" to \"\\\"$(awk '{printf \"%s\\\\n\",
        $0}' controller.pem)\\\"\"\n    drpcli -T \"$PROFILE_TOKEN\" profiles add
        \"$CLUSTER_PROFILE\" param \"consul/controller-client-key\" to \"\\\"$(awk
        '{printf \"%s\\\\n\", $0}' controller.key)\\\"\"\n\n    # Clean up\n    rm
        {tmp.ca,tmp.csr,controller.pem,controller.key}\n\n  fi\n  {{else -}}\n  xiterr
        1 \"STAGE REQUIRES CERT PLUGIN!!  It is freely available, download from RackN
        SaaS.\"\n  {{end}}\nfi\n\necho \"Waiting for ${CONSUL_SERVER_COUNT} servers
        to be ready\"\nwait_for_count \"consul/servers\" $CONSUL_SERVER_COUNT\n\necho
        \"${CONSUL_SERVER_COUNT} servers ready!\"\n\n\n\n# Add the consul user if
        it doesn't exist\nid -u consul &>/dev/null || ( echo \"Creating consul user\";
        useradd consul -d /var/lib/consul )\n\nmkdir -p /etc/consul/pki\n\nCONSUL_NAME=\"consul$CONSUL_INDEX\"\n\nbuild_cert
        \"server\" $SERVER_CA $SERVER_CA_PW $CONSUL_NAME $CONSUL_IP\n\n\necho \"Source
        /etc/consul.d/env.sh in your .bash_profile to add the necessary environment
        variables to run consul CLI with TLS\"\n\nsystemctl daemon-reload\nsystemctl
        enable consul-server\n\necho \"Consul installed, ready to start\"\nadd_me_if_not_count
        \"consul/servers-done\" $CONSUL_SERVER_COUNT $CONSUL_IP\n\necho \"Waiting
        for ${CONSUL_SERVER_COUNT} servers to complete consul install\"\nwait_for_count
        \"consul/servers-done\" $CONSUL_SERVER_COUNT\n\nrm -rf ${TMP_DIR}\n\nexit
        0\n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: consul-server-install.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    consul-server.json.tmpl:
      Available: false
      Bundle: ""
      Contents: "{\n  \"server\": true,\n  \"node_name\": \"{{.Machine.ShortName}}-server\",\n
        \ \"datacenter\": \"dc1\",\n  \"data_dir\": \"/var/lib/consul/server\",\n
        \ \"bind_addr\": \"{{.Machine.Address}}\",\n  \"client_addr\": \"0.0.0.0\",\n
        \ \"advertise_addr\": \"{{.Machine.Address}}\",\n  \"bootstrap_expect\": 3,\n
        \ \"ui\": false,\n  \"log_level\": \"INFO\",\n  \"enable_syslog\": true,\n
        \ \"encrypt\": \"{{ .Param \"consul/encryption-key\" }}\",  \n  \"acl_enforce_version_8\":
        false,\n  \"verify_incoming\": true,\n  \"verify_outgoing\": true,\n  \"verify_server_hostname\":
        true,\n  \"ca_file\": \"/etc/consul/pki/server-ca.pem\",\n  \"cert_file\":
        \"/etc/consul/pki/server.pem\",\n  \"key_file\": \"/etc/consul/pki/server-key.pem\",\n
        \ \"ports\": {\n    \"http\": -1,\n    \"https\": 8501\n  }  \n}"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: consul-server.json.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    consul-server.service.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        ### BEGIN INIT INFO
        # Provides:          consul
        # Required-Start:    $local_fs $remote_fs
        # Required-Stop:     $local_fs $remote_fs
        # Default-Start:     2 3 4 5
        # Default-Stop:      0 1 6
        # Short-Description: Consul agent
        # Description:       Consul service discovery framework
        ### END INIT INFO

        [Unit]
        Description=Consul server agent
        Requires=network-online.target
        After=network-online.target

        [Service]
        User=consul
        Group=consul
        PIDFile=/var/run/consul/consul-server.pid
        PermissionsStartOnly=true
        ExecStartPre=-/bin/mkdir -p /var/run/consul
        ExecStartPre=/bin/chown -R consul:consul /var/run/consul
        ExecStart=/usr/local/bin/consul agent \
            -config-file=/etc/consul.d/consul-server.json \
            -pid-file=/var/run/consul/consul-server.pid
        ExecReload=/bin/kill -HUP $MAINPID
        KillMode=process
        KillSignal=SIGTERM
        Restart=on-failure
        RestartSec=42s

        [Install]
        WantedBy=multi-user.target
      Description: ""
      Endpoint: ""
      Errors: []
      ID: consul-server.service.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    containerd-install.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: "#!/usr/bin/env bash\n# Kubernetes Rebar Integrated Boot (KRIB) Docker
        Install\nset -e\n\necho \"starting containerd v{{.Param \"containerd/version\"}}
        install\"\n# Get access and who we are.\n{{template \"setup.tmpl\" .}}\n[[
        $RS_UUID ]] && export RS_UUID=\"{{.Machine.UUID}}\"\n\n{{if .ParamExists \"kubectl/working-dir\"
        -}}\n# Only do this if it exists.\n# if it isn't setup, don't use it.\nif
        [[ -e {{.Param \"kubectl/working-dir\"}} ]] ; then\n  echo \"Linking the kubectl
        working directory into place.\"\n  ln -s {{.Param \"kubectl/working-dir\"}}
        /var/lib/kubectl\nfi\n{{end -}}\n\n# Allow for a local repository for installation
        files\n{{if .ParamExists \"krib/package-repository\" -}}\n  echo \"local repo
        download containerd v{{.Param \"containerd/version\"}} binaries in {{.Param
        \"krib/package-repository\"}}\"\n  curl -L {{.Param \"krib/package-repository\"}}/cri-containerd-{{.Param
        \"containerd/version\"}}.linux-amd64.tar.gz | tar xvz -C /\n{{else}}\n  BINARIES=\"cri-containerd-{{.Param
        \"containerd/version\"}}.linux-amd64.tar.gz\"\n  if [[ ! -f $BINARIES ]] ;
        then\n\t  echo \"download containerd v{{.Param \"containerd/version\"}} to
        $BINARIES\"\n\t  curl -L https://storage.googleapis.com/cri-containerd-release/$BINARIES
        -o $BINARIES\n  else\n    echo \"$BINARIES already downloaded\"\n  fi\n  CHECK=$(sha256sum
        $BINARIES)\n  SHA256=$(curl -L https://storage.googleapis.com/cri-containerd-release/$BINARIES.sha256)\n
        \ if [[ \"$CHECK\" == \"$SHA256  $BINARIES\" ]] ; then\n  \techo \"verified
        checksum for $BINARIES!\"\n    tar --no-overwrite-dir -C / -xzf $BINARIES\n
        \ else\n  \techo \"checksum does not match! computed $CHECK vs download $SHA256\"\n
        \ \texit 1\n  fi\n{{end -}}\n# Configure containerd\n\n{{if .ParamExists \"containerd/config\"
        -}}\necho \"Creating /etc/containerd/config.toml file from Param containerd/config\"\ncat
        <<EOF >/etc/containerd/config.toml\n{{.Param \"containerd/config\"}}\nEOF\ncat
        /etc/containerd/config.toml\n{{else -}}\necho \"Skipping custom etc/containerd/config.toml:
        No containerd/config defined\"\n{{end -}}\n\n# start containerd\nsystemctl
        start containerd\nsystemctl enable containerd\n\nctr --version\necho \"Containerd
        v{{.Param \"containerd/version\"}} installed successfully\"\nexit 0\n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: containerd-install.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    contrail.cfg.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        ---
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: env
          namespace: kube-system
        data:
          AAA_MODE: no-auth
          AUTH_MODE: noauth
          CLOUD_ORCHESTRATOR: kubernetes
          LOG_LEVEL: SYS_NOTICE
          METADATA_PROXY_SECRET: contrail
          RABBITMQ_NODE_PORT: "5673"
          ZOOKEEPER_ANALYTICS_PORT: "2182"
          ZOOKEEPER_PORTS: "2888:3888"
          ZOOKEEPER_NODES: {{ .Param "krib/cluster-master-vip" }}
          RABBITMQ_NODES: {{ .Param "krib/cluster-master-vip" }}
          CONTROLLER_NODES: {{ .Param "krib/cluster-master-vip" }}
          VROUTER_GATEWAY: {{ .Param "krib/cluster-master-vip" }}
        ---
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: configzookeeperenv
          namespace: kube-system
        data:
          ZOOKEEPER_PORT: "2181"
        ---
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: analyticszookeeperenv
          namespace: kube-system
        data:
          ZOOKEEPER_PORT: "2182"
        ---
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: nodemgr-config
          namespace: kube-system
        data:
          DOCKER_HOST: "unix://mnt/docker.sock"
        ---
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: contrail-analyticsdb-config
          namespace: kube-system
        data:
          CASSANDRA_SEEDS: {{ .Param "krib/cluster-master-vip" }}
          CASSANDRA_CLUSTER_NAME: Contrail
          CASSANDRA_START_RPC: "true"
          CASSANDRA_LISTEN_ADDRESS: auto
          CASSANDRA_PORT: "9160"
          CASSANDRA_CQL_PORT: "9042"
          CASSANDRA_SSL_STORAGE_PORT: "7001"
          CASSANDRA_STORAGE_PORT: "7000"
          CASSANDRA_JMX_LOCAL_PORT: "7200"
        ---
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: contrail-configdb-config
          namespace: kube-system
        data:
          CASSANDRA_SEEDS: {{ .Param "krib/cluster-master-vip" }}
          CASSANDRA_CLUSTER_NAME: ContrailConfigDB
          CASSANDRA_START_RPC: "true"
          CASSANDRA_LISTEN_ADDRESS: auto
          CASSANDRA_PORT: "9161"
          CASSANDRA_CQL_PORT: "9041"
          CASSANDRA_SSL_STORAGE_PORT: "7011"
          CASSANDRA_STORAGE_PORT: "7010"
          CASSANDRA_JMX_LOCAL_PORT: "7201"
        ---
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: rabbitmq-config
          namespace: kube-system
        data:
          RABBITMQ_ERLANG_COOKIE: "47EFF3BB-4786-46E0-A5BB-58455B3C2CB4"
        ---
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: kube-manager-config
          namespace: kube-system
        data:
          KUBERNETES_API_SERVER: {{ .Param "krib/cluster-master-vip" }}
          KUBERNETES_API_SECURE_PORT: "6443"
          K8S_TOKEN_FILE: "/tmp/serviceaccount/token"
        # Containers section
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: config-zookeeper
          namespace: kube-system
          labels:
            app: config-zookeeper
        spec:
          template:
            metadata:
              labels:
                app: config-zookeeper
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              hostNetwork: true
              containers:
              - name: config-zookeeper
                image: "docker.io/opencontrailnightly/contrail-external-zookeeper:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /var/lib/zookeeper
                  name: zookeeper-data
                - mountPath: /var/log/zookeeper
                  name: zookeeper-logs
              volumes:
              - name: zookeeper-data
                hostPath:
                  path: /var/lib/contrail/config-zookeeper
              - name: zookeeper-logs
                hostPath:
                  path: /var/log/contrail/config-zookeeper
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: analytics-zookeeper
          namespace: kube-system
          labels:
            app: analytics-zookeeper
        spec:
          template:
            metadata:
              labels:
                app: analytics-zookeeper
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              hostNetwork: true
              containers:
              - name: analytics-zookeeper
                image: "docker.io/opencontrailnightly/contrail-external-zookeeper:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: analyticszookeeperenv
                volumeMounts:
                - mountPath: /var/lib/zookeeper
                  name: zookeeper-data
                - mountPath: /var/log/zookeeper
                  name: zookeeper-logs
              volumes:
              - name: zookeeper-data
                hostPath:
                  path: /var/lib/contrail/analytics-zookeeper
              - name: zookeeper-logs
                hostPath:
                  path: /var/log/contrail/analytics-zookeeper
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: kafka
          namespace: kube-system
          labels:
            app: kafka
        spec:
          template:
            metadata:
              labels:
                app: kafka
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              hostNetwork: true
              containers:
              - name: kafka
                image: "docker.io/opencontrailnightly/contrail-external-kafka:latest"
                imagePullPolicy: ""
                env:
                - name: NODE_TYPE
                  value: database
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: analyticszookeeperenv
                volumeMounts:
                - mountPath: /tmp/kafka-logs
                  name: kafka-logs
              volumes:
              - name: kafka-logs
                hostPath:
                  path: /var/lib/contrail/kafka-logs
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: contrail-analyticsdb
          namespace: kube-system
          labels:
            app: contrail-analyticsdb
        spec:
          template:
            metadata:
              labels:
                app: contrail-analyticsdb
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              hostNetwork: true
              containers:
              - name: contrail-analyticsdb
                image: "docker.io/opencontrailnightly/contrail-external-cassandra:latest"
                imagePullPolicy: ""
                env:
                - name: NODE_TYPE
                  value: database
                envFrom:
                - configMapRef:
                    name: contrail-analyticsdb-config
                volumeMounts:
                - mountPath: /var/lib/cassandra
                  name: analyticsdb-data
                - mountPath: /var/log/cassandra
                  name: analyticsdb-logs
              volumes:
              - name: analyticsdb-data
                hostPath:
                  path: /var/lib/contrail/analyticsdb
              - name: analyticsdb-logs
                hostPath:
                  path: /var/log/contrail/analyticsdb
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: contrail-configdb
          namespace: kube-system
          labels:
            app: contrail-configdb
        spec:
          template:
            metadata:
              labels:
                app: contrail-configdb
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              hostNetwork: true
              containers:
              - name: contrail-configdb
                image: "docker.io/opencontrailnightly/contrail-external-cassandra:latest"
                imagePullPolicy: ""
                env:
                - name: NODE_TYPE
                  value: config
                envFrom:
                - configMapRef:
                    name: contrail-configdb-config
                volumeMounts:
                - mountPath: /var/lib/cassandra
                  name: configdb-data
                - mountPath: /var/log/cassandra
                  name: configdb-log
              volumes:
              - name: configdb-data
                hostPath:
                  path: /var/lib/contrail/configdb
              - name: configdb-log
                hostPath:
                  path: /var/log/contrail/configdb
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: contrail-database-nodemgr
          namespace: kube-system
          labels:
            app: contrail-database-nodemgr
        spec:
          template:
            metadata:
              labels:
                app: contrail-database-nodemgr
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              hostNetwork: true
              initContainers:
              - name: contrail-node-init
                image: "docker.io/opencontrailnightly/contrail-node-init:latest"
                imagePullPolicy: ""
                securityContext:
                  privileged: true
                env:
                - name: CONTRAIL_STATUS_IMAGE
                  value: "docker.io/opencontrailnightly/contrail-status:latest"
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: analyticszookeeperenv
                volumeMounts:
                - mountPath: /host/usr/bin
                  name: host-usr-bin
              containers:
              - name: contrail-database-nodemgr
                image: "docker.io/opencontrailnightly/contrail-nodemgr:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: nodemgr-config
                - configMapRef:
                    name: analyticszookeeperenv
                env:
                - name: NODE_TYPE
                  value: database
                - name: DATABASE_NODEMGR__DEFAULTS__minimum_diskGB
                  value: "2"
        # todo: there is type Socket in new kubernetes, it is possible to use full
        # path:
        # hostPath:
        #   path: /var/run/docker.sock and
        #   type: Socket
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: analyticsdb-logs
                - mountPath: /mnt
                  name: docker-unix-socket
              volumes:
              - name: analyticsdb-logs
                hostPath:
                  path: /var/log/contrail/analyticsdb
              - name: docker-unix-socket
                hostPath:
                  path: /var/run
              - name: host-usr-bin
                hostPath:
                  path: /usr/bin
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: contrail-analytics
          namespace: kube-system
          labels:
            app: contrail-analytics
        spec:
          template:
            metadata:
              labels:
                app: contrail-analytics
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              hostNetwork: true
              initContainers:
              - name: contrail-node-init
                image: "docker.io/opencontrailnightly/contrail-node-init:latest"
                imagePullPolicy: ""
                securityContext:
                  privileged: true
                env:
                - name: CONTRAIL_STATUS_IMAGE
                  value: "docker.io/opencontrailnightly/contrail-status:latest"
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: analyticszookeeperenv
                volumeMounts:
                - mountPath: /host/usr/bin
                  name: host-usr-bin
              containers:
              - name: contrail-analytics-api
                image: "docker.io/opencontrailnightly/contrail-analytics-api:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: analyticszookeeperenv
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: analytics-logs
              - name: contrail-analytics-collector
                image: "docker.io/opencontrailnightly/contrail-analytics-collector:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: analytics-logs
              - name: contrail-analytics-alarm-gen
                image: "docker.io/opencontrailnightly/contrail-analytics-alarm-gen:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: analyticszookeeperenv
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: analytics-logs
              - name: contrail-analytics-query-engine
                image: "docker.io/opencontrailnightly/contrail-analytics-query-engine:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: analytics-logs
              - name: contrail-analytics-snmp-collector
                image: "docker.io/opencontrailnightly/contrail-analytics-snmp-collector:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: analytics-logs
              - name: contrail-analytics-topology
                image: "docker.io/opencontrailnightly/contrail-analytics-topology:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: analyticszookeeperenv
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: analytics-logs
              - name: contrail-analytics-nodemgr
                image: "docker.io/opencontrailnightly/contrail-nodemgr:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: analyticszookeeperenv
                - configMapRef:
                    name: nodemgr-config
                env:
                - name: NODE_TYPE
                  value: analytics
        # todo: there is type Socket in new kubernetes, it is possible to use full
        # path:
        # hostPath:
        #   path: /var/run/docker.sock and
        #   type: Socket
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: analytics-logs
                - mountPath: /mnt
                  name: docker-unix-socket
              volumes:
              - name: analytics-logs
                hostPath:
                  path: /var/log/contrail/analytics
              - name: docker-unix-socket
                hostPath:
                  path: /var/run
              - name: host-usr-bin
                hostPath:
                  path: /usr/bin
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: contrail-controller-control
          namespace: kube-system
          labels:
            app: contrail-controller-control
        spec:
          template:
            metadata:
              labels:
                app: contrail-controller-control
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              hostNetwork: true
              initContainers:
              - name: contrail-node-init
                image: "docker.io/opencontrailnightly/contrail-node-init:latest"
                imagePullPolicy: ""
                securityContext:
                  privileged: true
                env:
                - name: CONTRAIL_STATUS_IMAGE
                  value: "docker.io/opencontrailnightly/contrail-status:latest"
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /host/usr/bin
                  name: host-usr-bin
              containers:
              - name: contrail-controller-control
                image: "docker.io/opencontrailnightly/contrail-controller-control-control:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: control-logs
              - name: contrail-controller-control-dns
                image: "docker.io/opencontrailnightly/contrail-controller-control-dns:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /etc/contrail
                  name: dns-config
                - mountPath: /var/log/contrail
                  name: control-logs
              - name: contrail-controller-control-named
                image: "docker.io/opencontrailnightly/contrail-controller-control-named:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                securityContext:
                  privileged: true
                volumeMounts:
                - mountPath: /etc/contrail
                  name: dns-config
                - mountPath: /var/log/contrail
                  name: control-logs
              - name: contrail-controller-nodemgr
                image: "docker.io/opencontrailnightly/contrail-nodemgr:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                - configMapRef:
                    name: nodemgr-config
                env:
                - name: NODE_TYPE
                  value: control
        # todo: there is type Socket in new kubernetes, it is possible to use full
        # path:
        # hostPath:
        #   path: /var/run/docker.sock and
        #   type: Socket
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: control-logs
                - mountPath: /mnt
                  name: docker-unix-socket
              volumes:
              - name: control-logs
                hostPath:
                  path: /var/log/contrail/control
              - name: docker-unix-socket
                hostPath:
                  path: /var/run
              - name: dns-config
                emptyDir: {}
              - name: host-usr-bin
                hostPath:
                  path: /usr/bin
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: contrail-controller-config
          namespace: kube-system
          labels:
            app: contrail-controller-config
        spec:
          template:
            metadata:
              labels:
                app: contrail-controller-config
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              hostNetwork: true
              initContainers:
              - name: contrail-node-init
                image: "docker.io/opencontrailnightly/contrail-node-init:latest"
                imagePullPolicy: ""
                securityContext:
                  privileged: true
                env:
                - name: CONTRAIL_STATUS_IMAGE
                  value: "docker.io/opencontrailnightly/contrail-status:latest"
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /host/usr/bin
                  name: host-usr-bin
              containers:
              - name: contrail-controller-config-api
                image: "docker.io/opencontrailnightly/contrail-controller-config-api:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: config-logs
              - name: contrail-controller-config-devicemgr
                image: "docker.io/opencontrailnightly/contrail-controller-config-devicemgr:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: config-logs
              - name: contrail-controller-config-schema
                image: "docker.io/opencontrailnightly/contrail-controller-config-schema:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: config-logs
              - name: contrail-controller-config-svcmonitor
                image: "docker.io/opencontrailnightly/contrail-controller-config-svcmonitor:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: config-logs
              - name: contrail-controller-config-nodemgr
                image: "docker.io/opencontrailnightly/contrail-nodemgr:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                - configMapRef:
                    name: nodemgr-config
                env:
                - name: NODE_TYPE
                  value: config
                - name: CASSANDRA_CQL_PORT
                  value: "9041"
                - name: CASSANDRA_JMX_LOCAL_PORT
                  value: "7201"
                - name: CONFIG_NODEMGR__DEFAULTS__minimum_diskGB
                  value: "2"
        # todo: there is type Socket in new kubernetes, it is possible to use full
        # path:
        # hostPath:
        #   path: /var/run/docker.sock and
        #   type: Socket
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: config-logs
                - mountPath: /mnt
                  name: docker-unix-socket
              volumes:
              - name: config-logs
                hostPath:
                  path: /var/log/contrail/config
              - name: docker-unix-socket
                hostPath:
                  path: /var/run
              - name: host-usr-bin
                hostPath:
                  path: /usr/bin
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: contrail-controller-webui
          namespace: kube-system
          labels:
            app: contrail-controller-webui
        spec:
          template:
            metadata:
              labels:
                app: contrail-controller-webui
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              hostNetwork: true
              initContainers:
              - name: contrail-node-init
                image: "docker.io/opencontrailnightly/contrail-node-init:latest"
                imagePullPolicy: ""
                securityContext:
                  privileged: true
                env:
                - name: CONTRAIL_STATUS_IMAGE
                  value: "docker.io/opencontrailnightly/contrail-status:latest"
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /host/usr/bin
                  name: host-usr-bin
              containers:
              - name: contrail-controller-webui-job
                image: "docker.io/opencontrailnightly/contrail-controller-webui-job:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: webui-logs
              - name: contrail-controller-webui-web
                image: "docker.io/opencontrailnightly/contrail-controller-webui-web:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: webui-logs
              volumes:
              - name: webui-logs
                hostPath:
                  path: /var/log/contrail/webui
              - name: host-usr-bin
                hostPath:
                  path: /usr/bin
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: redis
          namespace: kube-system
          labels:
            app: redis
        spec:
          template:
            metadata:
              labels:
                app: redis
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              hostNetwork: true
              containers:
              - name: redis
                image: "redis:4.0.2"
                imagePullPolicy: ""
                volumeMounts:
                - mountPath: /var/lib/redis
                  name: redis-data
                - mountPath: /var/log/redis
                  name: redis-logs
              volumes:
              - name: redis-data
                hostPath:
                  path: /var/lib/contrail/redis
              - name: redis-logs
                hostPath:
                  path: /var/log/contrail/redis
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: rabbitmq
          namespace: kube-system
          labels:
            app: rabbitmq
        spec:
          template:
            metadata:
              labels:
                app: rabbitmq
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              hostNetwork: true
              containers:
              - name: rabbitmq
                image: "docker.io/opencontrailnightly/contrail-external-rabbitmq:latest"
                imagePullPolicy: ""
                env:
                - name: NODE_TYPE
                  value: config
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                - configMapRef:
                    name: rabbitmq-config
                volumeMounts:
                - mountPath: /var/lib/rabbitmq
                  name: rabbitmq-data
                - mountPath: /var/log/rabbitmq
                  name: rabbitmq-logs
              volumes:
              - name: rabbitmq-data
                hostPath:
                  path: /var/lib/contrail/rabbitmq
              - name: rabbitmq-logs
                hostPath:
                  path: /var/log/contrail/rabbitmq
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: contrail-kube-manager
          namespace: kube-system
          labels:
            app: contrail-kube-manager
        spec:
          template:
            metadata:
              labels:
                app: contrail-kube-manager
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                    - matchExpressions:
                      - key: "node-role.kubernetes.io/master"
                        operator: Exists
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              automountServiceAccountToken: false
              hostNetwork: true
              initContainers:
              - name: contrail-node-init
                image: "docker.io/opencontrailnightly/contrail-node-init:latest"
                imagePullPolicy: ""
                securityContext:
                  privileged: true
                env:
                - name: CONTRAIL_STATUS_IMAGE
                  value: "docker.io/opencontrailnightly/contrail-status:latest"
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /host/usr/bin
                  name: host-usr-bin
              containers:
              - name: contrail-kube-manager
                image: "docker.io/opencontrailnightly/contrail-kubernetes-kube-manager:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                - configMapRef:
                    name: kube-manager-config
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: kube-manager-logs
                - mountPath: /tmp/serviceaccount
                  name: pod-secret
              volumes:
              - name: kube-manager-logs
                hostPath:
                  path: /var/log/contrail/kube-manager
              - name: pod-secret
                secret:
                  secretName: contrail-kube-manager-token
              - name: host-usr-bin
                hostPath:
                  path: /usr/bin
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: contrail-agent
          namespace: kube-system
          labels:
            app: contrail-agent
        spec:
          template:
            metadata:
              labels:
                app: contrail-agent
            spec:
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
              automountServiceAccountToken: false
              hostNetwork: true
              initContainers:
              - name: contrail-node-init
                image: "docker.io/opencontrailnightly/contrail-node-init:latest"
                imagePullPolicy: ""
                securityContext:
                  privileged: true
                env:
                - name: CONTRAIL_STATUS_IMAGE
                  value: "docker.io/opencontrailnightly/contrail-status:latest"
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /host/usr/bin
                  name: host-usr-bin
              - name: contrail-vrouter-kernel-init
                image: "docker.io/opencontrailnightly/contrail-vrouter-kernel-init:latest"
                imagePullPolicy: ""
                securityContext:
                  privileged: true
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /usr/src
                  name: usr-src
                - mountPath: /lib/modules
                  name: lib-modules
                - mountPath: /etc/sysconfig/network-scripts
                  name: network-scripts
                - mountPath: /host/bin
                  name: host-bin
              - name: contrail-kubernetes-cni-init
                image: "docker.io/opencontrailnightly/contrail-kubernetes-cni-init:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /var/lib/contrail
                  name: var-lib-contrail
                - mountPath: /host/etc_cni
                  name: etc-cni
                - mountPath: /host/opt_cni_bin
                  name: opt-cni-bin
                - mountPath: /host/log_cni
                  name: var-log-contrail-cni
                - mountPath: /var/log/contrail
                  name: agent-logs
              containers:
              - name: contrail-vrouter-agent
                image: "docker.io/opencontrailnightly/contrail-vrouter-agent:latest"
                imagePullPolicy: ""
                # TODO: Priveleged mode is requied because w/o it the device /dev/net/tun
                # is not present in the container. The mounting it into container
                # doesnt help because of permissions are not enough syscalls,
                # e.g. https://github.com/Juniper/contrail-controller/blob/master/src/vnsw/agent/contrail/linux/pkt0_interface.cc: 48.
                securityContext:
                  privileged: true
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                volumeMounts:
                - mountPath: /dev
                  name: dev
                - mountPath: /etc/sysconfig/network-scripts
                  name: network-scripts
                - mountPath: /host/bin
                  name: host-bin
                - mountPath: /var/log/contrail
                  name: agent-logs
                - mountPath: /usr/src
                  name: usr-src
                - mountPath: /lib/modules
                  name: lib-modules
                - mountPath: /var/lib/contrail
                  name: var-lib-contrail
                - mountPath: /var/crashes
                  name: var-crashes
                - mountPath: /tmp/serviceaccount
                  name: pod-secret
              - name: contrail-agent-nodemgr
                image: "docker.io/opencontrailnightly/contrail-nodemgr:latest"
                imagePullPolicy: ""
                envFrom:
                - configMapRef:
                    name: env
                - configMapRef:
                    name: configzookeeperenv
                - configMapRef:
                    name: nodemgr-config
                env:
                - name: NODE_TYPE
                  value: vrouter
        # todo: there is type Socket in new kubernetes, it is possible to use full
        # path:
        # hostPath:
        #   path: /var/run/docker.sock and
        #   type: Socket
                volumeMounts:
                - mountPath: /var/log/contrail
                  name: agent-logs
                - mountPath: /mnt
                  name: docker-unix-socket
              volumes:
              - name: dev
                hostPath:
                  path: /dev
              - name: network-scripts
                hostPath:
                  path: /etc/sysconfig/network-scripts
              - name: host-bin
                hostPath:
                  path: /bin
              - name: docker-unix-socket
                hostPath:
                  path: /var/run
              - name: pod-secret
                secret:
                  secretName: contrail-kube-manager-token
              - name: usr-src
                hostPath:
                  path: /usr/src
              - name: lib-modules
                hostPath:
                  path: /lib/modules
              - name: var-lib-contrail
                hostPath:
                  path: /var/lib/contrail
              - name: var-crashes
                hostPath:
                  path: /var/contrail/crashes
              - name: etc-cni
                hostPath:
                  path: /etc/cni
              - name: opt-cni-bin
                hostPath:
                  path: /opt/cni/bin
              - name: var-log-contrail-cni
                hostPath:
                  path: /var/log/contrail/cni
              - name: agent-logs
                hostPath:
                  path: /var/log/contrail/agent
              - name: host-usr-bin
                hostPath:
                  path: /usr/bin

        # Meta information section
        ---
        kind: ClusterRole
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: contrail-kube-manager
          namespace: kube-system
        rules:
          - apiGroups: ["*"]
            resources: ["*"]
            verbs: ["*"]
        ---
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: contrail-kube-manager
          namespace: kube-system
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRoleBinding
        metadata:
          name: contrail-kube-manager
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: contrail-kube-manager
        subjects:
        - kind: ServiceAccount
          name: contrail-kube-manager
          namespace: kube-system
        ---
        apiVersion: v1
        kind: Secret
        metadata:
          name: contrail-kube-manager-token
          namespace: kube-system
          annotations:
            kubernetes.io/service-account.name: contrail-kube-manager
        type: kubernetes.io/service-account-token
      Description: ""
      Endpoint: ""
      Errors: []
      ID: contrail.cfg.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    docker-install.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |+
        #!/usr/bin/env bash
        # Kubernetes Rebar Integrated Boot (KRIB) Docker Install
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}
        [[ $RS_UUID ]] && export RS_UUID="{{.Machine.UUID}}"

        {{if .ParamExists "docker/working-dir" -}}
        # Only do this if it exists.
        # if it isn't setup, don't use it.
        if [[ -e {{.Param "docker/working-dir"}} ]] ; then
          echo "Linking the docker working directory into place."
          ln -s {{.Param "docker/working-dir"}} /var/lib/docker
        fi
        {{end -}}

        {{if .ParamExists "kubectl/working-dir" -}}
        # Only do this if it exists.
        # if it isn't setup, don't use it.
        if [[ -e {{.Param "kubectl/working-dir"}} ]] ; then
          echo "Linking the kubectl working directory into place."
          ln -s {{.Param "kubectl/working-dir"}} /var/lib/kubectl
        fi
        {{end -}}

        if ! which docker ; then
          # Get latest docker...
          curl -fsSL https://get.docker.com/ | VERSION={{.Param "docker/version"}} sh
        fi

        {{if .ParamExists "docker/daemon"}}
        echo "Creating /etc/docker/daemon.json file from Param docker/daemon"
        mkdir -p /etc/docker
        cat <<EOF >/etc/docker/daemon.json
        {{.ParamAsJSON "docker/daemon"}}
        EOF
        cat /etc/docker/daemon.json
        {{else}}
        echo "Skipping custom etc/docker/daemon.json: No docker/daemon defined"
        {{end}}

        echo "Starting Docker Service"
        service docker enable
        service docker start

        echo "Docker installed successfully"
        exit 0

      Description: ""
      Endpoint: ""
      Errors: []
      ID: docker-install.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    etcd-config.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # Build an etcd cluster
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        # Skip the remainder of this template if this host is not a master in a selective-master deployment
        {{template "krib-skip-if-not-master.tmpl" .}}

        # we need a random backoff to avoid races.
        SLEEP=$[ ( $RANDOM % 30 ) ]

        export RS_UUID="{{.Machine.UUID}}"
        export RS_IP="{{.Machine.Address}}"

        ETCD_VERSION="{{ .Param "etcd/version" }}"

        # these need to be before krib-lib template
        {{if .ParamExists "etcd/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "etcd/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "etcd/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing etcd/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        build_cert() {
          local profile=$1
          local ca_name=$2
          local ca_pw=$3
          local myname=$4
          local myip=$5

          echo "Generating certificate for ${profile} with my name ${myname} and my IP ${myip}"
          drpcli machines runaction $RS_UUID getca certs/root $ca_name | jq -r . > /etc/kubernetes/pki/etcd/${profile}-ca.pem
          drpcli certs csr $ca_name $myname $RS_IP $myip $(hostname) {{if .ParamExists "krib/cluster-master-vip" }}{{ .Param "krib/cluster-master-vip" }}{{end}} > tmp.csr
          drpcli machines runaction $RS_UUID signcert certs/root $ca_name certs/root-pw $ca_pw certs/csr "$(jq .CSR tmp.csr)" certs/profile $profile | jq -r . > /etc/kubernetes/pki/etcd/$profile.pem
          jq -r .Key tmp.csr > /etc/kubernetes/pki/etcd/$profile-key.pem
          rm tmp.csr
        }

        {{if .ParamExists "etcd/servers" -}}
        get_endpoints() {
          ENDPOINTS=""
          {{ $port := .Param "etcd/client-port" -}}
          {{- range $elem := .Param "etcd/servers"}}
          ENDPOINTS="${ENDPOINTS},https://{{ $elem.Address }}:{{ $port }}"
          {{ end -}}
          ENDPOINTS="--endpoints=${ENDPOINTS:1}"
          echo ${ENDPOINTS}
        }


        get_member_list() {
          set +e
          ENDPOINTS=$1
          export ETCDCTL_API=3
          echo `etcdctl --cert=/etc/kubernetes/pki/etcd/client.pem \
           --key=/etc/kubernetes/pki/etcd/client-key.pem \
           --cacert=/etc/kubernetes/pki/etcd/server-ca.pem \
           --endpoints=${ENDPOINTS} \
           member list 2>/dev/null`
          set -e
        }

        get_member_id() {
          set +e
          ENDPOINTS=$1
          export ETCDCTL_API=3
          MEMBER_ID=`etcdctl --cert=/etc/kubernetes/pki/etcd/client.pem \
           --key=/etc/kubernetes/pki/etcd/client-key.pem \
           --cacert=/etc/kubernetes/pki/etcd/server-ca.pem \
           --endpoints=${ENDPOINTS} \
           member list | grep ${ETCD_IP} | awk -F', ' '{print $1}'`
          set -e
          echo ${MEMBER_ID}
        }
        {{ end -}}

        echo "Configure the etcd cluster"

        ETCD_CLUSTER_NAME={{.Param "etcd/name"}}
        ETCD_PEER_PORT={{.Param "etcd/peer-port"}}
        ETCD_CLIENT_PORT={{.Param "etcd/client-port"}}
        {{ if .ParamExists "etcd/ip" -}}
        ETCD_IP={{ .Param "etcd/ip" }}
        {{ else -}}
        ETCD_IP={{ .Machine.Address }}
        {{ end -}}

        # Either set controller IP to a param, or default to the DRP IP
        {{ if .ParamExists "etcd/controller-ip" -}}
        ETCD_CONTROLLER_IP={{ .Param "etcd/controller-ip" }}
        {{ else -}}
        ETCD_CONTROLLER_IP={{ .ProvisionerAddress }}
        {{ end -}}

        {{if eq (.ParamExists "etcd/servers") false -}}
        # add server management params if missing
        echo "Add initial variables to track members."
        drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "etcd/servers" to "[]" || true
        {{ end -}}

        {{if eq (.ParamExists "etcd/servers-done") false -}}
        drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "etcd/servers-done" to "[]" || true
        {{ end -}}

        # Get the number of servers to create
        ETCD_SERVER_COUNT={{.Param "etcd/server-count"}}
        echo "Creating $ETCD_SERVER_COUNT servers"

        echo "Electing etcd members to cluster profile: $CLUSTER_PROFILE"
        ETCD_INDEX=$(add_me_if_not_count "etcd/servers" $ETCD_SERVER_COUNT $ETCD_IP)

        echo "Added myself to cluster profile, my index is ${ETCD_INDEX}"

        if [[ $ETCD_INDEX == notme ]] ; then
          echo "I am not an ETCD server.  Move on."
          wait_for_count "etcd/servers-done" $ETCD_SERVER_COUNT
          exit 0
        fi

        CLIENT_CA=${ETCD_CLUSTER_NAME}-client-ca
        SERVER_CA=${ETCD_CLUSTER_NAME}-server-ca
        PEER_CA=${ETCD_CLUSTER_NAME}-peer-ca

        EXISTING_INDEX=$(find_me "etcd/servers-done" "Uuid" $RS_UUID)
        if [[ $EXISTING_INDEX == notme ]] ; then
          {{if .ParamExists "certs/root" -}}

          echo "Certs plugin detected....setting up CA"
          # If we are INDEX=0, let's setup the root certs for building keys
          if [[ $ETCD_INDEX == "0" ]] ; then
            echo "We are first machine in cluster, setting up the root certs..."
            # Are certs built yet?
            if ! drpcli machines runaction $RS_UUID getca certs/root $CLIENT_CA 2>/dev/null >/dev/null ; then
              CLIENT_CA_PW=$(drpcli machines runaction $RS_UUID makeroot certs/root $CLIENT_CA | jq -r .)
              echo "  Client CA created as $CLIENT_CA and now adding to profile: $CLUSTER_PROFILE"
              drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "etcd/client-ca-name" to "$CLIENT_CA" || true
              drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "etcd/client-ca-pw" to "$CLIENT_CA_PW" || true
            else
              if [[ $(get_param "etcd/client-ca-pw") == null ]] ; then
                xiterr 1 "Client CA Exists ($CLIENT_CA), but we did not set password.  Need to reset!!"
              else
                echo "  Client CA $CLIENT_CA configured"
              fi
            fi

            if ! drpcli machines runaction $RS_UUID getca certs/root $SERVER_CA 2>/dev/null >/dev/null ; then
              SERVER_CA_PW=$(drpcli machines runaction $RS_UUID makeroot certs/root $SERVER_CA | jq -r .)
              drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "etcd/server-ca-name" to "$SERVER_CA" || true
              drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "etcd/server-ca-pw" to "$SERVER_CA_PW" || true
            else
              if [[ $(get_param "etcd/server-ca-pw") == null ]] ; then
                xiterr 1 "SERVER CA Exists, but we did not set password.  Need to reset data in certs-data profile!!"
              fi
            fi

            if ! drpcli machines runaction $RS_UUID getca certs/root $PEER_CA 2>/dev/null >/dev/null ; then
              PEER_CA_PW=$(drpcli machines runaction $RS_UUID makeroot certs/root $PEER_CA | jq -r .)
              drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "etcd/peer-ca-name" to "$PEER_CA" || true
              drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "etcd/peer-ca-pw" to "$PEER_CA_PW" || true
            else
              if [[ $(get_param "etcd/peer-ca-pw") == null ]] ; then
                xiterr 1 "PEER CA Exists, but we didn't set password.  Need to reset!!"
              fi
            fi

            # On the assumption that we'll want to backup the etcd cluster externally (from DRP host or a "controller")
            # Generate a client cert for the controller
            echo "Generating controller client certificate with IP ${ETCD_CONTROLLER_IP}"
            # First, get the CA cert into a param
            drpcli machines runaction $RS_UUID getca certs/root $CLIENT_CA | jq -r . > tmp.ca
            drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "etcd/server-ca-cert" to "\"$(awk '{printf "%s\\n", $0}' tmp.ca)\""

            # Next, generate a client cert and key for the controller's IP
            drpcli certs csr client $ETCD_CONTROLLER_IP > tmp.csr
            drpcli machines runaction $RS_UUID signcert certs/root $CLIENT_CA certs/root-pw $CLIENT_CA_PW certs/csr "$(jq .CSR tmp.csr)" certs/profile client | jq -r . > controller.pem
            jq -r .Key tmp.csr > controller.key

            # Now put the etcd controller cert details _back_ into params, to be extracted on the controller
            drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "etcd/controller-client-cert" to "\"$(awk '{printf "%s\\n", $0}' controller.pem)\""
            drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "etcd/controller-client-key" to "\"$(awk '{printf "%s\\n", $0}' controller.key)\""

            # Clean up
            rm {tmp.ca,tmp.csr,controller.pem,controller.key}

          fi
          {{else -}}
          xiterr 1 "STAGE REQUIRES CERT PLUGIN!!  It is freely available, download from RackN SaaS."
          {{end}}
        fi

        echo "Waiting for ${ETCD_SERVER_COUNT} servers to start"
        wait_for_count "etcd/servers" $ETCD_SERVER_COUNT

        echo "${ETCD_SERVER_COUNT} servers started!"

        CLIENT_CA_PW=$(wait_for_variable "etcd/client-ca-pw")
        SERVER_CA_PW=$(wait_for_variable "etcd/server-ca-pw")
        PEER_CA_PW=$(wait_for_variable "etcd/peer-ca-pw")

        mkdir -p /var/lib/etcd
        mkdir -p /etc/kubernetes/pki/etcd

        ETCD_NAME="etcd$ETCD_INDEX"

        build_cert "client" $CLIENT_CA $CLIENT_CA_PW $ETCD_NAME $ETCD_IP
        build_cert "server" $SERVER_CA $SERVER_CA_PW $ETCD_NAME $ETCD_IP
        build_cert "peer" $PEER_CA $PEER_CA_PW $ETCD_NAME $ETCD_IP

        ETCD_URLS=""
        INDEX=0
        OLD_IFS=$IFS
        IFS=" " ; while read ip ; do
          if [[ $INDEX -gt 0 ]] ; then
            ETCD_URLS="${ETCD_URLS},"
          fi
          ETCD_URLS="${ETCD_URLS}etcd${INDEX}=https://${ip}:${ETCD_PEER_PORT}"
          INDEX=$(($INDEX+1))
        done <<< $(get_param "etcd/servers" | jq -r '.[].Address')
        IFS=$OLD_IFS

        TMP_DIR=/tmp/etcd-tmp
        INSTALL_DIR=/usr/local/bin
        if [[ $OS_FAMILY == coreos ]] ; then
          INSTALL_DIR=/opt/bin
        fi

        mkdir -p ${TMP_DIR}

        echo "Download etcd version: v${ETCD_VERSION}"
        # Allow for a local repository for installation files
        {{if .ParamExists "krib/package-repository" -}}
        KRIB_REPO={{.Param "krib/package-repository"}}
        {{end -}}

        if [[ ! -z "$KRIB_REPO" ]] ; then
          download -L ${KRIB_REPO}/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz -o ${TMP_DIR}/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz
        else
          download -L https://github.com/coreos/etcd/releases/download/v${ETCD_VERSION}/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz -o ${TMP_DIR}/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz
        fi

        echo "Install etcd version: ${ETCD_VERSION}"
        tar -C ${INSTALL_DIR} -xzf ${TMP_DIR}/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz --strip-components=1

        CLUSTER_STATE="new"
        if [[ $EXISTING_INDEX != notme ]] ; then
          CLUSTER_STATE="existing"
          ENDPOINTS=$(get_endpoints)

          if [[ "$(get_member_list ${ENDPOINTS})" == "" ]] ; then
            xiterr 1 "etcd/servers-done is not empty but no cluster members are up.  Did you forget to run krib-cluster-reset?"
          fi

          MEMBER_ID=$(get_member_id ${ENDPOINTS})
          export ETCDCTL_API=3

          if [[ "${MEMBER_ID}" != "" ]] ; then
            etcdctl --cert=/etc/kubernetes/pki/etcd/client.pem \
             --key=/etc/kubernetes/pki/etcd/client-key.pem \
             --cacert=/etc/kubernetes/pki/etcd/server-ca.pem \
             --endpoints=${ENDPOINTS} \
             member remove ${MEMBER_ID}
          fi

          etcdctl --cert=/etc/kubernetes/pki/etcd/client.pem \
           --key=/etc/kubernetes/pki/etcd/client-key.pem \
           --cacert=/etc/kubernetes/pki/etcd/server-ca.pem \
           --endpoints=${ENDPOINTS} \
           member add ${ETCD_NAME} --peer-urls https://${ETCD_IP}:${ETCD_PEER_PORT}
        fi

        cat << EOF > /etc/etcd.env
        PEER_NAME=${ETCD_NAME}
        ADVERTISE_IP=${ETCD_IP}
        CLIENT_PORT=${ETCD_CLIENT_PORT}
        PEER_PORT=${ETCD_PEER_PORT}
        URLS=${ETCD_URLS}
        EOF

        cat >/etc/systemd/system/etcd.service <<EOF
        [Unit]
        Description=etcd
        Documentation=https://github.com/coreos/etcd
        Conflicts=etcd.service
        Conflicts=etcd2.service

        [Service]
        EnvironmentFile=/etc/etcd.env
        Type=notify
        Restart=always
        RestartSec=5s
        LimitNOFILE=40000
        TimeoutStartSec=0

        ExecStart=${INSTALL_DIR}/etcd \
         --name \${PEER_NAME} \
         --data-dir /docker/etcd \
         --listen-client-urls https://\${ADVERTISE_IP}:\${CLIENT_PORT},http://127.0.0.1:\${CLIENT_PORT} \
         --advertise-client-urls https://\${ADVERTISE_IP}:\${CLIENT_PORT} \
         --listen-peer-urls https://\${ADVERTISE_IP}:\${PEER_PORT},http://127.0.0.1:\${PEER_PORT} \
         --initial-advertise-peer-urls https://\${ADVERTISE_IP}:\${PEER_PORT} \
         --cert-file=/etc/kubernetes/pki/etcd/server.pem \
         --key-file=/etc/kubernetes/pki/etcd/server-key.pem \
         --client-cert-auth --trusted-ca-file=/etc/kubernetes/pki/etcd/client-ca.pem \
         --peer-cert-file=/etc/kubernetes/pki/etcd/peer.pem --peer-key-file=/etc/kubernetes/pki/etcd/peer-key.pem \
         --peer-client-cert-auth --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/peer-ca.pem \
         --initial-cluster \${URLS} --initial-cluster-token my-etcd-token --initial-cluster-state ${CLUSTER_STATE}

        [Install]
        WantedBy=multi-user.target
        EOF

        systemctl daemon-reload
        systemctl enable etcd
        systemctl start etcd
        systemctl status etcd

        echo "Etcd started, lets go!"
        add_me_if_not_count "etcd/servers-done" $ETCD_SERVER_COUNT $ETCD_IP

        echo "Waiting for ${ETCD_SERVER_COUNT} servers to complete etcd install"
        wait_for_count "etcd/servers-done" $ETCD_SERVER_COUNT

        rm -rf ${TMP_DIR}

        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: etcd-config.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    helm-rook.after.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |2-
                if [[ $(kubectl -n kube-system get services -o json | jq -r '.items | contains([{"metadata":{"name":"nginx-ingress-controller"}}])') == "true" ]] ; then
                  echo "Wait until nginx-ingress-controller gets a LoadBalancer IP"
                  INGRESSIP=$(wait_for_ingress)

                  echo "expanding inline template to /tmp/rook-ceph-mgr-db-ingress.yaml"
                  cat > /tmp/rook-ceph-mgr-db-ingress.yaml << EOF
        apiVersion: extensions/v1beta1
        kind: Ingress
        metadata:
          name: rook-ceph-mgr-db
          namespace: rook-ceph
          annotations:
            kubernetes.io/ingress.class: nginx
            nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
        {{- if .ParamExists "certmanager/email"}}{{- if .ParamExists "ingress/rook-dashboard-hostname"}}
            kubernetes.io/tls-acme: "true"
        {{- end}}{{- end}}
        spec:
          rules:
        {{- if .ParamExists "ingress/rook-dashboard-hostname"}}
          - host: "{{.Param "ingress/rook-dashboard-hostname"}}"
        {{- else}}
          - host: "rook-db.$INGRESSIP.xip.io"
        {{- end}}
            http:
              paths:
              - backend:
                  serviceName: rook-ceph-mgr-db
                  servicePort: 8443
          tls:
          - hosts:
        {{- if .ParamExists "ingress/rook-dashboard-hostname"}}
            - "{{.Param "ingress/rook-dashboard-hostname"}}"
        {{- else}}
            - "rook-db.$INGRESSIP.xip.io"
        {{- end}}
        {{- if .ParamExists "certmanager/email"}}{{- if .ParamExists "ingress/rook-dashboard-hostname"}}
            secretName: rook-ceph-mgr-db-tls
        {{- end}}{{- end}}
        EOF

                  if [[ $(kubectl get -n rook-system ingress -o json | jq -r '.items | contains([{"metadata":{"name":"rook-ceph-mgr-db"}}])') == "true" ]] ; then
                    echo "Removing existing rook-ceph-mgr-db Ingress"
                    kubectl delete -f /tmp/rook-ceph-mgr-db-ingress.yaml
                  fi
                  echo "Creating rook-ceph-mgr-db Ingress"
                  kubectl apply -f /tmp/rook-ceph-mgr-db-ingress.yaml
        {{- if .ParamExists "ingress/rook-dashboard-hostname"}}
                  echo "Ensure dns record for {{.Param "ingress/rook-dashboard-hostname"}} points to $INGRESSIP"
        {{- else}}
                  echo "Rook dashboard ingress at https://rook-db.$INGRESSIP.xip.io"
        {{- end}}
                fi
      Description: ""
      Endpoint: ""
      Errors: []
      ID: helm-rook.after.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    helm-rook.before.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        {{- if .ParamExists "rook/data-dir-host-path"}}
                echo "Creating rook/data-dir-host-path if it doesn't exist"
                mkdir -p {{.Param "rook/data-dir-host-path"}}
        {{- end}}
      Description: ""
      Endpoint: ""
      Errors: []
      ID: helm-rook.before.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    helm-rook.cfg.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        # Rook Yaml - See https://rook.io/docs/rook/master/ceph-quickstart.html
        # This example first defines some necessary namespace and RBAC security objects.
        # The actual Ceph Cluster CRD example can be found at the bottom of this example.
        apiVersion: v1
        kind: Namespace
        metadata:
          name: rook-ceph
        ---
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: rook-ceph-osd
          namespace: rook-ceph
        ---
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: rook-ceph-mgr
          namespace: rook-ceph
        ---
        kind: Role
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: rook-ceph-osd
          namespace: rook-ceph
        rules:
        - apiGroups: [""]
          resources: ["configmaps"]
          verbs: [ "get", "list", "watch", "create", "update", "delete" ]
        ---
        # Aspects of ceph-mgr that require access to the system namespace
        kind: Role
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: rook-ceph-mgr-system
          namespace: rook-ceph
        rules:
        - apiGroups:
          - ""
          resources:
          - configmaps
          verbs:
          - get
          - list
          - watch
        ---
        # Aspects of ceph-mgr that operate within the cluster's namespace
        kind: Role
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: rook-ceph-mgr
          namespace: rook-ceph
        rules:
        - apiGroups:
          - ""
          resources:
          - pods
          - services
          verbs:
          - get
          - list
          - watch
        - apiGroups:
          - batch
          resources:
          - jobs
          verbs:
          - get
          - list
          - watch
          - create
          - update
          - delete
        - apiGroups:
          - ceph.rook.io
          resources:
          - "*"
          verbs:
          - "*"
        ---
        # Allow the operator to create resources in this cluster's namespace
        kind: RoleBinding
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: rook-ceph-cluster-mgmt
          namespace: rook-ceph
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: rook-ceph-cluster-mgmt
        subjects:
        - kind: ServiceAccount
          name: rook-ceph-system
          namespace: rook-ceph-system
        ---
        # Allow the osd pods in this namespace to work with configmaps
        kind: RoleBinding
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: rook-ceph-osd
          namespace: rook-ceph
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: rook-ceph-osd
        subjects:
        - kind: ServiceAccount
          name: rook-ceph-osd
          namespace: rook-ceph
        ---
        # Allow the ceph mgr to access the cluster-specific resources necessary for the mgr modules
        kind: RoleBinding
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: rook-ceph-mgr
          namespace: rook-ceph
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: rook-ceph-mgr
        subjects:
        - kind: ServiceAccount
          name: rook-ceph-mgr
          namespace: rook-ceph
        ---
        # Allow the ceph mgr to access the rook system resources necessary for the mgr modules
        kind: RoleBinding
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: rook-ceph-mgr-system
          namespace: rook-ceph-system
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: rook-ceph-mgr-system
        subjects:
        - kind: ServiceAccount
          name: rook-ceph-mgr
          namespace: rook-ceph
        ---
        # Allow the ceph mgr to access cluster-wide resources necessary for the mgr modules
        kind: RoleBinding
        apiVersion: rbac.authorization.k8s.io/v1beta1
        metadata:
          name: rook-ceph-mgr-cluster
          namespace: rook-ceph
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: rook-ceph-mgr-cluster
        subjects:
        - kind: ServiceAccount
          name: rook-ceph-mgr
          namespace: rook-ceph
        ---
        #################################################################################
        # The Ceph Cluster CRD example
        #################################################################################
        apiVersion: ceph.rook.io/v1
        kind: CephCluster
        metadata:
          name: rook-ceph
          namespace: rook-ceph
        spec:
          dataDirHostPath: {{.Param "rook/data-dir-host-path"}}
          dashboard:
            enabled: true
          mon:
            count: 3
            allowMultiplePerNode: true
          cephVersion:
            image: ceph/ceph:v13.2.2-20181023
          storage:
            useAllNodes: true
            useAllDevices: false
            config:
              databaseSizeMB: "1024"
              journalSizeMB: "1024"
        ---
        #################################################################################
        # The Ceph Manager Dashboard Service
        #################################################################################
        apiVersion: v1
        kind: Service
        metadata:
          name: rook-ceph-mgr-db
          namespace: rook-ceph
          labels:
            app: rook-ceph-mgr
            rook_cluster: rook-ceph
        spec:
          ports:
          - name: dashboard
            port: 8443
            protocol: TCP
            targetPort: 8443
          selector:
            app: rook-ceph-mgr
            rook_cluster: rook-ceph
          sessionAffinity: None
          type: ClusterIP
      Description: ""
      Endpoint: ""
      Errors: []
      ID: helm-rook.cfg.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    k3s-config.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # k3s part of Kubernetes Rebar Integrated Boot (KRIB) Kubeadm Installer
        # Copyright RackN 2019
        set -e

        # Get access and who we are.
        {{ template "setup.tmpl" .}}

        if [[ {{ .Param "krib/k3s" }} != true ]]; then
          echo "krib/k3s must be true"
          exit 1
        fi

        # Fix hostname lookup
        echo "{{.Machine.Address}} $(hostname -s) $(hostname)" >> /etc/hosts

        echo "Starting krib/k3s enabled stage"
        drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"black\", \"icon\": \"cube\" }}" | jq .Meta

        {{ if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{ .Param "krib/cluster-profile" }}
        PROFILE_TOKEN={{ .GenerateProfileToken (.Param "krib/cluster-profile") 7200 }}
        {{ else -}}
        xiterr 1 "Missing krib/cluster-profile on the machine!"
        {{ end -}}

        {{ template "krib-lib.sh.tmpl" .}}

        echo "Download k3s (for now only use latest and AMD64 arch)"

        KRIB_REPO={{ .Param "krib/repo" }}
        RELEASE=`curl -w "%{url_effective}" -I -L -s -S ${KRIB_REPO}/latest -o /dev/null | sed -e 's|.*/||'`
        TMP_DIR=/tmp/k3s-tmp
        INSTALL_DIR=/usr/bin
        SYSTEMD_DIR="/etc/systemd/system"
        K3SBIN="$INSTALL_DIR/k3s"

        if ! which k3s ; then
          echo "Download k3s ${RELEASE} from ${KRIB_REPO}/download/${RELEASE}/k3s to $K3SBIN"
          if [[ ! -z "$KRIB_REPO" ]] ; then
            download -L --remote-name-all "${KRIB_REPO}/download/${RELEASE}/k3s" -o $K3SBIN
          else
            echo "missing REPO AND RELEASE"
            exit 1
          fi
          if [ -f $K3SBIN ] ; then
            chmod +x $K3SBIN
          else
            echo "$K3SBIN not installed on system"
            exit 1
          fi
        else
          echo "found $K3SBIN binary, moving on..."
        fi

        echo "Configure master or nodes..."

        {{ if .ParamExists "krib/ip" -}}
        KRIB_IP={{ .Param "krib/ip" }}
        {{ else -}}
        KRIB_IP={{ .Machine.Address }}
        {{ end -}}

        # we need a random backoff to avoid races.
        SLEEP=$[ ( $RANDOM % 25 ) ]
        sleep $SLEEP

        MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
        echo "My Master index is $MASTER_INDEX"
        echo "k3s version: $(k3s -v)"

        CLUSTERNAME={{ .Param "krib/cluster-name" }}

        if [[ $MASTER_INDEX != notme ]] ; then

          echo "I am master - run k3s server"

          EXECSTART="${K3SBIN} server --bind-address={{ .Param "krib/cluster-master-vip" }} --https-listen-port={{ .Param "krib/cluster-api-port" }}"
          SYSTEMD_TYPE="notify"

          drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"yellow\", \"icon\": \"anchor\" }}" | jq .Meta

        else

          echo "I am a node - run k3s agent"

          wait_for_variable $KRIB_JOIN_PARAM
          # we need to get the decoded version
          TOKEN=$(drpcli -T $PROFILE_TOKEN profiles get $CLUSTER_PROFILE param $KRIB_JOIN_PARAM --decode)

          echo "Running Agent: join $TOKEN"
          EXECSTART="${K3SBIN} agent --token $TOKEN"
          SYSTEMD_TYPE="exec"

          # Set machine icon and color for KRIB cluster building
          drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"yellow\", \"icon\": \"ship\" }}" | jq .Meta

        fi

        MODEPROBE=$(which modprobe)

        ## Build Systemd environment vars
        tee /etc/systemd/system/k3s.service.env >/dev/null << EOF
        K3S_URL=https://{{ .Param "krib/cluster-master-vip" }}:{{ .Param "krib/cluster-api-port" }}
        EOF

        ## Build Systemd service
        tee /etc/systemd/system/k3s.service >/dev/null << EOF
        [Unit]
        Description=Lightweight Kubernetes
        Documentation=https://k3s.io
        After=network-online.target

        [Service]
        Type=${SYSTEMD_TYPE}
        EnvironmentFile=/etc/systemd/system/k3s.service.env
        ExecStartPre=${MODEPROBE} br_netfilter
        ExecStartPre=${MODEPROBE} overlay
        ExecStart=${EXECSTART}
        KillMode=process
        Delegate=yes
        LimitNOFILE=infinity
        LimitNPROC=infinity
        LimitCORE=infinity
        TasksMax=infinity
        TimeoutStartSec=0
        Restart=always
        RestartSec=5s

        [Install]
        WantedBy=multi-user.target
        EOF

        systemctl daemon-reload && systemctl start k3s
        sleep 15

        if [[ $MASTER_INDEX != notme ]] ; then
          # need more delay for master
          sleep 15
          echo "verify install"
          k3s kubectl get nodes
          echo "Recording cluster admin config ..."
          drpcli -T $PROFILE_TOKEN profiles add $CLUSTER_PROFILE param $KRIB_ADMIN_CONF_PARAM to - < /etc/rancher/k3s/k3s.yaml
          if [[ -f /var/lib/rancher/k3s/server/node-token ]] ; then
            # collect the cluster bootstrap token, then store it on the Param
            TOKEN=$(cat /var/lib/rancher/k3s/server/node-token)
            drpcli -T $PROFILE_TOKEN profiles add $CLUSTER_PROFILE param $KRIB_JOIN_PARAM to "$TOKEN"
          else
            echo "halting, could not find /var/lib/rancher/k3s/server/node-token"
            exit 1
          fi
          drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"green\", \"icon\": \"anchor\" }}" | jq .Meta
        else
          # Set machine icon and color for KRIB cluster building
          drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"green\", \"icon\": \"ship\" }}" | jq .Meta
        fi

        echo "Finished successfully"
        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: k3s-config.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-config.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: "#!/usr/bin/env bash\n# Kubernetes Rebar Integrated Boot (KRIB) Kubeadm
        Installer\nset -e\n\n# Get access and who we are.\n{{ template \"setup.tmpl\"
        .}}\n\n{{ if .ParamExists \"krib/cluster-profile\" -}}\nCLUSTER_PROFILE={{
        .Param \"krib/cluster-profile\" }}\nPROFILE_TOKEN={{ .GenerateProfileToken
        (.Param \"krib/cluster-profile\") 7200 }}\n{{ else -}}\nxiterr 1 \"Missing
        krib/cluster-profile on the machine!\"\n{{ end -}}\n\n{{ template \"krib-lib.sh.tmpl\"
        .}}\n\nset_taint() {\n  {{ if eq (.Param \"krib/cluster-masters-untainted\")
        true -}}\n  echo \"Removing taint from master nodes:\"\n  kubectl taint nodes
        -l node-role.kubernetes.io/master node-role.kubernetes.io/master- || true\n
        \ {{ else -}}\n  echo \"Ensure taint on master nodes:\"\n  kubectl taint nodes
        -l node-role.kubernetes.io/master node-role.kubernetes.io/master=:NoSchedule
        || true\n  {{ end -}}\n}\n\necho \"Configure kubeadm master and minions...\"\n\nCLUSTER_NAME={{
        .Param \"krib/cluster-name\" }}\n\necho \"MAKE SURE SWAP IS OFF!- kubeadm
        requirement\"\nif free | grep -q Swap ; then\n  swapoff -a\n  if [ -f /etc/fstab
        ]; then\n    sed -i /swap/d /etc/fstab\n  fi\nfi\n\necho \"MAKE SURE kernel
        module br_netfilter is loaded - kubeadm requirement\"\nif ! lsmod | grep ^br_netfilter
        > /dev/null ; then\n  modprobe br_netfilter\n  echo \"br_netfilter\" > /etc/modules-load.d/br_netfilter.conf\nfi\n\necho
        \"MAKE SURE bridge-nf-call-iptables CONTAINS 1 - kubeadm requirement\"\nif
        [ ! -f /etc/sysctl.d/99-net.bridge.bridge-nf-call-iptables ]; then\n  cat
        <<EOF >/etc/sysctl.d/99-net.bridge.bridge-nf-call-iptables\nnet.bridge.bridge-nf-call-iptables=1\nEOF\n\n
        \ sysctl -p /etc/sysctl.d/99-net.bridge.bridge-nf-call-iptables\nfi\n\nVALUE=$(sysctl
        -n net.bridge.bridge-nf-call-iptables)\n[ $VALUE != 1 ] && sysctl -w net.bridge.bridge-nf-call-iptables=1\n\nmkdir
        -p /etc/kubernetes/manifests\n\n{{ if .ParamExists \"krib/ip\" -}}\nKRIB_IP={{
        .Param \"krib/ip\" }}\n{{ else -}}\nKRIB_IP={{ .Machine.Address }}\n{{ end
        -}}\n\n# we need a random backoff to avoid races.\nSLEEP=$[ ( $RANDOM % 25
        ) ]\nsleep $SLEEP\n\n# Making sure I use the right cluster InternalIP\necho
        \"KUBELET_EXTRA_ARGS=--node-ip=${KRIB_IP}\" > /etc/default/kubelet\n\nMASTER_INDEX=$(find_me
        $KRIB_MASTERS_PARAM \"Uuid\" $RS_UUID)\necho \"My Master index is $MASTER_INDEX\"\nif
        [[ $MASTER_INDEX != notme ]] ; then\n\n  echo \"I am master - run kubeadm\"\n
        \ export KUBECONFIG=/etc/kubernetes/admin.conf\n\n  # todo: why does this
        have a default value?\n  {{ if .ParamExists \"krib/cluster-bootstrap-token\"
        -}}{{ if ne (.Param \"krib/cluster-bootstrap-token\") \"fedcba.fedcba9876543210\"
        -}}\n  echo \"looks like master recovery on existing cluster, just run kubeadm\"\n
        \ if [[ $MASTER_INDEX == 0 ]] ; then\n    MASTER_INDEX={{.Param \"etcd/server-count\"}}\n
        \ fi\n  {{ end -}}{{ end -}}\n\n  if [[ $MASTER_INDEX == 0 ]] ; then\n\n    echo
        \"Starting Master kubeadm init process.\"\n\n    # generate a cluster bootstrap
        token, then store it on the Param\n    # for the kubeadm.cfg file\n    TOKEN=$(kubeadm
        token generate)\n    drpcli -T $PROFILE_TOKEN profiles add $CLUSTER_PROFILE
        param $KRIB_BOOTSTRAP_TOKEN to \"$TOKEN\"\n\n    {{ if .ParamExists \"krib/kubeadm-cfg\"
        -}}\n    echo \"Using specified 'kubeadm.cfg' instead of DRP template (krib-kubeadm.cfg.sh.tmpl).\"\n
        \   KC={{ .Param \"krib/kubeadm-cfg\" }}\n    echo \"($KC)\"\n    echo \"#
        externally defined template specified by 'krib/kubeadm-cfg'\" > /tmp/kubeadm.cfg\n
        \   echo \"# source: '$KC'\" >> /tmp/kubeadm.cfg\n    T=$(mktemp /tmp/kube.cfg.XXXXXXXXX)\n
        \   curl -gfsSL -o $T $KC\n    cat $T >> /tmp/kubeadm.cfg && rm -f $T\n    {{
        end -}}\n\n    kubeadm init --config=/tmp/kubeadm.cfg --ignore-preflight-errors
        {{ .Param \"krib/ignore-preflight-errors\" }} 2>&1 | tee -a /tmp/kubeadm.init.log\n\n
        \   echo \"Building certificate file.\"\n    tar -zcvf /tmp/cert.tgz \\\n
        \     /etc/kubernetes/pki/ca.crt \\\n      /etc/kubernetes/pki/ca.key \\\n
        \     /etc/kubernetes/pki/sa.key \\\n      /etc/kubernetes/pki/sa.pub \\\n
        \     /etc/kubernetes/pki/front-proxy-ca.*\n\n    base64 -w 0 /tmp/cert.tgz
        > /tmp/cert.b64\n    drpcli -T $PROFILE_TOKEN profiles add $CLUSTER_PROFILE
        param $KRIB_MASTER_CERTS_PARAM to /tmp/cert.b64\n    rm /tmp/cert.tgz /tmp/cert.b64\n\n
        \   JOIN_CMD=$(kubeadm token create --print-join-command)\n\n    NET_TYPE={{
        .Param \"krib/networking-provider\" }}\n\n    # example of appending a featureGate
        to cluster after initial init\n    #kubeadm init --feature-gates=CoreDNS=true
        | tee -a ~/kubeadm_init.log\n\n{{ if .ParamExists \"krib/sign-kubelet-server-certs\"
        -}}{{ if eq (.Param \"krib/sign-kubelet-server-certs\") true -}}\n    # We
        are using serverTLSBootstrap for kublets, so enable auto-approval of kubelet
        server CSRs\n\n  {{ if .ParamExists \"krib/kubelet-rubber-stamp-container-image\"
        }}\n  # Replace default container image with the one specified in \"krib/nginx-ingress-controller-container-image\"\n
        \ sed -i 's|quay.io/kontena/kubelet-rubber-stamp-amd64|{{ .Param \"krib/kubelet-rubber-stamp-container-image\"
        }}|' /tmp/krib-kubelet-rubber-stamp.yaml\n  {{ end -}}\n\n    kubectl create
        -f /tmp/krib-kubelet-rubber-stamp.yaml\n    rm /tmp/krib-kubelet-rubber-stamp.yaml\n{{
        end -}}{{ end -}}\n\n    case $NET_TYPE in\n      calico)\n        echo \"Starting
        calico networking...\"\n        # Disable NetworkManager interference per
        https://docs.projectcalico.org/v3.8/maintenance/troubleshooting#configure-networkmanager\n
        \       if [ -d \"/etc/NetworkManager/conf.d\" ]; then\n          echo -e
        \"[keyfile]\\nunmanaged-devices=interface-name:cali*;interface-name:tunl*\"
        > /etc/NetworkManager/conf.d/calico.conf\n        fi\n\n        curl -gfsSL
        {{ .Param \"provider/calico-config\" }} > /tmp/calico.yaml\n        # This
        next line assures that in the event of multiple NICs in the node, the NIC
        able to reach 1.1.1.1 will be chosen\n        # This can optionally be changed
        to a regex or even an explicit NIC name\n        sed -i \"/autodetect\\\"/a\\
        \           - name: IP_AUTODETECTION_METHOD\\n              value: \\\"can-reach=1.1.1.1\\\"\"
        /tmp/calico.yaml\n        sed -i \"s#value: \\\"192.168.0.0/16\\\"#value:
        \\\"{{ .Param \"krib/cluster-pod-subnet\" }}\\\"#g\" /tmp/calico.yaml\n\n
        \       ##### Optionally replace default container images with ones from\n
        \       ##### Our trusted registry\n        {{ if .ParamExists \"krib/calico-container-image-cni\"
        }}\n        # Replace default container image with the one specified in \"krib/calico-container-image-cni\"\n
        \       sed -i 's|calico/cni|{{ .Param \"krib/calico-container-image-cni\"
        }}|' /tmp/calico.yaml\n        {{ end -}}\n        {{ if .ParamExists \"krib/calico-container-image-kube-controllers\"
        }}\n        # Replace default container image with the one specified in \"krib/calico-container-image-kube-controllers\"\n
        \       sed -i 's|calico/kube-controllers|{{ .Param \"krib/calico-container-image-kube-controllers\"
        }}|' /tmp/calico.yaml\n        {{ end -}} \n        {{ if .ParamExists \"krib/calico-container-image-node\"
        }}\n        # Replace default container image with the one specified in \"krib/calico-container-image-node\"\n
        \       sed -i 's|calico/node|{{ .Param \"krib/calico-container-image-node\"
        }}|' /tmp/calico.yaml\n        {{ end -}}  \n        {{ if .ParamExists \"krib/calico-container-image-pod2daemon-flexvol\"
        }}\n        # Replace default container image with the one specified in \"krib/calico-container-pod2daemon-flexvol\"\n
        \       sed -i 's|calico/pod2daemon-flexvol|{{ .Param \"krib/calico-container-image-pod2daemon-flexvol\"
        }}|' /tmp/calico.yaml\n        {{ end -}}         \n        ##### End optional
        image adjustments               \n\n        kubectl apply -f /tmp/calico.yaml\n
        \       mkdir -p /tmp/cleanup\n        mv /tmp/calico.yaml /tmp/cleanup/\n\n
        \       ;;\n      flannel)\n        echo \"Starting flannel networking...\"\n
        \       curl -gfsSL {{ .Param \"provider/flannel-config\" }} | kubectl apply
        -f -\n        ;;\n      weave)\n        echo \"Starting weave networking...\"\n
        \       kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl
        version | base64 | tr -d '\\n')\"\n        ;;\n    esac\n\n  else\n    echo
        \"waiting for $KRIB_MASTER_CERTS_PARAM\"\n    wait_for_variable $KRIB_MASTER_CERTS_PARAM\n\n
        \   cd /\n    get_param $KRIB_MASTER_CERTS_PARAM | jq -r . | base64 -d - |
        tar -zxvf -\n    cd -\n\n    kubeadm init --config=/tmp/kubeadm.cfg --ignore-preflight-errors
        {{ .Param \"krib/ignore-preflight-errors\" }} 2>&1 > /tmp/kubeadm.init.log\n\n
        \ fi\n\n  MAX_WAIT_SECONDS=300\n  INCREMENT_SECONDS=10\n  N=0\n  READY=false\n\n
        \ while [ \"$READY\" != \"true\" ]; do\n    READY=true\n\n    if [ $N -gt
        ${MAX_WAIT_SECONDS} ]; then\n      echo \"All masters did not come up!\"\n
        \     echo \"DEBUG: review /tmp/kubeadm.init.log:\"\n      xiterr 1 \"$(cat
        /tmp/kubeadm.init.log)\"\n    else\n      N=$(expr $N + ${INCREMENT_SECONDS})\n
        \   fi\n\n    OLD_IFS=$IFS\n    IFS=\" \" ; while read name ; do\n    # desensitize
        get nodes to case, if hostname has mix or upper case\n    if kubectl get nodes
        | tr '[:upper:]' '[:lower:]' | egrep -qi \"^$name[ \\t]*ready\"; then\n      echo
        \"Node $name is ready.\"\n      HOSTNAME=$name\n    else\n      echo \"Node
        $name is not yet ready.\"\n      echo \"$(kubectl get nodes)\"\n      READY=false\n
        \   fi\n  done <<< $(get_param $KRIB_MASTERS_PARAM | jq -r '.[].Name' )\n
        \   IFS=$OLD_IFS\n\n    if [ \"$READY\" != \"true\" ]; then\n      sleep ${INCREMENT_SECONDS}s\n
        \   fi\n  done\n\n  if [[ $MASTER_INDEX == 0 ]] ; then\n    set_taint\n\n
        \   echo \"Recording 'kubeadm' bootstrap config ...\"\n    drpcli -T $PROFILE_TOKEN
        profiles add $CLUSTER_PROFILE param $KRIB_CLUSTER_KUBEADM_CFG_PARAM to - <
        /tmp/kubeadm.cfg\n\n    echo \"Recording cluster admin config ...\"\n    drpcli
        -T $PROFILE_TOKEN profiles add $CLUSTER_PROFILE param $KRIB_ADMIN_CONF_PARAM
        to - < /etc/kubernetes/admin.conf\n\n    echo \"Recording cluster join script
        ...\"\n    drpcli -T $PROFILE_TOKEN profiles add $CLUSTER_PROFILE param $KRIB_JOIN_PARAM
        to \"$JOIN_CMD\"\n\n    drpcli machines tasks add {{ .Machine.UUID }} at 0
        krib-settings krib-dashboard | jq .Tasks\n\n    drpcli machines update $RS_UUID
        \"{\\\"Meta\\\":{\\\"color\\\":\\\"purple\\\", \\\"icon\\\": \\\"anchor\\\"
        }}\" | jq .Meta\n  else\n    {{ if .ParamExists \"krib/cluster-bootstrap-token\"
        -}}{{ if ne (.Param \"krib/cluster-bootstrap-token\") \"fedcba.fedcba9876543210\"
        -}}\n    echo \"remove taint during master recovery\"\n    set_taint\n    {{
        end -}}{{ end -}}\n    drpcli machines update $RS_UUID \"{\\\"Meta\\\":{\\\"color\\\":\\\"green\\\",
        \\\"icon\\\": \\\"anchor\\\" }}\" | jq .Meta\n  fi\n\nelse\n\n  echo \"I am
        a worker - run kubeadm join when defined\"\n\n  JC=$(wait_for_variable $KRIB_JOIN_PARAM)\n\n
        \ echo \"Running: $JC\"\n  $JC\n\n  # Set machine icon and color for KRIB
        cluster building\n  drpcli machines update $RS_UUID \"{\\\"Meta\\\":{\\\"color\\\":\\\"green\\\",
        \\\"icon\\\": \\\"ship\\\" }}\" | jq .Meta\n\nfi\n\nwhile [ ! -f /etc/kubernetes/kubelet.conf
        ] ;\ndo\n  sleep 2\ndone\n\n# Wait for KRIB config to be updated from bootstrap
        node\necho \"waiting for $KRIB_ADMIN_CONF_PARAM\"\nwait_for_variable $KRIB_ADMIN_CONF_PARAM
        > label.conf\n\n# Set labels for nodes / runs on all nodes\ndrpcli -T $PROFILE_TOKEN
        profiles get $CLUSTER_PROFILE param $KRIB_ADMIN_CONF_PARAM > label.conf\n#
        Re-set HOSTNAME to kubernetes node name\nHOSTNAME=$(kubectl --kubeconfig=label.conf
        get nodes | cut -d\" \" -f1 | grep -i $(hostname -s))\necho \"Adding env={{.Param
        \"krib/label-env\"}} label to machine $HOSTNAME\"\nkubectl --kubeconfig=label.conf
        label nodes $HOSTNAME env={{.Param \"krib/label-env\"}} --overwrite=true\n#
        using adhoc labels\n{{if .ParamExists \"krib/labels\" -}}\n  {{range $key,
        $value := .Param \"krib/labels\" -}}\n    echo \"Adding {{$key}}=\\\"{{$value
        | toString | replace \" \" \"_\" -}}\\\" label to machine $HOSTNAME from krib/labels\"\n
        \   kubectl --kubeconfig=label.conf label nodes $HOSTNAME {{$key}}=\"{{$value
        | toString | replace \" \" \"_\" -}}\" --overwrite=true || true\n  {{end -}}\n{{else
        -}}\n  echo \"use of krib/labels to create adhoc labels\"\n{{end -}}\n# using
        inventory\n{{if .ParamExists \"inventory/data\" -}}\n  {{range $key, $value
        := .Param \"inventory/data\" -}}\n    echo \"Adding {{$key}}=\\\"{{$value
        | toString | replace \" \" \"_\" -}}\\\" label to machine $HOSTNAME from inventory/data\"\n
        \   kubectl --kubeconfig=label.conf label nodes $HOSTNAME {{$key}}=\"{{$value
        | toString | replace \" \" \"_\" -}}\" --overwrite=true || true\n  {{end }}\n{{else
        -}}\n  echo \"use of inventory/data to create machine specific labels\"\n{{end
        -}}\nkubectl --kubeconfig=label.conf get node $HOSTNAME --show-labels\nrm
        label.conf\n\n# Fix kubelet to use the vip/lb\n{{ if .ParamExists \"krib/cluster-master-vip\"
        -}}\nMASTER_VIP={{ .Param \"krib/cluster-master-vip\" }}\nMASTER_COUNT={{
        .Param \"krib/cluster-master-count\" }}\nAPI_PORT={{ .Param \"krib/cluster-api-port\"
        }}\nif [[ $MASTER_COUNT -gt 1 ]] ; then\n  API_PORT={{ .Param \"krib/cluster-api-vip-port\"
        }}\nfi\nsed -i \"s#server:.*#server: https://${MASTER_VIP}:${API_PORT}#g\"
        /etc/kubernetes/kubelet.conf\nsystemctl restart kubelet\n{{ else -}}\nxiterr
        1 \"Missing required krib/cluster-master-vip\"\n{{ end -}}\n\n# Clean up\nrm
        -f /tmp/kubeadm.cfg\n\necho \"Finished successfully\"\nexit 0\n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-config.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-contrail.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |+
        #!/usr/bin/env bash
        # Kubernetes Rebar Integrated Boot (KRIB) Kubeadm Installer
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        echo "Run Contrail install on the master (skip for minions)..."

        {{if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing krib/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        echo "test pull contrail-nodemgr..."
        sudo docker pull docker.io/opencontrailnightly/contrail-nodemgr:latest
        # TODO: ENSURE CONTRAIL REGISTERY IS AVAILABLE

        MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
        echo "My Master index is $MASTER_INDEX"
        if [[ $MASTER_INDEX != notme ]] ; then

          if [[ $MASTER_INDEX == 0 ]] ; then

            echo "I am the elected leader - I can run Contrail install for the cluster"

            # help requires the admin config
            export KUBECONFIG="/etc/kubernetes/admin.conf"

            # TODO check if exists first
            mkdir -pm 777 /var/lib/contrail/kafka-logs
            kubectl apply --filename "./contrail-single-step-cni-install-centos.yaml"

            # WAIT FOR A LITTLE
            sleep 10

            # CHECK STATUS - really should loop and check
            # TODO download contrail-status CLI
            # contrail-status

          else

            echo "I was not the leader, skipping contrail install"

          fi

        else

          echo "I am a worker - no contrail actions"

        fi

        echo "Finished successfully"
        exit 0

      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-contrail.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-dashboard.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # Kubernetes Rebar Immutable Boot (KRIB) Kubeadm Installer
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        {{if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing krib/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}
        export RS_UUID="{{.Machine.UUID}}"

        {{ if eq (.Param "krib/dashboard-enabled") true -}}
        echo "Start Dashboard"

        MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
        MASTER_COUNT={{.Param "krib/cluster-master-count"}}

        if [[ $MASTER_INDEX == 0 ]] ; then
          export KUBECONFIG=/etc/kubernetes/admin.conf

          if ! which rsync ; then
            install rsync
          fi
          if ! which git ; then
            install git
          fi
          if ! which wget ; then
            install wget
          fi

          wget -O /tmp/kubernetes-dashboard.yaml {{ .Param "krib/dashboard-config" }}

          OPWD=$(pwd)
          cd /tmp
          cd $OPWD

          mkdir -p /root/setup/dashboard
          cp /tmp/kubernetes-dashboard.yaml /root/setup/dashboard/kubernetes-dashboard.yaml

          cat > /root/setup/dashboard/dashboard-serviceaccount.yaml << EOFSA
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          labels:
            k8s-app: kubernetes-dashboard
          name: kubernetes-dashboard
          namespace: kubernetes-dashboard
        EOFSA

          cat >/root/setup/dashboard/dashboard-admin.yaml <<EOFDA
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRoleBinding
        metadata:
          name: kubernetes-dashboard
          labels:
            k8s-app: kubernetes-dashboard
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: cluster-admin
        subjects:
        - kind: ServiceAccount
          name: kubernetes-dashboard
          namespace: kubernetes-dashboard
        EOFDA

          kubectl apply -f /root/setup/dashboard/kubernetes-dashboard.yaml
          kubectl delete -f /root/setup/dashboard/dashboard-serviceaccount.yaml
          kubectl delete -f /root/setup/dashboard/dashboard-admin.yaml
          kubectl apply -f /root/setup/dashboard/dashboard-serviceaccount.yaml
          kubectl apply -f /root/setup/dashboard/dashboard-admin.yaml
          kubectl scale --replicas=$MASTER_COUNT -n kubernetes-dashboard deployment/kubernetes-dashboard

          rm -rf /root/setup
        fi
        echo "Finished successfully"
        {{end -}}
        drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"green\", \"icon\": \"anchor\"}}" | jq .Meta
        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-dashboard.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-dev-reset.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # Kubernetes Rebar Integrated Boot (KRIB) Kubeadm Installer
        # this snippet is called by the krib-dev-reset task

        {{if .Param "krib/cluster-is-production" -}}
          xiterr 1 "CANNOT USE RESET WHEN CLUSTER IS PRODUCTION"
        {{else -}}

          for p in "${WIPE_PARAMS[@]}"; do

            JC=$(drpcli -T "$PROFILE_TOKEN" profiles get "$CLUSTER_PROFILE" param "$p" | jq -r .)
            echo "=== clearing $p : value was ==========================="
            if [[ $JC != null ]] ; then
                drpcli -T "$PROFILE_TOKEN" profiles remove "$CLUSTER_PROFILE" param "$p" | jq -r '.'
            else
              echo "  skipping $p, was not set"
            fi

          done
          echo "==================================================="

        {{end -}}
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-dev-reset.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-encryption-config.yaml.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        kind: EncryptionConfig
        apiVersion: v1
        resources:
         - resources:
           - secrets
           providers:
           - kms:
               name: vault
               endpoint: unix:///etc/kubernetes/pki/vault-kms-plugin/vault-kms-plugin.sock
               cachesize: 100
           - identity: {}
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-encryption-config.yaml.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-external-dns.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: "#!/usr/bin/env bash\n# Kubernetes Rebar Integrated Boot (KRIB) nginx-ingress
        Installer\nset -e\n\n# Get access and who we are.\n{{template \"setup.tmpl\"
        .}}\n\n{{if .ParamExists \"krib/cluster-profile\" -}}\nCLUSTER_PROFILE={{.Param
        \"krib/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken (.Param
        \"krib/cluster-profile\") 7200}}\n{{else -}}\nxiterr 1 \"Missing krib/cluster-profile
        on the machine!\"\n{{end -}}\n\n{{template \"krib-lib.sh.tmpl\" .}}\n\nMASTER_INDEX=$(find_me
        $KRIB_MASTERS_PARAM \"Uuid\" $RS_UUID)\n\nif [[ $MASTER_INDEX == 0 ]] ; then\n
        \ export KUBECONFIG=/etc/kubernetes/admin.conf\n\n  {{ if .ParamExists \"krib/externaldns-container-image\"
        }}\n  # Replace default container image with the one specified in \"externaldns/container-image\"
        \n  sed -i 's|registry.opensource.zalan.do/teapot/external-dns|{{ .Param \"krib/externaldns-container-image\"
        }}|' /tmp/krib-external-dns.yaml\n  {{ end -}}\n\n  # Deploy\n  kubectl apply
        -f /tmp/krib-external-dns.yaml\n\n  # Clean up\n  mkdir -p /tmp/cleanup\n
        \ mv /tmp/*.yaml /tmp/cleanup\n\nelse\n  echo \"I was not the leader, skipping
        external-dns install\"\nfi\n\n\n\necho \"Finished external-dns deployment
        successfully\"\n\necho \"Finished successfully\"\nexit 0\n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-external-dns.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-external-dns.yaml.tmpl:
      Available: false
      Bundle: ""
      Contents: "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: external-dns\n---\napiVersion:
        v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n  namespace: external-dns
        \ \n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n
        \ name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n
        \ verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n
        \ verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\"] \n  resources:
        [\"ingresses\"] \n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n
        \ resources: [\"nodes\"]\n  verbs: [\"list\",\"watch\"]\n---\napiVersion:
        rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name:
        external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind:
        ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name:
        external-dns\n  namespace: external-dns\n---\napiVersion: v1\nkind: Secret\nmetadata:\n
        \ name: external-dns\n  namespace: external-dns\ndata:\n{{- if .ParamExists
        \"certmanager/route53-secret-access-key\"}}\n   aws-secret-access-key: \"{{.Param
        \"certmanager/route53-secret-access-key\"}}\"\n{{- end}}\n---\napiVersion:
        extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace:
        external-dns\nspec:\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n
        \     labels:\n        app: external-dns\n    spec:\n      serviceAccountName:
        external-dns\n      containers:\n      - name: external-dns\n        image:
        registry.opensource.zalan.do/teapot/external-dns:latest\n        args:\n        -
        --source=service\n        - --source=ingress\n        - --policy=upsert-only
        # would prevent ExternalDNS from deleting any records, omit to enable full
        synchronization\n        - --registry=txt\n        - --txt-owner-id={{ .Param
        \"krib/cluster-profile\" }}\n        - --domain-filter={{ .Param \"certmanager/dns-domain\"
        }}\n        env:\n{{if eq (.Param \"krib/external-dns-provider\") \"aws\"
        }}        \n          - name: EXTERNAL_DNS_PROVIDER\n            value: \"aws\"\n
        \         - name: AWS_ACCESS_KEY_ID\n            value: \"{{ .Param \"certmanager/route53-access-key-id\"
        }}\"\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              secretKeyRef:\n
        \               name: external-dns\n                key: aws-secret-access-key\n{{
        end -}}\n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-external-dns.yaml.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-get-masters.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |+
        #!/usr/bin/env bash
        # Build Select K8S masters
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        # Skip the remainder of this template if this host is not a master in a selective-master deployment
        {{template "krib-skip-if-not-master.tmpl" .}}

        export RS_UUID="{{.Machine.UUID}}"
        export RS_IP="{{.Machine.Address}}"

        # Skip the remainder of this template if this host is not a master in a selective-master deployment
        {{template "krib-skip-if-not-master.tmpl" .}}

        echo "Elect and wait for the masters"

        CLUSTER_NAME={{.Param "krib/cluster-name"}}
        {{if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing krib/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        MASTER_ON_ETCDS={{.Param "krib/cluster-masters-on-etcds"}}
        ETCD_COUNT={{.Param "etcd/server-count"}}

        {{ if .ParamExists "krib/ip" -}}
        KRIB_IP={{ .Param "krib/ip" }}
        {{ else -}}
        KRIB_IP={{ .Machine.Address }}
        {{ end -}}

        # Get the number of servers to create
        KRIB_MASTER_COUNT={{.Param "krib/cluster-master-count"}}
        echo "Creating $KRIB_MASTER_COUNT k8s masters"

        if [[ {{ .Param "krib/k3s" }} == true ]] ; then
          echo "K3s, add $KRIB_MASTERS_PARAM to $CLUSTER_PROFILE"
          drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "$KRIB_MASTERS_PARAM" to "[]" || true
          echo "K3s, skipping etcd config"
        elif [[ $MASTER_ON_ETCDS == true ]] ; then
          if (( $KRIB_MASTER_COUNT >= $ETCD_COUNT )) ; then
            # If we have the same or more masters than etcd, start with the etcd list.
            ETCD_MASTERS=$(get_param $ETCD_SERVERS_PARAM)
            drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "$KRIB_MASTERS_PARAM" to "$ETCD_MASTERS" || true
          else
            drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "$KRIB_MASTERS_PARAM" to "[]" || true
          fi

          # Need a subset of the ETCD members - let them through one at a time
          ETCD_INDEX=$(find_me $ETCD_SERVERS_PARAM "Uuid" $RS_UUID)
          if [[ $ETCD_INDEX == notme ]] ; then
            echo "I am not a master server.  Move on."

            # Set machine icon and color for KRIB cluster building
            drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"yellow\", \"icon\": \"ship\"} }" | jq .Meta

            exit 0
          fi
        else
          drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "$KRIB_MASTERS_PARAM" to "[]" || true

          ETCD_INDEX=$(find_me $ETCD_SERVERS_PARAM "Uuid" $RS_UUID)
          if [[ $ETCD_INDEX != notme ]] ; then
            echo "I am not a master server.  Move on."

            # Set machine icon and color for KRIB cluster building
            drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"yellow\", \"icon\": \"ship\"} }" | jq .Meta

            exit 0
          fi
        fi

        echo "Electing Members to $CLUSTER_PROFILE"
        MASTER_INDEX=$(add_me_if_not_count "$KRIB_MASTERS_PARAM" $KRIB_MASTER_COUNT $KRIB_IP)
        if [[ $MASTER_INDEX == notme ]] ; then
          echo "I am not a master server.  Move on."

          # Set machine icon and color for KRIB cluster building
          drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"yellow\", \"icon\": \"ship\"} }" | jq .Meta

          exit 0
        fi

        if (( $KRIB_MASTER_COUNT > 1 )) ; then
          {{if .ParamExists "krib/cluster-master-vip" -}}
            MASTER_VIP={{.Param "krib/cluster-master-vip"}}
          {{else -}}
            xiterr 1 "Missing krib/cluster-master-vip on the machine!"
          {{end -}}

          drpcli machines tasks add {{.Machine.UUID}} at 0 krib-keepalived krib-haproxy
        else
          # If we are single machine, don't setup VIP.
          {{if .ParamExists "krib/cluster-master-vip" -}}
            drpcli -T "$PROFILE_TOKEN" profiles set "$CLUSTER_PROFILE" param "$KRIB_MASTER_VIP_PARAM" to {{.Machine.Address}} || true
          {{else -}}
            drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "$KRIB_MASTER_VIP_PARAM" to {{.Machine.Address}} || true
          {{end -}}
        fi

        echo "Wait for all the masters to show up"
        wait_for_count "$KRIB_MASTERS_PARAM" $KRIB_MASTER_COUNT

        echo "Masters are here.  Let's go!"

        # Set machine icon and color for KRIB cluster building
        drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"yellow\", \"icon\": \"anchor\"} }" | jq .Meta

        exit 0

      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-get-masters.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-haproxy.cfg.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        global
          log /dev/log  local0
          log /dev/log  local1 notice
          chroot /var/lib/haproxy
          stats timeout 30s
          user haproxy
          group haproxy
          daemon

          # Default SSL material locations
          ca-base /etc/ssl/certs
          crt-base /etc/ssl/private

          # Default ciphers to use on SSL-enabled listening sockets.
          # For more information, see ciphers(1SSL). This list is from:
          #  https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/
          # An alternative list with additional directives can be obtained from
          #  https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy
          ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS
          ssl-default-bind-options no-sslv3

        defaults
          log global
          mode  http
          option  httplog
          option  dontlognull
          timeout connect 5000
          timeout client  50000
          timeout server  50000
          timeout http-request 15s
          timeout http-keep-alive 15s

        frontend monitor-in
          bind *:33305
          mode http
          option httplog
          monitor-uri /monitor

        listen stats
          bind    *:9000
          mode    http
          stats   enable
          stats   hide-version
          stats   uri       /stats
          stats   refresh   30s
          stats   realm     Haproxy\ Statistics
          stats   auth      Admin:Password

        frontend k8s-api
          bind *:{{ .Param "krib/cluster-api-vip-port" }}
          mode tcp
          option tcplog
          tcp-request inspect-delay 5s
          #tcp-request content reject if !HTTP
          tcp-request content accept if { req.ssl_hello_type 1 }
          default_backend k8s-api

        frontend etcd-client
          bind *:{{ .Param "etcd/cluster-client-vip-port" }}
          mode tcp
          option tcplog
          tcp-request inspect-delay 5s
          #tcp-request content reject if !HTTP
          tcp-request content accept if { req.ssl_hello_type 1 }
          default_backend etcd-client

        backend k8s-api
          mode tcp
          option tcplog
          option tcp-check
          balance roundrobin
          default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
        {{ $port := .Param "krib/cluster-api-port" -}}
        {{ range $index, $elem := .Param "krib/cluster-masters" }}
          server k8s-api-{{ $index }} {{ $elem.Address }}:{{ $port }} check
        {{ end -}}

        backend etcd-client
          mode tcp
          option tcplog
          option tcp-check
          balance roundrobin
          default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
        {{ $port := .Param "etcd/client-port" -}}
        {{ range $index, $elem := .Param "krib/cluster-masters" }}
          server etcd-client-{{ $index }} {{ $elem.Address }}:{{ $port }} check
        {{ end -}}
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-haproxy.cfg.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-haproxy.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # Kubernetes Rebar Immutable Boot (KRIB) Kubeadm Installer
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        # these need to be before krib-lib template
        {{if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing etcd/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        install haproxy

        systemctl enable haproxy
        systemctl start haproxy

        echo "Finished successfully"
        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-haproxy.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-helm-init.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: "#!/usr/bin/env bash\n# Kubernetes Rebar Integrated Boot (KRIB) Kubeadm
        Installer\nset -e\n\n# Get access and who we are.\n{{template \"setup.tmpl\"
        .}}\n\necho \"Configure helm on the master (skip for minions)...\"\n\n{{if
        .ParamExists \"krib/cluster-profile\" -}}\nCLUSTER_PROFILE={{.Param \"krib/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken
        (.Param \"krib/cluster-profile\") 7200}}\n{{else -}}\nxiterr 1 \"Missing krib/cluster-profile
        on the machine!\"\n{{end -}}\n\n{{template \"krib-lib.sh.tmpl\" .}}\n\nMASTER_INDEX=$(find_me
        $KRIB_MASTERS_PARAM \"Uuid\" $RS_UUID)\necho \"My Master index is $MASTER_INDEX\"\nif
        [[ $MASTER_INDEX != notme ]] ; then\n\n  if [[ $MASTER_INDEX == 0 ]] ; then\n\n
        \   echo \"I am the elected leader - install helm and helm init\"\n\n    #
        help requires the admin config\n    export KUBECONFIG=\"/etc/kubernetes/admin.conf\"\n\n
        \   INSTALL_DIR=/usr/local/bin\n    if [[ $OS_FAMILY == coreos ]] ; then\n
        \     INSTALL_DIR=/opt/bin\n    fi\n\n    if [[ ! -f \"${INSTALL_DIR}/helm\"
        ]]; then\n      # todo: provide more options for installing helm\n      {{
        if eq (.Param \"helm/version\") \"latest\" -}}\n      # curl has exit code
        23... meh\n      set +e\n      HELM_VERSION=$(curl -skX GET \"https://api.github.com/repos/helm/helm/releases/latest\"
        | awk '/tag_name/{print $4;exit}' FS='[\"\"]')\n      set -e\n      {{ else
        -}}\n      HELM_VERSION={{.Param \"helm/version\"}}\n      {{ end -}}\n      curl
        https://raw.githubusercontent.com/helm/helm/master/scripts/get > get_helm.sh\n
        \     chmod 700 get_helm.sh\n\n      HELM_INSTALL_DIR=$INSTALL_DIR ./get_helm.sh
        -v ${HELM_VERSION}\n      echo \"When helm fail to get the latest version,
        please add param \"helm/version\" and re-run again\"\n      echo \"You can
        find the version release or tag on: https://github.com/helm/helm/releases/\"\n
        \     \n    fi\n\n    if [[ ! -z $(kubectl -n kube-system rollout status deploy/tiller-deploy
        -w=0 | grep success) ]] ; then\n      echo \"Tiller already installed - exiting\"\n
        \     helm version\n      exit 0\n    fi\n\n    echo \"Create tiller service
        account\"\n    kubectl apply -f tiller-rbac.yaml\n\n    echo \"Helm initialize\"\n
        \   helm init --service-account tiller\n\n    set +e\n    echo \"Wait until
        Tiller is running\"\n    CNT=$(kubectl -n kube-system rollout status deploy/tiller-deploy
        -w=0 | grep success)\n    ESCAPE=0\n    while [[ -z $CNT && $ESCAPE -lt 20
        ]] ; do\n      echo $(kubectl -n kube-system rollout status deploy/tiller-deploy
        -w=0)\n      sleep 5\n      CNT=$(kubectl -n kube-system rollout status deploy/tiller-deploy
        -w=0 | grep success)\n      ((ESCAPE=ESCAPE+1))\n    done\n    set -e\n\n
        \   echo \"Check versions....(includes 10 second wait for Tiller initialize)\"\n
        \   # we need some extra time for Tiller to actually initialize\n    sleep
        10\n    helm version\n    rm ./get_helm.sh\n\n  else\n\n    echo \"I was not
        the leader, skipping helm init\"\n\n  fi\n\nelse\n\n  echo \"I am a worker
        - no helm actions\"\n\nfi\n\necho \"Finished successfully\"\nexit 0\n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-helm-init.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-helm.cfg.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: tiller
          namespace: kube-system
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRoleBinding
        metadata:
          name: tiller
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: cluster-admin
        subjects:
          - kind: ServiceAccount
            name: tiller
            namespace: kube-system
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-helm.cfg.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-helm.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        #!/usr/bin/env bash
        # Kubernetes Rebar Integrated Boot (KRIB) Kubeadm Installer
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        {{ if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{ else -}}
        xiterr 1 "Missing krib/cluster-profile on the machine!"
        {{ end -}}

        {{template "krib-lib.sh.tmpl" .}}

        CFG=$(mktemp /tmp/.XXXXXXXXXXX.conf)
        chmod 600 $CFG
        {{ if .ParamExists "krib/cluster-admin-conf" -}}
        cat > $CFG << 'EOFCFG'
        {{.ParamAsJSON "krib/cluster-admin-conf"}}
        EOFCFG
        {{ else -}}
        drpcli machines update $RS_UUID '{"Meta":{"color":"red", "icon": "exclamation triangle"}}'
        xiterr 1 "Required 'krib/cluster-admin-conf' Param not set - is this a KRIB cluster ???"
        {{ end -}}

        [[ ! -s "$CFG" ]] && xiterr 1 "generated admin config file has zero size"
        export KUBECONFIG=$CFG

        MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
        echo "My Master index is $MASTER_INDEX"

        {{$render := .}}

        {{- range $index, $chart := .Param "helm/charts" -}}

        # check if it's installed without using helm
        INSTALLED=false
        if [[ ! -z $(kubectl --all-namespaces=true -n heritage=Tiller -o name get all | grep {{$chart.name}}) ]] ; then
          INSTALLED=true
        fi

        if [[ $MASTER_INDEX != notme ]] ; then
          if [[ $MASTER_INDEX == 0 ]] ; then
            if [[ "$INSTALLED" == false ]] ; then
              echo "I am the elected leader - I can run helm for the cluster"

              echo "Making sure Tiller is running"
              if [[ -z $(kubectl -n kube-system rollout status deploy/tiller-deploy -w=0 | grep success) ]] ; then
                xiterr 1 "Tiller NOT running - something went wrong with helm init"
              fi

              mkdir -p .helm
              export HELM_HOME=$(cd .helm && pwd)

              echo "Helm initialize"
              helm init -c --service-account tiller
              helm repo update

              echo "Installing {{$chart.name}} (from {{$chart.chart}})..."

              {{ range $name, $template := $chart.templates -}}

              echo "expanding template {{$template}} as {{$name}}.yaml"
              cat > {{$name}}.yaml << EOF
        {{ $render.CallTemplate $template $render }}
        EOF
            {{ end -}}

            fi
            {{ range $index, $tmpl := $chart.templatesbefore -}}{{ if eq ($tmpl.nodes) "leader" -}}

            if [[ "$INSTALLED" == false ]] || [[ "$INSTALLED" == {{if not $tmpl.runIfInstalled}}false{{else}}{{$tmpl.runIfInstalled}}{{end}} ]] ; then
              echo "expanding {{$chart.name}} templatebefore {{$tmpl.name}}"
        {{ $render.CallTemplate $tmpl.name $render }}
              echo "========= done {{$chart.name}} templatebefore {{$tmpl}} ==========="
            fi
            {{ end }}{{ end -}}
          fi
          {{ range $index, $tmpl := $chart.templatesbefore -}}{{ if eq ($tmpl.nodes) "masters" -}}

          if [[ "$INSTALLED" == false ]] || [[ "$INSTALLED" == {{if not $tmpl.runIfInstalled}}false{{else}}{{$tmpl.runIfInstalled}}{{end}} ]] ; then
            echo "expanding {{$chart.name}} templatebefore {{$tmpl.name}}"
        {{ $render.CallTemplate $tmpl.name $render }}
            echo "========= done {{$chart.name}} templatebefore {{$tmpl}} ==========="
          fi
          {{ end }}{{ end -}}
        fi
        {{ range $index, $tmpl := $chart.templatesbefore -}}{{ if eq ($tmpl.nodes) "all" -}}

        if [[ "$INSTALLED" == false ]] || [[ "$INSTALLED" == {{if not $tmpl.runIfInstalled}}false{{else}}{{$tmpl.runIfInstalled}}{{end}} ]] ; then
          echo "expanding {{$chart.name}} templatebefore {{$tmpl.name}}"
        {{ $render.CallTemplate $tmpl.name $render }}
          echo "========= done {{$chart.name}} templatebefore {{$tmpl}} ==========="
        fi
        {{ end }}{{ end -}}

        if [[ $MASTER_INDEX != notme ]] ; then
          if [[ $MASTER_INDEX == 0 ]] ; then
            if [[ "$INSTALLED" == false ]] ; then
              {{ range $index, $before := $chart.kubectlbefore -}}

              echo "running kubectl {{$before}} ({{$index}})"
              kubectl {{$before}}
              {{ end -}}

              {{ if $chart.targz }}
              echo "retrieving chart from {{$chart.targz}}"
              curl -gL "{{$chart.targz}}" | tar xz
              {{ end -}}

              {{ if $chart.repos }}
              {{ range $name, $url := $chart.repos -}}

              echo "adding chart repo {{$name}}"
              helm repo add {{$name}} {{$url}}
              {{ end -}}
              helm repo update
              {{ end -}}

              helm install {{$chart.chart}} --name {{$chart.name}}{{ if $chart.namespace }} --namespace {{$chart.namespace}}{{ end }} \
                {{ range $param, $value := $chart.params }} --{{$param}} {{$value}}{{ end }}

              {{ if $chart.sleep }}
              echo "sleep {{$chart.sleep}}"
              sleep {{$chart.sleep}}
              {{ else -}}
              sleep 10s
              {{ end -}}

              {{ if $chart.wait }}
              set +e
              ESCAPE=0
              while [[ $ESCAPE -lt 30 && -z $(helm list {{$chart.name}} | grep DEPLOYED) ]] ; do
                echo "helm list {{$chart.name}} is not deployed. $ESCAPE"
                {{ if $chart.sleep -}}
                sleep {{$chart.sleep}}
                {{ else -}}
                sleep 10
                {{ end -}}
                ((ESCAPE=ESCAPE+1))
              done
              set -e
              {{ else -}}
              echo "let's go! I'm not waiting for chart to be ready"
              {{ end -}}

              {{ if and $chart.wait $chart.namespace }}
              set +e
              ESCAPE=0
              while [[ $ESCAPE -lt 30 && ! -z $(kubectl get pods --namespace={{$chart.namespace}} | grep ContainerCreating) ]] ; do
                echo "kubectl get pods --namespace={{$chart.namespace}} is still creating containers. $ESCAPE"
                {{ if $chart.sleep -}}
                sleep {{$chart.sleep}}
                {{ else -}}
                sleep 10
                {{ end -}}
                ((ESCAPE=ESCAPE+1))
              done
              set -e
              {{ end -}}

              {{ range $index, $after := $chart.kubectlafter -}}

              echo "running kubectl {{$after}} ({{$index}})"
              kubectl {{$after}}
              {{ end -}}

            fi
            {{ range $index, $tmpl := $chart.templatesafter -}}{{ if eq ($tmpl.nodes) "leader" -}}

            if [[ "$INSTALLED" == false ]] || [[ "$INSTALLED" == {{if not $tmpl.runIfInstalled}}false{{else}}{{$tmpl.runIfInstalled}}{{end}} ]] ; then
              echo "expanding {{$chart.name}} templateafter {{$tmpl.name}}"
        {{ $render.CallTemplate $tmpl.name $render }}
              echo "========= done {{$chart.name}} templateafter {{$tmpl}} ==========="
            fi
            {{ end }}{{ end -}}
          else
            echo "I was not the leader, skipping helm install"
          fi
          {{ range $index, $tmpl := $chart.templatesafter -}}{{ if eq ($tmpl.nodes) "masters" -}}

          if [[ "$INSTALLED" == false ]] || [[ "$INSTALLED" == {{if not $tmpl.runIfInstalled}}false{{else}}{{$tmpl.runIfInstalled}}{{end}} ]] ; then
            echo "expanding {{$chart.name}} templateafter {{$tmpl.name}}"
        {{ $render.CallTemplate $tmpl.name $render }}
            echo "========= done {{$chart.name}} templateafter {{$tmpl}} ==========="
          fi
          {{ end }}{{ end -}}
        else
          echo "I am a worker - no helm actions"
        fi
        {{ range $index, $tmpl := $chart.templatesafter -}}{{ if eq ($tmpl.nodes) "all" -}}

        if [[ "$INSTALLED" == false ]] || [[ "$INSTALLED" == {{if not $tmpl.runIfInstalled}}false{{else}}{{$tmpl.runIfInstalled}}{{end}} ]] ; then
          echo "expanding {{$chart.name}} templateafter {{$tmpl.name}}"
        {{ $render.CallTemplate $tmpl.name $render }}
          echo "========= done {{$chart.name}} templateafter {{$tmpl}} ==========="
        fi
        {{ end }}{{ end -}}
        echo "========= done {{$chart.name}} ==========="
        {{ else -}}
        echo "No charts included in helm/charts to install"
        {{ end -}}
        echo "Finished successfully"
        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-helm.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-ingress-nginx-tillerless.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: "#!/usr/bin/env bash\n# Kubernetes Rebar Integrated Boot (KRIB) nginx-ingress
        Installer\nset -e\n\n# Get access and who we are.\n{{template \"setup.tmpl\"
        .}}\n\n{{if .ParamExists \"krib/cluster-profile\" -}}\nCLUSTER_PROFILE={{.Param
        \"krib/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken (.Param
        \"krib/cluster-profile\") 7200}}\n{{else -}}\nxiterr 1 \"Missing krib/cluster-profile
        on the machine!\"\n{{end -}}\n\n{{template \"krib-lib.sh.tmpl\" .}}\n\nMASTER_INDEX=$(find_me
        $KRIB_MASTERS_PARAM \"Uuid\" $RS_UUID)\n\nif [[ $MASTER_INDEX == 0 ]] ; then\n
        \ export KUBECONFIG=/etc/kubernetes/admin.conf\n  if ! which wget ; then\n
        \   install wget\n  fi\n\n  # Get manifests, and duplicate service manifest
        for optional additional external ingress\n  wget -O /tmp/nginx-ingress-mandatory.yaml
        {{ .Param \"krib/ingress-nginx-mandatory\" }}\n  wget -O /tmp/nginx-ingress-service.yaml
        {{ .Param \"krib/ingress-nginx-config\" }}\n  cp /tmp/nginx-ingress-service.yaml
        /tmp/nginx-ingress-external-service.yaml\n\n  # if [[ -n $(kubectl get namespaces
        | grep -w ingress-nginx) ]] ; then\n  #   echo \"Purging existing nginx-ingress
        install\"\n  #   kubectl delete -f /tmp/nginx-ingress-mandatory.yaml\n  #
        fi\n\n  {{ if .ParamExists \"krib/nginx-ingress-controller-container-image\"
        }}\n  # Replace default container image with the one specified in \"krib/nginx-ingress-controller-container-image\"\n
        \ sed -i 's|quay.io/kubernetes-ingress-controller/nginx-ingress-controller|{{
        .Param \"krib/nginx-ingress-controller-container-image\" }}|' /tmp/nginx-ingress-mandatory.yaml\n
        \ {{ end -}}\n\n  {{ if .ParamExists \"krib/ingress-nginx-loadbalancer-ip\"
        }}\n  # Add explicit loadbalancer IP to service\n  sed -i \"/type: LoadBalancer/
        a \\  loadBalancerIP: {{ .Param \"krib/ingress-nginx-loadbalancer-ip\" }}
        \\\\\" /tmp/nginx-ingress-service.yaml\n  {{ end -}}\n\n  # If we're only
        doing one ingress, allow the possibilty to override the publish ip\n  {{ if
        .ParamExists \"krib/ingress-nginx-publish-ip\" }}\n  {{ if eq (.Param \"krib/ingress-external-enabled\")
        false -}}\n  sed -i \"s/publish-service=.*/publish-status-address={{ .Param
        \"krib/ingress-nginx-publish-ip\" }}/\" \\\n    /tmp/nginx-ingress-mandatory.yaml\n
        \ {{ end -}}\n  {{ end -}}\n\n  # Create the default \"internal\" nginx\n
        \ kubectl apply -f /tmp/nginx-ingress-mandatory.yaml\n  kubectl apply -f /tmp/nginx-ingress-service.yaml\n\n
        \ # Create exposed TCP/UDP services\n  kubectl apply -f /tmp/krib-nginx-tcp-services.yaml\n
        \ kubectl apply -f /tmp/krib-nginx-udp-services.yaml\n\n  {{ if eq (.Param
        \"krib/ingress-external-enabled\") true -}}\n  ##### Create create the \"external\"
        nginx (which is nginx in all but name)\n  # Rather than try to entangle the
        two controllers, just wholesale search/replace with sed\n  # and end up with
        each controller in its own namespace, with its own clusterrolebindings, etc.\n
        \ # This makes it easier to restrict access later, or to remove and redeploy
        one ingress independently\n  # of the other\n\n  # Add annotations per https://kubernetes.github.io/ingress-nginx/user-guide/multiple-ingress/\n
        \ sed \"/annotations-prefix=nginx.ingress/ a \\            - --ingress-class=nginx-external
        \\\\\" \\\n    /tmp/nginx-ingress-mandatory.yaml > /tmp/nginx-ingress-external-mandatory.yaml\n
        \ sed -i \"/annotations-prefix=nginx.ingress/ a \\            - --election-id=ingress-controller-leader-external
        \\\\\" \\\n    /tmp/nginx-ingress-external-mandatory.yaml\n\n\n  # If we're
        doing an _additional_ external ingress, then allow the possibility of overriding
        the external ingress publish ip using --publish-status-address\n  # This value
        is mutually exclusive from --publish-service\n  {{- if .ParamExists \"krib/ingress-nginx-publish-ip\"
        }}\n  sed -i \"s/publish-service=.*/publish-status-address={{ .Param \"krib/ingress-nginx-publish-ip\"
        }}/\" \\\n    /tmp/nginx-ingress-external-mandatory.yaml\n  {{ else }}\n    sed
        -i \"/annotations-prefix=nginx.ingress/ a \\            - --publish-service=ingress-nginx-external/ingress-nginx-external
        \\\\\" \\\n    /tmp/nginx-ingress-external-mandatory.yaml\n  {{ end -}}\n\n
        \ # Prepare the mandatory elements\n  sed -i \"s/name: nginx-ingress-controller/name:
        external-nginx-ingress-controller/\" /tmp/nginx-ingress-external-mandatory.yaml\n
        \ sed -i \"s/app.kubernetes.io\\/name: ingress-nginx/app.kubernetes.io\\/name:
        ingress-nginx-external/\" /tmp/nginx-ingress-external-mandatory.yaml\n  sed
        -i \"s/app.kubernetes.io\\/part-of: ingress-nginx/app.kubernetes.io\\/part-of:
        ingress-nginx-external/\" /tmp/nginx-ingress-external-mandatory.yaml\n  sed
        -i \"s/  name: ingress-nginx$/  name: ingress-nginx-external/\" /tmp/nginx-ingress-external-mandatory.yaml\n
        \ sed -i \"s/  name: nginx-ingress$/  name: external-nginx-ingress/\" /tmp/nginx-ingress-external-mandatory.yaml\n
        \ sed -i \"s/nginx-ingress-serviceaccount/external-nginx-ingress-serviceaccount/\"
        /tmp/nginx-ingress-external-mandatory.yaml\n  sed -i \"s/namespace: ingress-nginx$/namespace:
        ingress-nginx-external/\" /tmp/nginx-ingress-external-mandatory.yaml\n  sed
        -i \"s/nginx-ingress-clusterrole/external-nginx-ingress-clusterrole/\" /tmp/nginx-ingress-external-mandatory.yaml\n
        \ sed -i \"s/ingress-controller-leader-nginx/ingress-controller-leader-external-nginx-external/\"
        /tmp/nginx-ingress-external-mandatory.yaml\n\n  # Prepare the service\n  sed
        -i \"s/app.kubernetes.io\\/name: ingress-nginx/app.kubernetes.io\\/name: ingress-nginx-external/\"
        /tmp/nginx-ingress-external-service.yaml\n  sed -i \"s/app.kubernetes.io\\/part-of:
        ingress-nginx/app.kubernetes.io\\/part-of: ingress-nginx-external/\" /tmp/nginx-ingress-external-service.yaml\n
        \ sed -i \"s/  name: ingress-nginx$/  name: ingress-nginx-external/\" /tmp/nginx-ingress-external-service.yaml\n
        \ sed -i \"s/namespace: ingress-nginx$/namespace: ingress-nginx-external/\"
        /tmp/nginx-ingress-external-service.yaml\n\n  {{ if .ParamExists \"krib/ingress-nginx-external-loadbalancer-ip\"
        }}\n  # Add explicit loadbalancer IP to service\n  sed -i \"/type: LoadBalancer/
        a \\  loadBalancerIP: {{ .Param \"krib/ingress-nginx-external-loadbalancer-ip\"
        }} \\\\\" /tmp/nginx-ingress-external-service.yaml\n  {{ end -}}\n\n  # if
        [[ -n $(kubectl get namespaces | grep -w ingress-nginx-external) ]] ; then\n
        \ #   echo \"Purging existing nginx-ingress install\"\n  #   kubectl delete
        -f /tmp/nginx-ingress-external-mandatory.yaml\n  # fi\n\n  kubectl apply -f
        /tmp/nginx-ingress-external-mandatory.yaml\n  kubectl apply -f /tmp/nginx-ingress-external-service.yaml\n\n
        \ # Create exposed TCP/UDP services\n  kubectl apply -f /tmp/krib-nginx-external-tcp-services.yaml\n
        \ kubectl apply -f /tmp/krib-nginx-external-udp-services.yaml\n\n  ############
        End external ingress\n  {{ end }}\n\n  # now cert-manager\n{{if .ParamExists
        \"certmanager/email\"}}\n  echo \"Start cert-manager install\"\n  wget -O
        /tmp/certmanager-manifests.yaml {{ .Param \"certmanager/manifests\" }}\n\n
        \ echo \"Preparing namespace and disabling validation..\"\n  kubectl create
        namespace cert-manager || echo \"Failed to create namespace, it probably already
        exists. Proceeding...\"\n  kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=\"true\"
        || echo \"Failed to disable validation on namespace, it's probably already
        done. Proceeding...\"\n\n  ###### Perform optional substitution of the container
        image specified in the deployment YAML with\n  ###### an image from a local
        registry, for security\n  {{ if .ParamExists \"krib/cert-manager-container-image-controller\"
        }}\n  # Replace default container image with the one specified in \"krib/nginx-ingress-container-image-controller\"\n
        \ sed -i 's|quay.io/jetstack/cert-manager-controller|{{ .Param \"krib/cert-manager-container-image-controller\"
        }}|' \\\n    /tmp/certmanager-manifests.yaml\n  {{ end -}}\n\n  {{ if .ParamExists
        \"krib/cert-manager-container-image-cainjector\" }}\n  # Replace default container
        image with the one specified in \"krib/nginx-ingress-container-image-controller\"\n
        \ sed -i 's|quay.io/jetstack/cert-manager-cainjector|{{ .Param \"krib/cert-manager-container-image-cainjector\"
        }}|' \\\n    /tmp/certmanager-manifests.yaml\n  {{ end -}}\n\n  {{ if .ParamExists
        \"krib/cert-manager-container-image-webhook\" }}\n  # Replace default container
        image with the one specified in \"krib/nginx-ingress-container-image-controller\"\n
        \ sed -i 's|quay.io/jetstack/cert-manager-webhook|{{ .Param \"krib/cert-manager-container-image-webhook\"
        }}|' \\\n    /tmp/certmanager-manifests.yaml\n  {{ end -}}  \n  ######\n  ######
        End optional substitution\n\n\n  kubectl apply -f /tmp/certmanager-manifests.yaml\n\n{{if
        .ParamExists \"certmanager/acme-challenge-dns01-provider\"}}\n  if [[ -n $(kubectl
        -n kube-system get secrets | grep -w certmanager-provider) ]] ; then\n    echo
        \"Removing existing certmanager-provider Secret\"\n    kubectl delete -f /tmp/certmanager-provider-secret.yaml\n
        \ fi\n  echo \"Creating certmanager-provider Secret\"\n  kubectl apply -f
        /tmp/certmanager-provider-secret.yaml\n{{end}}\n\n  # It can take some time
        before this succeeds, because it requires un-tainted workers available to
        \n  # run workloads, and the download of container images for certmanager's
        validating webhook\n  # For this reason, we use with_backoff, and we retry
        every minute for 10 min\n  ATTEMPTS=10 TIMEOUT=10 with_backoff kubectl apply
        -f /tmp/certmanager-clusterissuer.yaml\n\n  # Clean up\n  mkdir -p /tmp/cleanup\n
        \ mv /tmp/*.yaml /tmp/cleanup\n  #rm -f /tmp/certmanager-provider-secret.yaml\n
        \ #rm -f /tmp/certmanager-clusterissuer.yaml\n  #rm -f /tmp/certmanager-crds.yaml\n
        \ #rm -f /tmp/k8s-db-ingress.yaml\n{{end}}\nelse\n  echo \"I was not the leader,
        skipping nginx ingress install\"\nfi\n\necho \"Finished nginx ingress deployment
        successfully\"\n\necho \"Finished successfully\"\nexit 0\n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-ingress-nginx-tillerless.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-ingress-nginx.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # Kubernetes Rebar Integrated Boot (KRIB) nginx-ingress Installer
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        echo "Run helm install on the master (skip for minions)..."

        {{if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing krib/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
        echo "My Master index is $MASTER_INDEX"

        if [[ $MASTER_INDEX != notme ]] ; then
          if [[ $MASTER_INDEX == 0 ]] ; then
            export KUBECONFIG=/etc/kubernetes/admin.conf

            echo "Making sure Tiller is running"
            if [[ -z $(kubectl -n kube-system rollout status deploy/tiller-deploy -w=0 | grep success) ]] ; then
              xiterr 1 "Tiller NOT running - something went wrong with helm init"
            fi

            echo "Helm initialize"
            helm init -c --service-account tiller

            echo "Start NGINX Ingress Install"

            if [[ -n $(helm list nginx-ingress) ]] ; then
              echo "Purging existing nginx-ingress helm install"
              helm del --purge nginx-ingress
            fi

            helm install \
             --name nginx-ingress \
             --namespace kube-system \
             --set service.externalTrafficPolicy="Local" \
             stable/nginx-ingress

        {{if .ParamExists "certmanager/email"}}
            echo "Start cert-manager install"
            wget -O /tmp/certmanager-crds.yaml {{ .Param "certmanager/crds" }}

            if [[ -n $(helm list cert-manager) ]] ; then
              echo "Purging existing cert-manager helm install"
              helm del --purge cert-manager
              kubectl delete -f /tmp/certmanager-crds.yaml
            fi

            echo "Adding  cert-manager crds"
            kubectl apply -f /tmp/certmanager-crds.yaml
            kubectl create namespace cert-manager
            kubectl label namespace cert-manager certmanager.k8s.io/disable-validation="true"

            # --set webhook.enabled=false currently required because of cert issue described here https://github.com/jetstack/cert-manager/issues/1302
            helm install \
             --name cert-manager \
             --namespace cert-manager \
             --set webhook.enabled=false \
             --set ingressShim.defaultIssuerName="letsencrypt-prod" \
             --set ingressShim.defaultIssuerKind="ClusterIssuer" \
        {{- if .ParamExists "certmanager/acme-challenge-dns01-provider"}}
             --set ingressShim.defaultACMEChallengeType="dns01" \
             --set ingressShim.defaultACMEDNS01ChallengeProvider="{{.Param "certmanager/acme-challenge-dns01-provider"}}" \
        {{- end}}
             stable/cert-manager

        {{if .ParamExists "certmanager/acme-challenge-dns01-provider"}}
            if [[ -n $(kubectl -n kube-system get secrets | grep -w certmanager-provider) ]] ; then
              echo "Removing existing certmanager-provider Secret"
              kubectl delete -f /tmp/certmanager-provider-secret.yaml
            fi
            echo "Creating certmanager-provider Secret"
            kubectl apply -f /tmp/certmanager-provider-secret.yaml

        {{end}}
            if [[ -n $(kubectl get crd -n app=cert-manager | grep -w clusterissuers.certmanager.k8s.io) ]] ; then
              if [[ $(kubectl get clusterissuers -o json | jq -r '.items | contains([{"metadata":{"name":"letsencrypt-prod"}}])') == "true" ]] ; then
                echo "Removing existing letsencrypt-prod ClusterIssuer"
                kubectl delete -f /tmp/certmanager-clusterissuer.yaml
              fi
              echo "Creating letsencrypt-prod ClusterIssuer"
              kubectl apply -f /tmp/certmanager-clusterissuer.yaml
            fi

        {{end}}
        {{ if eq (.Param "krib/dashboard-enabled") true -}}
            echo "Wait until nginx-ingress-controller gets a LoadBalancer IP"
            INGRESSIP=$(wait_for_ingress)

            echo "expanding inline template to /tmp/k8s-db-ingress.yaml"
            cat > /tmp/k8s-db-ingress.yaml << EOF
        apiVersion: extensions/v1beta1
        kind: Ingress
        metadata:
          name: k8s-db
          namespace: kube-system
          annotations:
            kubernetes.io/ingress.class: nginx
            nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
        {{- if .ParamExists "certmanager/email"}}{{- if .ParamExists "ingress/k8s-dashboard-hostname"}}
            kubernetes.io/tls-acme: "true"
        {{- end}}{{- end}}
        spec:
          rules:
        {{- if .ParamExists "ingress/k8s-dashboard-hostname"}}
          - host: "{{.Param "ingress/k8s-dashboard-hostname"}}"
        {{- else}}
          - host: "k8s-db.$INGRESSIP.xip.io"
        {{- end}}
            http:
              paths:
              - backend:
                  serviceName: kubernetes-dashboard
                  servicePort: 443
          tls:
          - hosts:
        {{- if .ParamExists "ingress/k8s-dashboard-hostname"}}
            - "{{.Param "ingress/k8s-dashboard-hostname"}}"
        {{- else}}
            - "k8s-db.$INGRESSIP.xip.io"
        {{- end}}
        {{- if .ParamExists "certmanager/email"}}{{- if .ParamExists "ingress/k8s-dashboard-hostname"}}
            secretName: k8s-db-tls
        {{- end}}{{- end}}
        EOF

            if [[ $(kubectl get -n kube-system ingress -o json | jq -r '.items | contains([{"metadata":{"name":"k8s-db"}}])') == "true" ]] ; then
              echo "Removing existing k8s-db Ingress"
              kubectl delete -f /tmp/k8s-db-ingress.yaml
            fi
            echo "Creating k8s-db Ingress"
            kubectl apply -f /tmp/k8s-db-ingress.yaml

        {{- if .ParamExists "ingress/k8s-dashboard-hostname"}}
            echo "Ensure dns record for {{.Param "ingress/k8s-dashboard-hostname"}} points to $INGRESSIP"
        {{- else}}
              echo "K8S dashboard ingress at https://k8s-db.$INGRESSIP.xip.io"
        {{- end}}
        {{end}}
            # Clean up
            rm -f /tmp/certmanager-provider-secret.yaml
            rm -f /tmp/certmanager-clusterissuer.yaml
            rm -f /tmp/certmanager-crds.yaml
            rm -f /tmp/k8s-db-ingress.yaml
          fi
        fi

        echo "Finished successfully"
        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-ingress-nginx.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-keepalived.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # Kubernetes Rebar Immutable Boot (KRIB) Kubeadm Installer
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        # these need to be before krib-lib template
        {{if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing etcd/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}
        export RS_UUID="{{.Machine.UUID}}"

        MASTER_VIP={{.Param "krib/cluster-master-vip"}}
        MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)

        echo "My Master index is $MASTER_INDEX"
        if [[ $MASTER_INDEX != notme ]] ; then

          if [[ $MASTER_INDEX == 0 ]] ; then
            STATE="MASTER"
            PRIOR=101
          else
            {{if .ParamExists "packet/uuid"}}
              echo "Skipping for now if packet - don't have a good story for vip"
              echo "The master 0 has the VIP through elastic IP."
              echo "Need failover strategy. for it. Don't turn on other nodes"
              exit 0
            {{ end }}
            STATE="BACKUP"
            PRIOR=100
          fi
          INTF=$(ip -o a | awk '/inet {{.Machine.Address}}\// { print $2 }')

          all_ips=()
          OLD_IFS=$IFS
          IFS=" " ; while read ip
          do
            all_ips+=($ip)
            if [[ $RS_IP != $ip ]] ; then
              not_me_ips+=($ip)
            fi
          done <<< $(drpcli machines get "$RS_UUID" param "$KRIB_MASTERS_PARAM" --aggregate | jq -r '.[].Address' )
          IFS=$OLD_IFS

          install keepalived psmisc
          cat >/etc/keepalived/keepalived.conf <<EOFKEEP
        vrrp_script haproxy-check {
            script "killall -0 haproxy"
            interval 2
            weight 20
        }

        vrrp_instance haproxy-vip {
            state $STATE
            priority $PRIOR
            interface $INTF
            virtual_router_id 47
            advert_int 3

            authentication {
                auth_type PASS
                auth_pass 4be37dc3b4c90194d1600c483e10ad1d
            }

            virtual_ipaddress {
                $MASTER_VIP
            }

            track_script {
                haproxy-check weight 20
            }
        }
        EOFKEEP
          systemctl enable keepalived
          systemctl start keepalived

        fi

        echo "Finished successfully"
        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-keepalived.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-kubeadm-example.cfg:
      Available: false
      Bundle: ""
      Contents: |
        ---
        ###
        #  This is an example 'kubeadm.cfg' file used to build a
        #  Kubernetes cluster.  Please note that the apiVersion
        #  may update and this document may not be up to date with
        #  newer apiVersion specs.   This is just an example.  You
        #  must clone this and modify it for your needs, and use
        #  it to build your KRIB kubeadm cluster with.
        #
        #  Additionaly - see:
        #    https://github.com/kubernetes/kubernetes/tree/master/cmd/kubeadm/app/apis/kubeadm
        #
        #  Note non-production API versions may be listed there, verify
        #  which version is stable/production and supported in your
        #  version of 'kubeadm' that gets pulled in.
        ###

        api:
          advertiseAddress: 147.75.76.253
          bindPort: 6443
          controlPlaneEndpoint: 147.75.76.253
        apiServerCertSANs:
        - 127.0.0.1
        - 147.75.76.253
        - 147.75.76.253
        - krib-test-03.unspecified.domain.local
        apiServerExtraArgs:
          endpoint-reconciler-type: lease
        apiVersion: kubeadm.k8s.io/v1alpha2
        auditPolicy:
          logDir: /var/log/kubernetes/audit
          logMaxAge: 2
          path: ""
        bootstrapTokens:
        - groups:
          - system:bootstrappers:kubeadm:default-node-token
          token: od4g9a.pyzxl6jfihjiiwsk
          ttl: 24h0m0s
          usages:
          - signing
          - authentication
        certificatesDir: /etc/kubernetes/pki
        clusterName: kubernetes
        etcd:
          external:
            caFile: /etc/kubernetes/pki/etcd/server-ca.pem
            certFile: /etc/kubernetes/pki/etcd/client.pem
            endpoints:
            - https://147.75.76.253:2379
            keyFile: /etc/kubernetes/pki/etcd/client-key.pem
        imageRepository: k8s.gcr.io
        kind: MasterConfiguration
        kubeProxy:
          config:
            bindAddress: 0.0.0.0
            clientConnection:
              acceptContentTypes: ""
              burst: 10
              contentType: application/vnd.kubernetes.protobuf
              kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
              qps: 5
            clusterCIDR: 10.244.0.0/16
            configSyncPeriod: 15m0s
            conntrack:
              max: null
              maxPerCore: 32768
              min: 131072
              tcpCloseWaitTimeout: 1h0m0s
              tcpEstablishedTimeout: 24h0m0s
            enableProfiling: false
            healthzBindAddress: 0.0.0.0:10256
            hostnameOverride: ""
            iptables:
              masqueradeAll: false
              masqueradeBit: 14
              minSyncPeriod: 0s
              syncPeriod: 30s
            ipvs:
              excludeCIDRs: null
              minSyncPeriod: 0s
              scheduler: ""
              syncPeriod: 30s
            metricsBindAddress: 127.0.0.1:10249
            mode: ""
            nodePortAddresses: null
            oomScoreAdj: -999
            portRange: ""
            resourceContainer: /kube-proxy
            udpIdleTimeout: 250ms
        kubeletConfiguration:
          baseConfig:
            address: 0.0.0.0
            authentication:
              anonymous:
                enabled: false
              webhook:
                cacheTTL: 2m0s
                enabled: true
              x509:
                clientCAFile: /etc/kubernetes/pki/ca.crt
            authorization:
              mode: Webhook
              webhook:
                cacheAuthorizedTTL: 5m0s
                cacheUnauthorizedTTL: 30s
            cgroupDriver: cgroupfs
            cgroupsPerQOS: true
            clusterDNS:
            - 10.96.0.10
            clusterDomain: cluster.local
            containerLogMaxFiles: 5
            containerLogMaxSize: 10Mi
            contentType: application/vnd.kubernetes.protobuf
            cpuCFSQuota: true
            cpuManagerPolicy: none
            cpuManagerReconcilePeriod: 10s
            enableControllerAttachDetach: true
            enableDebuggingHandlers: true
            enforceNodeAllocatable:
            - pods
            eventBurst: 10
            eventRecordQPS: 5
            evictionHard:
              imagefs.available: 15%
              memory.available: 100Mi
              nodefs.available: 10%
              nodefs.inodesFree: 5%
            evictionPressureTransitionPeriod: 5m0s
            failSwapOn: true
            fileCheckFrequency: 20s
            hairpinMode: promiscuous-bridge
            healthzBindAddress: 127.0.0.1
            healthzPort: 10248
            httpCheckFrequency: 20s
            imageGCHighThresholdPercent: 85
            imageGCLowThresholdPercent: 80
            imageMinimumGCAge: 2m0s
            iptablesDropBit: 15
            iptablesMasqueradeBit: 14
            kubeAPIBurst: 10
            kubeAPIQPS: 5
            makeIPTablesUtilChains: true
            maxOpenFiles: 1000000
            maxPods: 110
            nodeStatusUpdateFrequency: 10s
            oomScoreAdj: -999
            podPidsLimit: -1
            port: 10250
            registryBurst: 10
            registryPullQPS: 5
            resolvConf: /etc/resolv.conf
            rotateCertificates: true
            runtimeRequestTimeout: 2m0s
            serializeImagePulls: true
            staticPodPath: /etc/kubernetes/manifests
            streamingConnectionIdleTimeout: 4h0m0s
            syncFrequency: 1m0s
            volumeStatsAggPeriod: 1m0s
        kubernetesVersion: v1.11.1
        networking:
          dnsDomain: cluster.local
          podSubnet: 10.244.0.0/16
          serviceSubnet: 10.96.0.0/12
        nodeRegistration:
          criSocket: /var/run/dockershim.sock
          name: krib-test-03.unspecified.domain.local
          taints:
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
        unifiedControlPlaneImage: ""
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-kubeadm-example.cfg
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-kubeadm.cfg.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        apiVersion: kubeadm.k8s.io/v1beta1
        kind: InitConfiguration
        localAPIEndpoint:
          advertiseAddress: {{ .Param "krib/cluster-master-vip" }}
          bindPort: {{ .Param "krib/cluster-api-port" }}
        bootstrapTokens:
        - groups:
            - system:bootstrappers:kubeadm:default-node-token
          ttl: 24h0m0s
          usages:
          - signing
          - authentication
        nodeRegistration:
          criSocket: "{{ .Param "krib/cluster-cri-socket" }}"
          name: {{ .Machine.Name }}
          taints:
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
        ---
        apiVersion: kubeadm.k8s.io/v1beta1
        kind: ClusterConfiguration
        apiServer:
          certSANs:
          - 127.0.0.1
          - {{ .Param "krib/cluster-master-vip" }}
          {{ range $elem := .Param "krib/cluster-masters" -}}
          - {{ $elem.Address }}
          - {{ $elem.Name }}
          {{ end -}}
          {{ if .ParamExists "krib/apiserver-extra-SANs" -}}
          {{ range $san := .Param "krib/apiserver-extra-SANs" -}}
          - {{ $san.SAN }}
          {{ end }}
          {{ end }}
          extraArgs:
            audit-log-path: /var/log/kubernetes/audit
            audit-log-maxage: "2"
            endpoint-reconciler-type: "lease"
            {{ if .ParamExists "krib/sign-kubelet-server-certs" -}}{{ if eq (.Param "krib/sign-kubelet-server-certs") true -}}
            kubelet-certificate-authority: "/etc/kubernetes/pki/ca.crt"
            {{ end -}}
            {{ end -}}
            {{ if .ParamExists "krib/apiserver-extra-args" -}}
            {{ range $key, $value  := .Param "krib/apiserver-extra-args" -}}
            {{ $key }}: {{ $value }}
            {{ end }}
            {{ end }}
        certificatesDir: /etc/kubernetes/pki
        clusterName: {{ .Param "krib/cluster-name" }}
        {{ $ha := ne (int 1) (int (.Param "krib/cluster-master-count")) -}}
        {{ if $ha -}}
        controlPlaneEndpoint: {{ .Param "krib/cluster-master-vip" }}:{{ .Param "krib/cluster-api-vip-port" }}
        {{ else -}}
        controlPlaneEndpoint: {{ .Param "krib/cluster-master-vip" }}:{{ .Param "krib/cluster-api-port" }}
        {{ end -}}
        imageRepository: {{ .Param "krib/cluster-image-repository" }}
        etcd:
          external:
            caFile: /etc/kubernetes/pki/etcd/server-ca.pem
            certFile: /etc/kubernetes/pki/etcd/client.pem
            keyFile: /etc/kubernetes/pki/etcd/client-key.pem
            endpoints:
        {{if and $ha (.ParamExists "krib/cluster-master-vip") (.ParamExists "etcd/cluster-client-vip-port") }}
              - https://{{ .Param "krib/cluster-master-vip" }}:{{ .Param "etcd/cluster-client-vip-port" }}
        {{else}}
          {{ $port := .Param "etcd/client-port" -}}
          {{- range $elem := .Param "etcd/servers"}}
              - https://{{ $elem.Address }}:{{ $port }}
          {{ end -}}
        {{ end -}}
        useHyperKubeImage: true
        kubernetesVersion: {{ .Param "krib/cluster-kubernetes-version" }}
        networking:
          dnsDomain: "{{ .Param "krib/cluster-service-dns-domain" }}"
          podSubnet: "{{ .Param "krib/cluster-pod-subnet" }}"
          serviceSubnet: "{{ .Param "krib/cluster-service-subnet" }}"
        ---
        apiVersion: kubeproxy.config.k8s.io/v1alpha1
        kind: KubeProxyConfiguration
        bindAddress: 0.0.0.0
        clientConnection:
          acceptContentTypes: ""
          burst: 10
          contentType: application/vnd.kubernetes.protobuf
          kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
          qps: 5
        clusterCIDR: ""
        configSyncPeriod: 15m0s
        conntrack:
          maxPerCore: 32768
          min: 131072
          tcpCloseWaitTimeout: 1h0m0s
          tcpEstablishedTimeout: 24h0m0s
        enableProfiling: false
        healthzBindAddress: 0.0.0.0:10256
        hostnameOverride: ""
        iptables:
          masqueradeAll: false
          masqueradeBit: 14
          minSyncPeriod: 0s
          syncPeriod: 30s
        ipvs:
          excludeCIDRs: null
          minSyncPeriod: 0s
          scheduler: ""
          syncPeriod: 30s
        metricsBindAddress: 127.0.0.1:10249
        mode: ""
        nodePortAddresses: null
        oomScoreAdj: -999
        portRange: ""
        resourceContainer: /kube-proxy
        udpIdleTimeout: 250ms
        ---
        apiVersion: kubelet.config.k8s.io/v1beta1
        kind: KubeletConfiguration
        address: 0.0.0.0
        authentication:
          anonymous:
            enabled: false
          webhook:
            cacheTTL: 2m0s
            enabled: true
          x509:
            clientCAFile: /etc/kubernetes/pki/ca.crt
        authorization:
          mode: Webhook
          webhook:
            cacheAuthorizedTTL: 5m0s
            cacheUnauthorizedTTL: 30s
        cgroupDriver: cgroupfs
        cgroupsPerQOS: true
        clusterDNS:
        - {{ .Param "krib/cluster-dns" }}
        clusterDomain: {{ .Param "krib/cluster-domain" }}
        configMapAndSecretChangeDetectionStrategy: Watch
        containerLogMaxFiles: 5
        containerLogMaxSize: 10Mi
        contentType: application/vnd.kubernetes.protobuf
        cpuCFSQuota: true
        cpuCFSQuotaPeriod: 100ms
        cpuManagerPolicy: none
        cpuManagerReconcilePeriod: 10s
        enableControllerAttachDetach: true
        enableDebuggingHandlers: true
        enforceNodeAllocatable:
        - pods
        eventBurst: 10
        eventRecordQPS: 5
        evictionHard:
          imagefs.available: 15%
          memory.available: 100Mi
          nodefs.available: 10%
          nodefs.inodesFree: 5%
        evictionPressureTransitionPeriod: 5m0s
        failSwapOn: true
        fileCheckFrequency: 20s
        hairpinMode: promiscuous-bridge
        healthzBindAddress: 127.0.0.1
        healthzPort: 10248
        httpCheckFrequency: 20s
        imageGCHighThresholdPercent: 85
        imageGCLowThresholdPercent: 80
        imageMinimumGCAge: 2m0s
        iptablesDropBit: 15
        iptablesMasqueradeBit: 14
        kubeAPIBurst: 10
        kubeAPIQPS: 5
        makeIPTablesUtilChains: true
        maxOpenFiles: 1000000
        maxPods: 110
        nodeLeaseDurationSeconds: 40
        nodeStatusUpdateFrequency: 10s
        oomScoreAdj: -999
        podPidsLimit: -1
        port: 10250
        registryBurst: 10
        registryPullQPS: 5
        resolvConf: /etc/resolv.conf
        rotateCertificates: true
        runtimeRequestTimeout: 2m0s
        serializeImagePulls: true
        {{ if .ParamExists "krib/sign-kubelet-server-certs" -}}{{ if eq (.Param "krib/sign-kubelet-server-certs") true -}}
        serverTLSBootstrap: true
        {{ end -}}
        {{ end -}}
        streamingConnectionIdleTimeout: 4h0m0s
        syncFrequency: 1m0s
        volumeStatsAggPeriod: 1m0s
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-kubeadm.cfg.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-kubelet-rubber-stamp.yaml.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        ---
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: kubelet-rubber-stamp
          namespace: kube-system
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRole
        metadata:
          name: kubelet-rubber-stamp
        rules:
        - apiGroups:
          - certificates.k8s.io
          resources:
          - certificatesigningrequests
          verbs:
          - delete
          - get
          - list
          - watch
        - apiGroups:
          - certificates.k8s.io
          resources:
          - certificatesigningrequests/approval
          verbs:
          - create
          - update
        - apiGroups:
          - authorization.k8s.io
          resources:
          - subjectaccessreviews
          verbs:
          - create
        ---
        kind: ClusterRoleBinding
        apiVersion: rbac.authorization.k8s.io/v1
        metadata:
          name: kubelet-rubber-stamp
        subjects:
        - kind: ServiceAccount
          namespace: kube-system
          name: kubelet-rubber-stamp
        roleRef:
          kind: ClusterRole
          name: kubelet-rubber-stamp
          apiGroup: rbac.authorization.k8s.io
        ---
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: kubelet-rubber-stamp
          namespace: kube-system
        spec:
          replicas: 1
          selector:
            matchLabels:
              name: kubelet-rubber-stamp
          template:
            metadata:
              labels:
                name: kubelet-rubber-stamp
            spec:
              serviceAccountName: kubelet-rubber-stamp
              tolerations:
                - effect: NoSchedule
                  operator: Exists
              nodeSelector:
                node-role.kubernetes.io/master: ""
              priorityClassName: system-cluster-critical
              containers:
                - name: kubelet-rubber-stamp
                  image: quay.io/kontena/kubelet-rubber-stamp-amd64:latest
                  imagePullPolicy: Always
                  env:
                    - name: WATCH_NAMESPACE
                      value: ""
                    - name: POD_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.name
                    - name: OPERATOR_NAME
                      value: "kubelet-rubber-stamp"
        ---
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-kubelet-rubber-stamp.yaml.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-lib.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: "#\n# Library of common sync/control functions\n#\n# Assumes PROFILE_TOKEN
        and CLUSTER_PROFILE are set\n#\n# Also assumes that setup.tmpl has been include
        to get the task functions\n#   - get_param\n# which assumes that RS_UUID and
        RS_KEY or RS_TOKEN are set.\n#\n[[ -z \"$PROFILE_TOKEN\" ]] && xiterr 1 \"required
        PROFILE_TOKEN not set\"\n[[ -z \"$CLUSTER_PROFILE\" ]] && xiterr 1 \"required
        CLUSTER_PROFILE not set\"\n\nwait_for_variable() {\n  local varname=$1\n  local
        var=$(get_param \"$varname\" | jq -r .)\n  local sleep_time=1\n  local timer_count=0\n
        \ local timer_expire=600\n\n  while [[ $var == null ]] ; do\n    sleep $sleep_time\n
        \   var=$(get_param \"$varname\" | jq -r .)\n    (( timer_count = timer_count
        + sleep_time ))\n    [[ $timer_count -ge $timer_expire ]] && xiterr 1 \"timer
        expired ($timer_expire seconds) in loop ('${FUNCNAME[0]}')\"\n  done\n  echo
        $var\n}\n\nwait_for_count() {\n  local varname=$1\n  local count=$2\n  local
        sleep_time=1\n  local timer_count=0\n  local timer_expire=600\n\n  local cl=$(get_param
        \"$varname\")\n  while [[ $(jq length <<< \"$cl\") -lt $count ]]; do\n    #
        we're good!\n    sleep $sleep_time\n    cl=$(get_param \"$varname\")\n    ((
        timer_count = timer_count + sleep_time ))\n    [[ $timer_count -ge $timer_expire
        ]] && xiterr 1 \"timer expired ($timer_expire seconds) in loop ('${FUNCNAME[0]}')\"\n
        \ done\n  echo \"Done\"\n}\n\n#\n# add_me_if_not_count will add to the list
        if not in the list and not too many in the list.\n#\n# This assumes that the
        varname is of type array of objects (with Name, Uuid, and Address fields)\n#\n#
        Args:\n#  - varname\n#  - count\n#\n# Returns \"notme\" if not added or the
        index in the list if added.\n#\nadd_me_if_not_count() {\n  local varname=$1\n
        \ local count=$2\n  local me_ip=$3\n\n  local me_uuid={{.Machine.Uuid}}\n
        \ local me_name={{.Machine.Name}}\n\n  local index=\"notme\"\n\n  local cl=$(get_param
        $varname)\n  while [[ $(jq length <<< \"$cl\") -lt $count ]]; do\n    # we're
        good!\n    if [[ $(jq \"map(.Uuid) | index(\\\"${me_uuid}\\\")\" <<< \"$cl\")
        != null ]] ; then\n      index=$(jq \"map(.Uuid) | index(\\\"${me_uuid}\\\")\"
        <<< \"$cl\")\n      # set the machine icon for troubleshooting\n      drpcli
        machines update $RS_UUID \"{\\\"Meta\\\":{\\\"color\\\":\\\"yellow\\\", \\\"icon\\\":
        \\\"chess queen\\\"} }\" || true\n      echo \"   Selected ${me_name} (${me_uuid})\"\n
        \     break\n    fi\n    NEW_LEADERS=$(jq \". += [{\\\"Name\\\": \\\"${me_name}\\\",
        \\\"Uuid\\\": \\\"${me_uuid}\\\", \\\"Address\\\": \\\"${me_ip}\\\"}]\" <<<
        \"$cl\")\n    drpcli -r \"$cl\" -T \"$PROFILE_TOKEN\" profiles set \"$CLUSTER_PROFILE\"
        param $varname to \"$NEW_LEADERS\" 2>/dev/null >/dev/null && break\n    #
        sleep is is a hack but it allows for backoffs\n    sleep 1\n    # get the
        current cluster leaders\n    cl=$(get_param $varname)\n  done\n  cl=$(get_param
        $varname)\n  if [[ $(jq \"map(.Uuid) | index(\\\"${me_uuid}\\\")\" <<< \"$cl\")
        != null ]] ; then\n    index=$(jq \"map(.Uuid) | index(\\\"${me_uuid}\\\")\"
        <<< \"$cl\")\n  fi\n  echo $index\n}\n\n#\n# find_me looks for me specified
        by a field and value on varname.\n#\n# This assumes that the varname is of
        type array of objects (with Name, Uuid, and Address fields)\n#\n# Args:\n#
        \ - varname\n#  - field\n#  - value of field\n#\n# Returns \"notme\" if not
        added or the index in the list\n#\nfind_me() {\n  local varname=$1\n  local
        field=$2\n  local me=$3\n\n  local index=\"notme\"\n\n  val=$(get_param \"$varname\"
        | jq \"map(.$field) | index(\\\"${me}\\\")\")\n  if [[ $val != null ]] ; then\n
        \   index=$val\n  fi\n  echo $index\n}\n\nget_ingress_ip() {\n\techo $(kubectl
        get svc nginx-ingress-controller \\\n\t  -n kube-system -o jsonpath=\"{.status.loadBalancer.ingress[*].ip}\")\n}\n\nwait_for_ingress()
        {\n  set +e\n  local var=$(get_ingress_ip)\n  local sleep_time=1\n  local
        timer_count=0\n  local timer_expire=6\n  while [[ $var == \"\" ]] ; do\n    sleep
        $sleep_time\n    var=$(get_ingress_ip)\n    (( timer_count = timer_count +
        sleep_time ))\n    [[ $timer_count -ge $timer_expire ]] && xiterr 1 \"timer
        expired ($timer_expire seconds) in loop ('${FUNCNAME[0]}')\"\n  done\n  echo
        $var\n  set -e\n}\n\nwith_backoff() {\n  local max_attempts=${ATTEMPTS-5}\n
        \ local timeout=${TIMEOUT-1}\n  local attempt=1\n  local exitCode=0\n\n  while
        (( $attempt < $max_attempts ))\n  do\n    if \"$@\"\n    then\n      return
        0\n    else\n      exitCode=$?\n    fi\n\n    echo \"Failure ($@)! Retrying
        in $timeout..\" 1>&2\n    sleep $timeout\n    attempt=$(( attempt + 1 ))\n
        \   timeout=$(( timeout * 2 ))\n  done\n\n  if [[ $exitCode != 0 ]]\n  then\n
        \   echo \"Execution failed for: $@\" 1>&2\n  fi\n\n  return $exitCode\n}\n\ndownload()
        {\n  with_backoff curl --connect-timeout 10 --max-time 360 --retry 1 --retry-delay
        1 --retry-max-time 40 -sS \"$@\"\n}\n\n# Common parameters that are used.\nKRIB_BOOTSTRAP_TOKEN=\"krib/cluster-bootstrap-token\"\nKRIB_PROFILE_PARAM=\"krib/cluster-profile\"\nKRIB_NAME_PARAM=\"krib/cluster-name\"\nKRIB_MASTERS_PARAM=\"krib/cluster-masters\"\nKRIB_MASTER_VIP_PARAM=\"krib/cluster-master-vip\"\nKRIB_MASTER_COUNT_PARAM=\"krib/cluster-master-count\"\nKRIB_MASTER_ON_ETCDS_PARAM=\"krib/cluster-masters-on-etcds\"\nKRIB_MASTER_CERTS_PARAM=\"krib/cluster-master-certs\"\nKRIB_ADMIN_CONF_PARAM=\"krib/cluster-admin-conf\"\nKRIB_JOIN_PARAM=\"krib/cluster-join-command\"\nKRIB_CLUSTER_KUBEADM_CFG_PARAM=\"krib/cluster-kubeadm-cfg\"\n\nETCD_PROFILE_PARAM=\"etcd/cluster-profile\"\nETCD_SERVERS_PARAM=\"etcd/servers\"\nETCD_SERVER_CA_PARAM=\"etcd/server-ca-name\"\nETCD_CLIENT_CA_PARAM=\"etcd/client-ca-name\"\nETCD_CLIENT_CA_PW_PARAM=\"etcd/client-ca-pw\"\nETCD_CLIENT_PORT_PARAM=\"etcd/client-port\"\n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-lib.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-longhorn.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # Kubernetes Rebar Integrated Boot (KRIB) Rancher Longhorn Installer
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        echo "Run helm install on the master (skip for minions)..."

        {{if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing krib/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
        echo "My Master index is $MASTER_INDEX"

        echo "Linking the Rancher Longhorn working directory into place."
        mkdir -p /var/lib/rancher
        mkdir -p /mnt/hdd/rancher/longhorn
        ln -s /mnt/hdd/rancher/longhorn /var/lib/rancher/longhorn

        # iscsi-tools needed on all cluster members
        if ! which iscsiadm ; then
          install iscsi-initiator-utils
        fi

        if [[ $MASTER_INDEX != notme ]] ; then
          if [[ $MASTER_INDEX == 0 ]] ; then
            export KUBECONFIG=/etc/kubernetes/admin.conf
            if ! which wget ; then
              install wget
            fi

            echo "Start Rancher Longhorn Install"
            LONGHORNURL={{ .Param "krib/longhorn-config" }}
            LONGHORNCLEANUP=$(echo $LONGHORNURL | sed 's/deploy\/longhorn.yaml/scripts\/cleanup.sh/')
            wget -O /tmp/longhorn.yaml $LONGHORNURL
            sed -i 's/#- --driver/- --driver/' /tmp/longhorn.yaml
            sed -i 's/#- flexvolume/- csi/' /tmp/longhorn.yaml
            sed -i 's/type: LoadBalancer/type: ClusterIP/' /tmp/longhorn.yaml

            if [[ -n $(kubectl get namespaces | grep -w rancher-longhorn) ]] ; then
              echo "Purging existing Rancher Longhorn install"
              curl -sSfL $LONGHORNCLEANUP | bash
              kubectl delete -f /tmp/longhorn.yaml
            fi

            echo "Installing Rancher Longhorn"
            kubectl apply -f /tmp/longhorn.yaml

            if [[ $(kubectl -n kube-system get services -o json | jq -r '.items | contains([{"metadata":{"name":"nginx-ingress-controller"}}])') == "true" ]] ; then
              echo "Wait until nginx-ingress-controller gets a LoadBalancer IP"
              INGRESSIP=$(wait_for_ingress)

              echo "expanding inline template to /tmp/longhorn-db-ingress.yaml"
              cat > /tmp/longhorn-db-ingress.yaml << EOF
        apiVersion: extensions/v1beta1
        kind: Ingress
        metadata:
          name: longhorn-db
          namespace: longhorn-system
          annotations:
            kubernetes.io/ingress.class: nginx
        {{- if .ParamExists "certmanager/email"}}{{- if .ParamExists "ingress/longhorn-dashboard-hostname"}}
            kubernetes.io/tls-acme: "true"
        {{- end}}{{- end}}
        spec:
          rules:
        {{- if .ParamExists "ingress/longhorn-dashboard-hostname"}}
          - host: "{{.Param "ingress/longhorn-dashboard-hostname"}}"
        {{- else}}
          - host: "longhorn-db.$INGRESSIP.xip.io"
        {{- end}}
            http:
              paths:
              - backend:
                  serviceName: longhorn-frontend
                  servicePort: 80
          tls:
          - hosts:
        {{- if .ParamExists "ingress/longhorn-dashboard-hostname"}}
            - "{{.Param "ingress/longhorn-dashboard-hostname"}}"
        {{- else}}
            - "longhorn-db.$INGRESSIP.xip.io"
        {{- end}}
        {{- if .ParamExists "certmanager/email"}}{{- if .ParamExists "ingress/longhorn-dashboard-hostname"}}
            secretName: longhorn-db-tls
        {{- end}}{{- end}}
        EOF

              if [[ $(kubectl get -n longhorn-system ingress -o json | jq -r '.items | contains([{"metadata":{"name":"longhorn-db"}}])') == "true" ]] ; then
                echo "Removing existing longhorn-dashboard Ingress"
                kubectl delete -f /tmp/longhorn-dashboard-ingress.yaml
              fi
              echo "Creating longhorn-db Ingress"
              kubectl apply -f /tmp/longhorn-db-ingress.yaml
        {{- if .ParamExists "ingress/longhorn-dashboard-hostname"}}
              echo "Ensure dns record for {{.Param "ingress/longhorn-dashboard-hostname"}} points to $INGRESSIP"
        {{- else}}
              echo "Longhorn dashboard ingress at https://longhorn-db.$INGRESSIP.xip.io"
        {{- end}}
            fi

            echo "expanding inline template to /tmp/longhorn-storageclass.yaml"
            cat > /tmp/longhorn-storageclass.yaml << EOF
        kind: StorageClass
        apiVersion: storage.k8s.io/v1
        metadata:
          name: longhorn
        provisioner: rancher.io/longhorn
        parameters:
          numberOfReplicas: "3"
          staleReplicaTimeout: "30"
          fromBackup: ""
        EOF

            echo "Creating longhorn StorageClass"
            kubectl apply -f /tmp/longhorn-storageclass.yaml

            # Clean up
            rm -f /tmp/longhorn.yaml
            rm -f /tmp/longhorn-dashboard-ingress.yaml
            rm -f /tmp/longhorn-storageclass.yaml
          fi
        fi

        echo "Finished successfully"
        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-longhorn.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-metallb.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: "#!/usr/bin/env bash\n# Kubernetes Rebar Immutable Boot (KRIB) Install
        MetalLB\nset -e\n\n# Get access and who we are.\n{{template \"setup.tmpl\"
        .}}\n\n# these need to be before krib-lib template\n{{if .ParamExists \"krib/cluster-profile\"
        -}}\nCLUSTER_PROFILE={{.Param \"krib/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken
        (.Param \"krib/cluster-profile\") 7200}}\n{{else -}}\nxiterr 1 \"Missing etcd/cluster-profile
        on the machine!\"\n{{end -}}\n\n{{template \"krib-lib.sh.tmpl\" .}}\n\n{{
        if eq (.ParamExists \"metallb/l2-ip-range\") true -}}\n\nMASTER_INDEX=$(find_me
        $KRIB_MASTERS_PARAM \"Uuid\" $RS_UUID)\n\nif [[ $MASTER_INDEX == 0 ]] ; then\n
        \ export KUBECONFIG=/etc/kubernetes/admin.conf\n  if ! which wget ; then\n
        \   install wget\n  fi\n  \n  wget -O /tmp/metallb.yaml {{ .Param \"krib/metallb-config\"
        }}\n{{$port := .Param \"metallb/monitoring-port\"}}\n  sed -i 's/prometheus.io\\/port:
        \"7472\"/prometheus.io\\/port: \"{{$port}}\"/' /tmp/metallb.yaml\n  sed -i
        's/- --port=7472/- --port={{$port}}/' /tmp/metallb.yaml\n  sed -i 's/containerPort:
        7472/containerPort: {{$port}}/' /tmp/metallb.yaml\n{{$cpu := .Param \"metallb/limits-cpu\"}}
        \ \n  sed -i 's/cpu: 100m/cpu: {{$cpu}}/' /tmp/metallb.yaml\n{{$memory :=
        .Param \"metallb/limits-memory\"}}  \n  sed -i 's/memory: 100Mi/memory: {{$memory}}/'
        /tmp/metallb.yaml\n\n  {{ if .ParamExists \"krib/metallb-container-image-controller\"
        }}\n  # Replace default container image with the one specified in \"krib/metallb-container-image\"\n
        \ sed -i 's|metallb/controller:master|{{ .Param \"krib/metallb-container-image-controller\"
        }}|' /tmp/metallb.yaml\n  {{ end -}}\n\n  {{ if .ParamExists \"krib/metallb-container-image-speaker\"
        }}\n  # Replace default container image with the one specified in \"krib/metallb-container-image\"\n
        \ sed -i 's|metallb/speaker:master|{{ .Param \"krib/metallb-container-image-speaker\"
        }}|' /tmp/metallb.yaml\n  {{ end -}}\n\n  if [[ -n $(kubectl get namespaces
        | grep -w metallb-system) ]] ; then\n    echo \"Purging existing metallb install\"\n
        \   kubectl delete -f /tmp/metallb.yaml\n  fi\n\n{{$l2_ip_range := .Param
        \"metallb/l2-ip-range\"}} \n  cat >/tmp/metallb-config.yaml <<EOFCONFIG\n---\napiVersion:
        v1\nkind: ConfigMap\nmetadata:\n  namespace: metallb-system\n  name: config\ndata:\n
        \ config: |\n    address-pools:\n    - name: loadbalanced\n      protocol:
        layer2\n      addresses:\n      - {{$l2_ip_range}}  \nEOFCONFIG\n\n  kubectl
        apply -f /tmp/metallb.yaml\n  kubectl apply -f /tmp/metallb-config.yaml\nelse\n
        \ echo \"I was not the leader, skipping metallb install\"\nfi\n\necho \"Finished
        MetalLB L2 deployment successfully\"\n{{ else if eq (.ParamExists \"metallb/l3-ip-range\")
        true -}}\n\nMASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM \"Uuid\" $RS_UUID)\n\nif
        [[ $MASTER_INDEX == 0 ]] ; then\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n
        \ if ! which wget ; then\n    install wget\n  fi\n  \n  wget -O /tmp/metallb.yaml
        https://github.com/danderson/metallb/raw/{{ .Param \"krib/metallb-version\"
        }}/manifests/metallb.yaml\n\n  # Replace references to \"master\" with the
        user-specified tag (if user has left the default at \"master\" this will have
        no effect)\n  sed -i 's/speaker:master/speaker:{{ .Param \"krib/metallb-version\"
        }}/' /tmp/metallb.yaml\n  sed -i 's/controller:master/controller:{{ .Param
        \"krib/metallb-version\" }}/' /tmp/metallb.yaml\n\n{{$port := .Param \"metallb/monitoring-port\"}}\n
        \ sed -i 's/prometheus.io\\/port: \"7472\"/prometheus.io\\/port: \"{{$port}}\"/'
        /tmp/metallb.yaml\n  sed -i 's/- --port=7472/- --port={{$port}}/' /tmp/metallb.yaml\n
        \ sed -i 's/containerPort: 7472/containerPort: {{$port}}/' /tmp/metallb.yaml\n{{$cpu
        := .Param \"metallb/limits-cpu\"}}  \n  sed -i 's/cpu: 100m/cpu: {{$cpu}}/'
        /tmp/metallb.yaml\n{{$memory := .Param \"metallb/limits-memory\"}}  \n  sed
        -i 's/memory: 100Mi/memory: {{$memory}}/' /tmp/metallb.yaml\n\n  {{ if .ParamExists
        \"krib/metallb-container-image-controller\" }}\n  # Replace default container
        image with the one specified in \"krib/metallb-container-image\"\n  sed -i
        's|metallb/controller|{{ .Param \"krib/metallb-container-image-controller\"
        }}|' /tmp/metallb.yaml\n  {{ end -}}\n\n  {{ if .ParamExists \"krib/metallb-container-image-speaker\"
        }}\n  # Replace default container image with the one specified in \"krib/metallb-container-image\"\n
        \ sed -i 's|metallb/speaker|{{ .Param \"krib/metallb-container-image-speaker\"
        }}|' /tmp/metallb.yaml\n  {{ end -}}\n\n  if [[ -n $(kubectl get namespaces
        | grep -w metallb-system) ]] ; then\n    echo \"Purging existing metallb install\"\n
        \   kubectl delete -f /tmp/metallb.yaml\n  fi\n\n{{$l3_ip_range := .Param
        \"metallb/l3-ip-range\"}} \n{{$l3_peer_address := .Param \"metallb/l3-peer-address\"}}
        \n\n  cat >/tmp/metallb-config.yaml <<EOFCONFIG\n---\napiVersion: v1\nkind:
        ConfigMap\nmetadata:\n  namespace: metallb-system\n  name: config\ndata:\n
        \ config: |\n    peers:\n    - peer-address: {{$l3_peer_address}}  \n      peer-asn:
        64501\n      my-asn: 64500\n    address-pools:\n    - name: default\n      protocol:
        bgp\n      avoid-buggy-ips: true\n      addresses:\n      - {{$l3_ip_range}}
        \nEOFCONFIG\n\n  kubectl apply -f /tmp/metallb.yaml\n  kubectl apply -f /tmp/metallb-config.yaml\nelse\n
        \ echo \"I was not the leader, skipping metallb install\"\nfi\n{{else -}}\necho
        \"No metallb/l2-ip-range or metallb/l3-ip-range, skipping install\"\n{{end
        -}}\nexit 0\n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-metallb.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-nginx-external-tcp-services.yaml.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: tcp-services
          namespace: ingress-nginx-external
        data:
            {{ if .ParamExists "krib/nginx-external-tcp-services" -}}
            {{ range $key, $value  := .Param "krib/nginx-external-tcp-services" -}}
            {{ $key }}: {{ $value }}
            {{ end }}
            {{ end }}
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-nginx-external-tcp-services.yaml.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-nginx-external-udp-services.yaml.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: udp-services
          namespace: ingress-nginx-external
        data:
            {{ if .ParamExists "krib/nginx-external-tcp-services" -}}
            {{ range $key, $value  := .Param "krib/nginx-external-udp-services" -}}
            {{ $key }}: {{ $value }}
            {{ end }}
            {{ end }}
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-nginx-external-udp-services.yaml.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-nginx-tcp-services.yaml.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: tcp-services
          namespace: ingress-nginx
        data:
            {{ if .ParamExists "krib/nginx-tcp-services" -}}
            {{ range $key, $value  := .Param "krib/nginx-tcp-services" -}}
            {{ $key }}: {{ $value }}
            {{ end }}
            {{ end }}
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-nginx-tcp-services.yaml.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-nginx-udp-services.yaml.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: udp-services
          namespace: ingress-nginx
        data:
            {{ if .ParamExists "krib/nginx-udp-services" -}}
            {{ range $key, $value  := .Param "krib/nginx-udp-services" -}}
            {{ $key }}: {{ $value }}
            {{ end }}
            {{ end }}
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-nginx-udp-services.yaml.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-operate.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # Operate on a KRIB built Kubernetes node. Supports 'drain', 'delete', 'cordon', 'uncordon' ops.

        # if we exit, remove sensitive temporary config files
        trap cleanup EXIT SIGINT SIGTERM SIGQUIT
        set -e

        function xiterr() { [[ $1 =~ ^[0-9]+$ ]] && { XIT=$1; shift; } || XIT=1; echo "FATAL: $*"; exit $XIT; }
        function cleanup() {
          echo '>>> in cleanup'
        {{ if eq (.Param "rs-debug-enable") true -}}
          echo "DEBUG mode enabled, not cleaning up after ourselves"
          echo "CFG FILE:  $CFG"
        {{else -}}
          rm -f $CFG
        {{end -}}
        }

        # Get access and who we are.
        {{template "setup.tmpl" .}}
        [[ $RS_UUID ]] && export RS_UUID='{{.Machine.UUID}}'

        echo ">>> KRIB operation on Kubernetes node starting at date: $(date)"

        # get the Operation we want to run
        {{if .ParamExists "krib/operate-action" -}}
        export OP='{{.Param "krib/operate-action"}}'
        {{else -}}
        xiterr 1 "Required 'krib/operate-action' action not specified"
        {{end -}}

        OP_CMD="$OP"
        MSG=">>> Operation '$OP' is starting ..."
        case $OP in
          drain)      echo "$MSG"
                      OPTIONS="--ignore-daemonsets --delete-local-data"
            ;;
          delete)     echo "$MSG"
                      OP_CMD="delete node"
                      OPTIONS=""
            ;;
          cordon)     echo "$MSG"
                      OPTIONS=""
            ;;
          uncordon)   echo "$MSG"
                      OPTIONS=""
            ;;
          *) xiterr 1 "Operation not supported ('$OP'): only support drain, delete, cordon, uncordon"
            ;;
        esac

        _kubectl=$(which kubectl)
        [[ -z "$_kubectl" ]] && xiterr 1 "Unable to find 'kubectl' in PATH ($PATH)"
        [[ -z "$RS_ENDPOINT" ]] && xiterr 1 "RS_ENDPOINT not defined ... "

        {{if .ParamExists "krib/operate-on-node" -}}
        export NODE='{{.Param "krib/operate-on-node"}}'
        echo ">>> Attempting to run '$OP' on specified node '$NODE' ... "
        {{else -}}
        export NODE="{{.Machine.Name}}"
        echo ">>> No operate-on-node specified ('krib/operate-on-node') ... "
        echo ">>> Assuming operation '$OP' for current Machine ('$NODE')..."
        {{end -}}

        [[ -z "$NODE" ]] && xiterr "Somehow our 'NODE' to operate on is empty"

        ###
        #  Override the default OPTIONS specified in the case statement
        #  above, if the Operator defines the 'krib/operate-options' Param
        #  on a Machine object, it'll replace the default Profile in the Stage
        #
        #  drain options documentation:
        #  https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#drain
        #  uncordon options documentation:
        #  https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#uncordon
        ###
        {{if .ParamExists "krib/operate-options" -}}
        export OPTIONS='{{.Param "krib/operate-options"}}'
        {{end -}}
        echo ">>> Operate options have been set to '$OPTIONS' ... "

        CFG=$(mktemp /tmp/.XXXXXXXXXXX.conf)
        chmod 600 $CFG

        {{if .ParamExists "krib/cluster-admin-conf" -}}

        cat > $CFG << 'EOFCFG'
        {{.ParamAsJSON "krib/cluster-admin-conf"}}
        EOFCFG

        {{else -}}
        drpcli machines update $RS_UUID '{"Meta":{"color":"red", "icon": "exclamation triangle"}}'
        xiterr 1 "Required 'krib/cluster-admin-conf' Param not set - is this a KRIB cluster ???"
        {{end -}}

        [[ ! -s "$CFG" ]] && xiterr 1 "generated admin config file has zero size"
        export KUBECONFIG=$CFG

        echo ">>> NODES in this cluster ... "
        kubectl get nodes -o wide

        kubectl describe node $NODE > /dev/null 2>&1 \
          && echo ">>> Node '$NODE' is valid cluster member ... " \
          || xiterr 1 "Node '$NODE' does not appear to exist in the cluster"

        echo ">>>"
        echo ">>> PRIOR TO OPERATE ACTION ($OP):"
        kubectl get nodes -o wide

        echo ">>>"
        ###
        #  drain:
        #    blocks and waits to safely drain a node, respecting any PodDisruptionBudgets
        #    can be overriden by setting additonal 'drain' options - we require the
        #    'ignore-daemonsets' option because of calico and kube-proxy managed pods
        #    the defaults for OPTIONS includes '--delete-local-data --ignore-daemonsets'
        #    and if overridden, the drain operation may not succeed.
        #  uncordon:
        #    adds the node back in to the cluster, does not require additional options
        ###
        case $OP in
          drain|delete|cordon|uncordon)
            kubectl $OP_CMD $NODE $OPTIONS \
              && echo ">>> Node '$NODE' successfully operated ('$OP_CMD') on ... " \
              || xiterr 1 "Node '$NODE' FAILED action $OP_CMD"
            ;;
          *) xiterr 1 "unknown action '$OP'" ;;
        esac

        echo ">>>"
        echo ">>> AFTER OPERATE ACTION ($OP):"
        kubectl get nodes -o wide

        echo
        echo "Finished action ($OP) successfully on '$NODE' at $(date)"
        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-operate.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-rook-ceph.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: "#!/usr/bin/env bash\n# Kubernetes Rebar Integrated Boot (KRIB) nginx-ingress
        Installer\nset -e\n\n# Get access and who we are.\n{{template \"setup.tmpl\"
        .}}\n\n{{if .ParamExists \"krib/cluster-profile\" -}}\nCLUSTER_PROFILE={{.Param
        \"krib/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken (.Param
        \"krib/cluster-profile\") 7200}}\n{{else -}}\nxiterr 1 \"Missing krib/cluster-profile
        on the machine!\"\n{{end -}}\n\n{{template \"krib-lib.sh.tmpl\" .}}\n\nMASTER_INDEX=$(find_me
        $KRIB_MASTERS_PARAM \"Uuid\" $RS_UUID)\n\nif [[ $MASTER_INDEX == 0 ]] ; then\n
        \ export KUBECONFIG=/etc/kubernetes/admin.conf\n  if ! which wget ; then\n
        \   install wget\n  fi\n\n  download -L https://github.com/rook/rook/raw/v{{
        .Param \"rook/ceph-version\" }}/cluster/examples/kubernetes/ceph/common.yaml
        \\\n    -o /tmp/rook-common.yaml\n  download -L https://github.com/rook/rook/raw/v{{
        .Param \"rook/ceph-version\" }}/cluster/examples/kubernetes/ceph/operator.yaml
        \\\n    -o /tmp/rook-operator.yaml \n  download -L https://github.com/rook/rook/raw/v{{
        .Param \"rook/ceph-version\" }}/cluster/examples/kubernetes/ceph/cluster.yaml
        \\\n    -o /tmp/rook-cluster.yaml\n  download -L https://github.com/rook/rook/raw/v{{
        .Param \"rook/ceph-version\" }}/cluster/examples/kubernetes/ceph/storageclass.yaml
        \\\n    -o /tmp/rook-storageclass.yaml\n\n  ##### Optionally replace containers
        in distributed YAML manifests with operator-specified\n  ##### containers
        from a private registry\n  #####\n  {{ if .ParamExists \"krib/rook-ceph-container-image-daemon-base\"
        }}\n  # Replace default container image with the one specified in \"krib/metallb-container-image-daemon-base\"\n
        \ sed -i 's|ceph/daemon-base|{{ .Param \"krib/rook-ceph-container-image-daemon-base\"
        }}|' /tmp/rook-cluster.yaml\n  {{ end -}}\n    {{ if .ParamExists \"krib/rook-ceph-container-image-ceph\"
        }}\n  # Replace default container image with the one specified in \"krib/metallb-container-image-ceph\"\n
        \ sed -i 's|ceph/ceph|{{ .Param \"krib/rook-ceph-container-image-ceph\" }}|'
        /tmp/rook-cluster.yaml\n  {{ end -}}\n  {{ if .ParamExists \"krib/rook-ceph-container-image\"
        }}\n  # Replace default container image with the one specified in \"krib/metallb-container-image\"\n
        \ sed -i 's|image: rook/ceph|image: {{ .Param \"krib/rook-ceph-container-image\"
        }}|' /tmp/rook-operator.yaml\n  # Replace default container image with the
        one specified in \"krib/metallb-container-image\"\n  sed -i 's|image: rook/ceph|image:
        {{ .Param \"krib/rook-ceph-container-image\" }}|' /tmp/rook-ceph-toolbox.yaml\n
        \ {{ end -}}\n  #####\n  ##### Finish optional replacement\n\n  # if [[ -n
        $(kubectl get namespaces | grep -w ingress-nginx) ]] ; then\n  #   echo \"Purging
        existing nginx-ingress install\"\n  #   kubectl delete -f /tmp/nginx-ingress-mandatory.yaml\n
        \ # fi\n\n  # Create the common elements\n  kubectl apply -f /tmp/rook-common.yaml\n
        \ kubectl apply -f /tmp/rook-operator.yaml\n  \n  # Apply override to use
        the cluster range\n  kubectl apply -f /tmp/rook-ceph-override.yaml\n\n  #
        Create cluster\n  sed -i 's/hostNetwork: false$/hostNetwork: true/' /tmp/rook-cluster.yaml\n
        \ sed -i 's/useAllDevices:$/useAllDevices: false/' /tmp/rook-cluster.yaml\n
        \ {{ if .ParamExists \"rook/ceph-target-disk\" }}\n  sed -i 's/deviceFilter:$/deviceFilter:
        {{ .Param \"rook/ceph-target-disk\" }}/' /tmp/rook-cluster.yaml\n  {{ end
        -}}\n  kubectl apply -f /tmp/rook-cluster.yaml\n\n  # Create the storageclass\n
        \ kubectl apply -f /tmp/rook-storageclass.yaml\n\n  # Make storageclass the
        default (could potentially override this in future with a param)\n  kubectl
        patch storageclass rook-ceph-block -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n
        \ \n  # Create the dashboard\n  kubectl apply -f /tmp/rook-ceph-dashboard-ingress.yaml\n\n
        \ # Create the toolbox\n  kubectl apply -f /tmp/rook-ceph-toolbox.yaml\n\n\n
        \ # Clean up\n  mkdir -p /tmp/cleanup\n  mv /tmp/*.yaml /tmp/cleanup\n\nelse\n
        \ echo \"I was not the leader, skipping rook-ceph install\"\nfi\n\n\n\necho
        \"Finished rook-ceph deployment successfully\"\n\necho \"Finished successfully\"\nexit
        0\n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-rook-ceph.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-runtime-install.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash

        {{template "setup.tmpl" .}}

        cr={{.Param "krib/container-runtime"}}
        case $cr in
          docker) tasks="docker-install";;
          containerd) tasks="containerd-install";;
          *) echo "No idea what to do with $cr"; exit 1;;
        esac

        drpcli machines tasks add {{.Machine.UUID}} at 0 $tasks
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-runtime-install.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-settings.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # Kubernetes Rebar Immutable Boot (KRIB) Kubeadm Installer
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        {{if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing krib/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}
        export RS_UUID="{{.Machine.UUID}}"

        echo "Start all the cluster services."

        MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
        MASTER_COUNT={{.Param "krib/cluster-master-count"}}
        MASTER_VIP={{.Param "krib/cluster-master-vip"}}

        if [[ $MASTER_INDEX == 0 ]] ; then
          export KUBECONFIG=/etc/kubernetes/admin.conf

          {{ if .ParamExists "krib/return-nxdomain-for" }}
          # Let's fix the possibility of a broken coredns on alpine..
          # This is a convoluted way to add a single domain to a rewrite
          # plugin which will return NXDOMAIN for searches for the current searchdomain
          kubectl get configmaps coredns -n kube-system -o yaml > /tmp/coredns.yaml
          SEARCH_DOMAIN={{.Param "krib/return-nxdomain-for" }}
          sed -i '/prometheus/ a \        rewrite name suffix banana cluster.local' /tmp/coredns.yaml
          sed -i "s/banana/$SEARCH_DOMAIN/" /tmp/coredns.yaml
          kubectl delete configmap coredns -n kube-system
          kubectl create -f /tmp/coredns.yaml
          kubectl scale --replicas=0 -n kube-system deployment/coredns
          {{ end }}

          echo "Up the DNS replica count"
          kubectl scale --replicas=$MASTER_COUNT -n kube-system deployment/coredns

          API_PORT={{.Param "krib/cluster-api-port"}}
          if [[ $MASTER_COUNT -gt 1 ]] ; then
            API_PORT={{.Param "krib/cluster-api-vip-port"}}
          fi
          echo "Point the Kube-Proxy at $API_PORT"
          kubectl get configmap -n kube-system kube-proxy -o yaml >/tmp/kube-proxy-cm.yaml
          sed -i "s/server: *https:\/\/.*:{{.Param "krib/cluster-api-port"}}/server: https:\/\/${MASTER_VIP}:${API_PORT}/" /tmp/kube-proxy-cm.yaml
          kubectl apply -f /tmp/kube-proxy-cm.yaml --force
          rm -f /tmp/kube-proxy-cm.yaml
          kubectl delete pod -n kube-system -l k8s-app=kube-proxy

          drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"blue\", \"icon\": \"anchor\"} }" | jq .Meta
        fi

        echo "Finished successfully"
        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-settings.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-skip-if-not-master.tmpl:
      Available: false
      Bundle: ""
      Contents: "#### BEGIN krib-skip-if-not-master.tmpl\n# This snippet skips the
        remainder of the calling template, if \n# selective mastership is enabled,
        but the current machine is not\n# selected to be a master. This template is
        called from etcd-config.sh.tmpl _and_\n# from krib-get-masters.sh.tmpl, so
        it makes sense to decompose the contents\n# into this single, re-usable template\n\n{{if
        .ParamExists \"krib/selective-mastership\" -}}\nSELECTIVE_MASTERSHIP={{.Param
        \"krib/selective-mastership\" }}\n{{end -}}\n{{if .ParamExists \"krib/i-am-master\"
        -}}\nI_AM_MASTER={{.Param \"krib/i-am-master\" }}\n{{end -}}\nif [[ \"$SELECTIVE_MASTERSHIP\"
        == true ]] ; then\n  if [[ ! \"$I_AM_MASTER\" == true ]] ; then\n    echo
        \"krib/selective-mastership is true but krib/i-am-master is not. I am denied
        mastership, so move on\"\n    exit 0\n  fi\nfi\n\n#### END krib-skip-if-not-master.tmpl"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-skip-if-not-master.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-sonobuoy.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |+
        #!/usr/bin/env bash
        # Kubernetes Rebar Integrated Boot (KRIB) Kubeadm Installer
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        echo "Configure Sonobuoy on the master (skip for minions)..."

        {{if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing krib/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
        echo "My Master index is $MASTER_INDEX"
        if [[ $MASTER_INDEX != notme ]] ; then

          if [[ $MASTER_INDEX == 0 ]] ; then

            echo "I am the elected leader - install and run sonobuoy"

            # help requires the admin config
            export KUBECONFIG="/etc/kubernetes/admin.conf"
            wget -q -O "sonobuoy.tar.gz" {{.Param "sonobuoy/binary"}}
            tar --extract --file=sonobuoy.tar.gz sonobuoy
            echo "starting sonobuoy run - will fail if already started, that OK"
            ./sonobuoy run || true

            # needs some time to resolve before we can check status
            set +e
            echo "waiting for sonobuoy namespace to be active..."
            CNT=$(kubectl get ns | egrep "^heptio-sonobuoy(\s*)Active")
            ESCAPE=30
            while [[ -z $CNT && $ESCAPE -gt 0 ]] ; do
              echo "Namespace $ESCAPE: 2s loop for sonobuoy namespace..."
              sleep 2
              CNT=$(kubectl get ns | egrep "^heptio-sonobuoy(\s*)Active")
              ((ESCAPE=ESCAPE-1))
            done
            echo "sonobuoy namespace active"

            sleep 5
            echo "checking status (first time)"
            ./sonobuoy status

            CNT=$(./sonobuoy status | egrep "^e2e(\s*)complete")
            if [[ {{.Param "sonobuoy/wait-mins"}} -gt 0 ]] ; then
              echo "Wait up to {{.Param "sonobuoy/wait-mins"}} minutes until sonobuoy is complete"
              ESCAPE={{.Param "sonobuoy/wait-mins" | mul 4}}
              while [[ -z $CNT && $ESCAPE -gt 0 ]] ; do
                echo "Iteration $ESCAPE: 15s loop for sonobuoy status...$(./sonobuoy status)"
                sleep 15
                CNT=$(./sonobuoy status | egrep "^e2e(\s*)complete")
                ((ESCAPE=ESCAPE-1))
              done
              echo "exiting loop - $ESCAPE iterations remain"
            else
              echo "skipping wait loop (sonobuoy/wait-mins < 0)"
            fi
            set -e


            # REMOVED FOR NOW - status hangs the job
            #echo "!!!!!!!!!!!!!!!!!!!!!! start copy the logs into DRP log !!!!!!!!!!!!!!!!!!!!!!!!!"
            #./sonobuoy logs
            #echo "!!!!!!!!!!!!!!!!!!!!!! end copy the logs into DRP log !!!!!!!!!!!!!!!!!!!!!!!!!!!"

            echo "checking status (last time)"
            ./sonobuoy status

            if [[ ! -z $CNT ]] ; then
              echo "Retrieving Results"
              ./sonobuoy retrieve .
              OUTPUT=$(ls | grep _sonobuoy_)
              {{if .ParamExists "unsafe/rs-password"}}
                PASSWORD="{{.Param "unsafe/rs-password"}}"
              {{else}}
                PASSWORD="r0cketsk8ts"
              {{end}}
              echo "attempting to upload $OUTPUT results DRP/files (required admin access)"
              HOLD_TOKEN=$RS_TOKEN
              unset RS_TOKEN
              drpcli -U "rocketskates" -P "$PASSWORD" files upload $OUTPUT as $OUTPUT
              export RS_TOKEN=$HOLD_TOKEN
              echo "success: removing sonobuoy"
              ./sonobuoy delete
            else
              echo "Sonobuoy is not complete"
              if [[ {{.Param "sonobuoy/wait-mins"}} -lt 0 ]] ; then
                echo "Check back latter...stage is idempotent"
                exit 0
              else
                echo "ERROR: Wait time ({{.Param "sonobuoy/wait-mins"}} mins) expired."
                exit 1
              fi
            fi

          else

            echo "I was not the leader, skipping helm init"

          fi

        else

          echo "I am a worker - no helm actions"

        fi

        echo "Finished successfully"
        exit 0

      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-sonobuoy.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    krib-vault-kms-plugin.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # This script installs vault, but doesn't configure it

        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        # Skip the remainder of this template if this host is not a master in a selective-master deployment
        {{template "krib-skip-if-not-master.tmpl" .}}

        # these need to be before krib-lib template
        {{if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing krib/cluster-profile on the machine!"
        {{end -}}
        {{template "krib-lib.sh.tmpl" .}}


        # Allow for a local repository for installation files
        {{if .ParamExists "krib/package-repository" -}}
        KRIB_REPO={{.Param "krib/package-repository"}}
        {{end -}}

        TMP_DIR=/tmp/kms-plugin
        INSTALL_DIR=/usr/local/bin
        mkdir -p ${TMP_DIR}

        echo "Download oracle kubernetes-vault-kms-plugin master.."
        if [[ ! -z "$KRIB_REPO" ]] ; then
          download -L ${KRIB_REPO}/kubernetes-vault-kms-plugin_0.0.7_Linux_x86_64.tar.gz -o $TMP_DIR/kubernetes-vault-kms-plugin_0.0.7_Linux_x86_64.tar.gz
        else
          download -L https://github.com/funkypenguin/kubernetes-vault-kms-plugin/releases/download/v0.0.7/kubernetes-vault-kms-plugin_0.0.7_Linux_x86_64.tar.gz -o ${TMP_DIR}/kubernetes-vault-kms-plugin_0.0.7_Linux_x86_64.tar.gz
        fi


        echo "Installing kubernete-vault-kms-plugin..."
        tar -C ${INSTALL_DIR} -xzf ${TMP_DIR}/kubernetes-vault-kms-plugin_0.0.7_Linux_x86_64.tar.gz --strip-components=1

        # We should secure the encryption.yaml file for root-only read
        chmod 600 /etc/kubernetes/pki/encryption.yaml

        {{if .ParamExists "vault/kms-plugin-token" }}
        TOKEN={{ .Param "vault/kms-plugin-token" }}
        {{else -}}
        TOKEN=setmemanually
        {{end -}}

        # Create the plugin config (requires a param for token)
        cat <<EOF > /etc/vault/vault-kms-plugin.yaml
        keyNames:
          - kube-secret-enc-key
        transitPath: /transit
        vaultCACert: /etc/vault/pki/server-ca.pem
        addr: https://localhost:8200
        token: $TOKEN
        EOF

        # Ensure that /etc/kubernetes/pki/vault-kms-plugin/ exists and is writeable by the vault user
        mkdir -p /etc/kubernetes/pki/vault-kms-plugin
        chown vault /etc/kubernetes/pki/vault-kms-plugin

        systemctl daemon-reload
        systemctl enable vault-kms-plugin
        systemctl restart vault-kms-plugin
        systemctl status vault-kms-plugin

        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: krib-vault-kms-plugin.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    kubernetes-install.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # Kubernetes and Kubeadm Install Content
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}
        [[ $RS_UUID ]] && export RS_UUID="{{.Machine.UUID}}"

        CLUSTER_NAME={{.Param "krib/cluster-name"}}
        {{if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing krib/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        CNI_VERSION="{{ .Param "krib/cluster-cni-version" }}"
        CRICTL_VERSION="{{ .Param "krib/cluster-crictl-version" }}"
        RELEASE="{{ .Param "krib/cluster-kubernetes-version" }}"

        TMP_DIR=/tmp/k8s-tmp
        INSTALL_DIR=/usr/bin

        echo "Install prerequisite: socat"
        case $OS_FAMILY in
          debian)
            if ! which socat ; then
              install socat
            fi
            ;;
          rhel)
            # Fix kernel config ref for kubeadm if missing
            if [[ ! -e /boot/config-$(uname -r) ]] ; then
              yum install -y kernel-devel
              config_file=$(find /usr/src/kernels -type f | grep "\/\.config" | tail -1)
              ln -s $config_file /boot/config-$(uname -r)
            fi

            if ! which socat ; then
              install socat
            fi

            # Fix hostname lookup
            echo "{{.Machine.Address}} $(hostname -s) $(hostname)" >> /etc/hosts

            if sestatus | grep -q disabled ; then
                echo "SELinux already disabled"
            else
                setenforce 0
            fi
            ;;
          coreos)
            # Fix hostname lookup
            echo "{{.Machine.Address}} $(hostname -s) $(hostname)" >> /etc/hosts

            INSTALL_DIR=/opt/bin
            ;;
        esac

        mkdir -p ${TMP_DIR}
        mkdir -p ${INSTALL_DIR}
        mkdir -p /opt/cni/bin

        # Allow for a local repository for installation files
        {{if .ParamExists "krib/package-repository" -}}
        KRIB_REPO={{.Param "krib/package-repository"}}
        {{end -}}

        echo "Download cni plugin version: ${CNI_VERSION}"
        if [[ ! -z "$KRIB_REPO" ]] ; then
          download -L "${KRIB_REPO}/cni-plugins-linux-amd64-${CNI_VERSION}.tgz" -o ${TMP_DIR}/cni-plugins-amd64-${CNI_VERSION}.tgz
        else
          download -L "https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-linux-amd64-${CNI_VERSION}.tgz" -o ${TMP_DIR}/cni-plugins-amd64-${CNI_VERSION}.tgz
        fi
        echo "Install cni plugin version: ${CNI_VERSION}"
        tar -C /opt/cni/bin -xzf ${TMP_DIR}/cni-plugins-amd64-${CNI_VERSION}.tgz

        echo "Download crictl version: ${CRICTL_VERSION}"
        if [[ ! -z "$KRIB_REPO" ]] ; then
          download -L "${KRIB_REPO}/crictl-${CRICTL_VERSION}-linux-amd64.tar.gz" -o ${TMP_DIR}/crictl-${CRICTL_VERSION}-linux-amd64.tar.gz
        else
          download -L "https://github.com/kubernetes-incubator/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-amd64.tar.gz" -o ${TMP_DIR}/crictl-${CRICTL_VERSION}-linux-amd64.tar.gz
        fi
        echo "Install crictl version: ${CRICTL_VERSION}"
        tar -C ${INSTALL_DIR} -xzf ${TMP_DIR}/crictl-${CRICTL_VERSION}-linux-amd64.tar.gz

        cd ${INSTALL_DIR}
        echo "Download kubelet, kubeadm, and kubectl version: ${RELEASE}"
        if [[ ! -z "$KRIB_REPO" ]] ; then
          download -L --remote-name-all "${KRIB_REPO}/{kubeadm,kubelet,kubectl}"
        else
          download -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/${RELEASE}/bin/linux/amd64/{kubeadm,kubelet,kubectl}
        fi
        chmod +x {kubeadm,kubelet,kubectl}

        echo "Download kubelet systemd service unit file for version: ${RELEASE}"
        download -L "https://raw.githubusercontent.com/kubernetes/kubernetes/${RELEASE}/build/debs/kubelet.service" -o /etc/systemd/system/kubelet.service
        sed -i "s:/usr/bin:$INSTALL_DIR:g" /etc/systemd/system/kubelet.service

        mkdir -p /etc/systemd/system/kubelet.service.d
        echo "Download kubelet systemd service extension unit file for version: ${RELEASE}"
        download -L "https://raw.githubusercontent.com/kubernetes/kubernetes/${RELEASE}/build/debs/10-kubeadm.conf" -o /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
        sed -i "s:/usr/bin:$INSTALL_DIR:g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

        systemctl enable kubelet && systemctl start kubelet

        rm -rf ${TMP_DIR}

        echo "Finished successfully"
        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: kubernetes-install.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    kubevirt.cfg.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        # KubeVirt v0.10.0 add-on
        # From https://github.com/kubevirt/kubevirt/releases
        # Monitoring
        apiVersion: v1
        kind: Service
        metadata:
          name: kubevirt-prometheus-metrics
          namespace: kube-system
          labels:
            prometheus.kubevirt.io: ""
            kubevirt.io: ""
        spec:
          ports:
            - name: metrics
              port: 443
              targetPort: metrics
              protocol: TCP
          selector:
            prometheus.kubevirt.io: ""
        ---
        # RBAC
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRole
        metadata:
          name: kubevirt.io:admin
          labels:
            kubevirt.io: ""
            rbac.authorization.k8s.io/aggregate-to-admin: "true"
        rules:
          - apiGroups:
              - subresources.kubevirt.io
            resources:
              - virtualmachineinstances/console
              - virtualmachineinstances/vnc
            verbs:
              - get
          - apiGroups:
              - kubevirt.io
            resources:
              - virtualmachineinstances
              - virtualmachines
              - virtualmachineinstancepresets
              - virtualmachineinstancereplicasets
            verbs:
              - get
              - delete
              - create
              - update
              - patch
              - list
              - watch
              - deletecollection
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRole
        metadata:
          name: kubevirt.io:edit
          labels:
            kubevirt.io: ""
            rbac.authorization.k8s.io/aggregate-to-edit: "true"
        rules:
          - apiGroups:
              - subresources.kubevirt.io
            resources:
              - virtualmachineinstances/console
              - virtualmachineinstances/vnc
            verbs:
              - get
          - apiGroups:
              - kubevirt.io
            resources:
              - virtualmachineinstances
              - virtualmachines
              - virtualmachineinstancepresets
              - virtualmachineinstancereplicasets
            verbs:
              - get
              - delete
              - create
              - update
              - patch
              - list
              - watch
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRole
        metadata:
          name: kubevirt.io:view
          labels:
            kubevirt.io: ""
            rbac.authorization.k8s.io/aggregate-to-view: "true"
        rules:
          - apiGroups:
              - kubevirt.io
            resources:
              - virtualmachineinstances
              - virtualmachines
              - virtualmachineinstancepresets
              - virtualmachineinstancereplicasets
            verbs:
              - get
              - list
              - watch
        ---
        kind: ServiceAccount
        apiVersion: v1
        metadata:
          name: kubevirt-apiserver
          namespace: kube-system
          labels:
            kubevirt.io: ""
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRoleBinding
        metadata:
          name: kubevirt-apiserver
          namespace: kube-system
          labels:
            kubevirt.io: ""
        roleRef:
          kind: ClusterRole
          name: kubevirt-apiserver
          apiGroup: rbac.authorization.k8s.io
        subjects:
          - kind: ServiceAccount
            name: kubevirt-apiserver
            namespace: kube-system
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRoleBinding
        metadata:
          name: kubevirt-apiserver-auth-delegator
          namespace: kube-system
          labels:
            kubevirt.io: ""
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: system:auth-delegator
        subjects:
        - kind: ServiceAccount
          name: kubevirt-apiserver
          namespace: kube-system
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: RoleBinding
        metadata:
          name: kubevirt-apiserver
          namespace: kube-system
          labels:
            kubevirt.io: ""
        roleRef:
          kind: Role
          name: kubevirt-apiserver
          apiGroup: rbac.authorization.k8s.io
        subjects:
          - kind: ServiceAccount
            name: kubevirt-apiserver
            namespace: kube-system
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: Role
        metadata:
          name: kubevirt-apiserver
          namespace: kube-system
          labels:
            kubevirt.io: ""
        rules:
          - apiGroups:
              - ''
            resources:
              - secrets
            verbs:
              - get
              - list
              - delete
              - update
              - create
          - apiGroups:
              - ''
            resources:
              - configmaps
            verbs:
              - get
              - list
              - watch
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRole
        metadata:
          name: kubevirt-apiserver
          namespace: kube-system
          labels:
            kubevirt.io: ""
        rules:
          - apiGroups:
              - admissionregistration.k8s.io
            resources:
              - validatingwebhookconfigurations
              - mutatingwebhookconfigurations
            verbs:
              - get
              - create
              - update
          - apiGroups:
              - apiregistration.k8s.io
            resources:
              - apiservices
            verbs:
              - get
              - create
              - update
          - apiGroups:
              - ''
            resources:
              - pods
            verbs:
              - get
              - list
          - apiGroups:
              - ''
            resources:
              - configmaps
            resourceNames:
              - extension-apiserver-authentication
            verbs:
              - get
              - list
              - watch
          - apiGroups:
              - ''
            resources:
              - pods/exec
            verbs:
              - create
          - apiGroups:
              - kubevirt.io
            resources:
              - virtualmachineinstances
              - virtualmachineinstancemigrations
            verbs:
              - get
              - list
              - watch
          - apiGroups:
              - ''
            resources:
              - limitranges
            verbs:
              - watch
              - list
          - apiGroups:
              - kubevirt.io
            resources:
              - virtualmachineinstancepresets
            verbs:
              - watch
              - list
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRole
        metadata:
          name: kubevirt-controller
          namespace: kube-system
          labels:
            kubevirt.io: ""
        rules:
          - apiGroups:
              - ''
            resources:
              - pods
              - configmaps
              - endpoints
            verbs:
              - get
              - list
              - watch
              - delete
              - update
              - create
          - apiGroups:
              - ''
            resources:
              - events
            verbs:
              - update
              - create
              - patch
          - apiGroups:
              - ''
            resources:
              - nodes
            verbs:
              - get
              - list
              - watch
              - update
              - patch
          - apiGroups:
              - ''
            resources:
              - persistentvolumeclaims
            verbs:
              - get
              - list
              - watch
          - apiGroups:
              - kubevirt.io
            resources:
              - '*'
            verbs:
              - '*'
          - apiGroups:
              - cdi.kubevirt.io
            resources:
              - '*'
            verbs:
              - '*'
        ---
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: kubevirt-controller
          namespace: kube-system
          labels:
            kubevirt.io: ""
        ---
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: kubevirt-privileged
          namespace: kube-system
          labels:
            kubevirt.io: ""
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRoleBinding
        metadata:
          name: kubevirt-controller
          namespace: kube-system
          labels:
            kubevirt.io: ""
        roleRef:
          kind: ClusterRole
          name: kubevirt-controller
          apiGroup: rbac.authorization.k8s.io
        subjects:
          - kind: ServiceAccount
            name: kubevirt-controller
            namespace: kube-system
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRoleBinding
        metadata:
          name: kubevirt-privileged-cluster-admin
          namespace: kube-system
          labels:
            kubevirt.io: ""
        roleRef:
          kind: ClusterRole
          name: cluster-admin
          apiGroup: rbac.authorization.k8s.io
        subjects:
          - kind: ServiceAccount
            name: kubevirt-privileged
            namespace: kube-system
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          annotations:
            rbac.authorization.kubernetes.io/autoupdate: "true"
          labels:
            kubevirt.io: ""
            kubernetes.io/bootstrapping: rbac-defaults
          name: kubevirt.io:default
        rules:
        - apiGroups:
            - subresources.kubevirt.io
          resources:
            - version
          verbs:
            - get
            - list
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          labels:
            kubevirt.io: ""
          annotations:
            rbac.authorization.kubernetes.io/autoupdate: "true"
          name: kubevirt.io:default
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: kubevirt.io:default
        subjects:
        - apiGroup: rbac.authorization.k8s.io
          kind: Group
          name: system:authenticated
        - apiGroup: rbac.authorization.k8s.io
          kind: Group
          name: system:unauthenticated
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: virt-api
          namespace: kube-system
          labels:
            kubevirt.io: "virt-api"
        spec:
          ports:
            - port: 443
              targetPort: 8443
              protocol: TCP
          selector:
            kubevirt.io: virt-api
        ---
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: virt-api
          namespace: kube-system
          labels:
            kubevirt.io: "virt-api"
        spec:
          replicas: 2
          template:
            metadata:
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ""
                scheduler.alpha.kubernetes.io/tolerations: |
                          [
                            {
                              "key": "CriticalAddonsOnly",
                              "operator": "Exists"
                            }
                          ]
              labels:
                kubevirt.io: virt-api
                prometheus.kubevirt.io: ""
            spec:
              serviceAccountName: kubevirt-apiserver
              containers:
                - name: virt-api
                  image: docker.io/kubevirt/virt-api:v0.10.0
                  imagePullPolicy: IfNotPresent
                  command:
                      - "virt-api"
                      - "--port"
                      - "8443"
                      - "--subresources-only"
                  ports:
                    - containerPort: 8443
                      name: "virt-api"
                      protocol: "TCP"
                    - containerPort: 8443
                      name: "metrics"
                      protocol: "TCP"
                  readinessProbe:
                    tcpSocket:
                      port: 8443
                    initialDelaySeconds: 5
                    periodSeconds: 10
              securityContext:
                runAsNonRoot: true
        ---
        # kubevirt controller
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: virt-controller
          namespace: kube-system
          labels:
            kubevirt.io: "virt-controller"
        spec:
          replicas: 2
          template:
            metadata:
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ""
                scheduler.alpha.kubernetes.io/tolerations: |
                          [
                            {
                              "key": "CriticalAddonsOnly",
                              "operator": "Exists"
                            }
                          ]
              labels:
                kubevirt.io: virt-controller
                prometheus.kubevirt.io: ""
            spec:
              serviceAccountName: kubevirt-controller
              containers:
                - name: virt-controller
                  image: docker.io/kubevirt/virt-controller:v0.10.0
                  imagePullPolicy: IfNotPresent
                  command:
                      - "virt-controller"
                      - "--launcher-image"
                      - "docker.io/kubevirt/virt-launcher:v0.10.0"
                      - "--port"
                      - "8443"
                  ports:
                    - containerPort: 8443
                      name: "metrics"
                      protocol: "TCP"
                  livenessProbe:
                    failureThreshold: 8
                    httpGet:
                      scheme: HTTPS
                      port: 8443
                      path: /healthz
                    initialDelaySeconds: 15
                    timeoutSeconds: 10
                  readinessProbe:
                    httpGet:
                      scheme: HTTPS
                      port: 8443
                      path: /leader
                    initialDelaySeconds: 15
                    timeoutSeconds: 10
                  securityContext:
                    runAsNonRoot: true
        ---
        # virt-handler daemon set
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: virt-handler
          namespace: kube-system
          labels:
            kubevirt.io: "virt-handler"
        spec:
          updateStrategy:
            type: RollingUpdate
          template:
            metadata:
              name: virt-handler
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ""
                scheduler.alpha.kubernetes.io/tolerations: |
                          [
                            {
                              "key": "CriticalAddonsOnly",
                              "operator": "Exists"
                            }
                          ]
              labels:
                kubevirt.io: virt-handler
                prometheus.kubevirt.io: ""
            spec:
              serviceAccountName: kubevirt-privileged
              hostPID: true
              containers:
                - name: virt-handler
                  ports:
                    - containerPort: 8443
                      name: "metrics"
                      protocol: "TCP"
                  image: docker.io/kubevirt/virt-handler:v0.10.0
                  imagePullPolicy: IfNotPresent
                  command:
                    - "virt-handler"
                    - "-v"
                    - "3"
                    -  "--port"
                    - "8443"
                    - "--hostname-override"
                    - "$(NODE_NAME)"
                    - "--pod-ip-address"
                    - "$(MY_POD_IP)"
                  securityContext:
                    privileged: true
                  volumeMounts:
                    - name: libvirt-runtimes
                      mountPath: /var/run/kubevirt-libvirt-runtimes
                    - name: virt-share-dir
                      mountPath: /var/run/kubevirt
                    - name: virt-private-dir
                      mountPath: /var/run/kubevirt-private
                    - name: device-plugin
                      mountPath: /var/lib/kubelet/device-plugins
                  env:
                    - name: NODE_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: spec.nodeName
                    - name: MY_POD_IP
                      valueFrom:
                        fieldRef:
                          fieldPath: status.podIP
              volumes:
                - name: libvirt-runtimes
                  hostPath:
                    path: /var/run/kubevirt-libvirt-runtimes
                - name: virt-share-dir
                  hostPath:
                    path: /var/run/kubevirt
                - name: virt-private-dir
                  hostPath:
                    path: /var/run/kubevirt-private
                - name: device-plugin
                  hostPath:
                    path: /var/lib/kubelet/device-plugins
        ---
        apiVersion: apiextensions.k8s.io/v1beta1
        kind: CustomResourceDefinition
        metadata:
          creationTimestamp: null
          labels:
            kubevirt.io: ""
          name: virtualmachineinstances.kubevirt.io
        spec:
          additionalPrinterColumns:
          - JSONPath: .metadata.creationTimestamp
            name: Age
            type: date
          - JSONPath: .status.phase
            name: Phase
            type: string
          - JSONPath: .status.interfaces[0].ipAddress
            name: IP
            type: string
          - JSONPath: .status.nodeName
            name: NodeName
            type: string
          group: kubevirt.io
          names:
            kind: VirtualMachineInstance
            plural: virtualmachineinstances
            shortNames:
            - vmi
            - vmis
            singular: virtualmachineinstance
          scope: Namespaced
          version: v1alpha2

        ---
        apiVersion: apiextensions.k8s.io/v1beta1
        kind: CustomResourceDefinition
        metadata:
          creationTimestamp: null
          labels:
            kubevirt.io: ""
          name: virtualmachineinstancereplicasets.kubevirt.io
        spec:
          additionalPrinterColumns:
          - JSONPath: .spec.replicas
            description: Number of desired VirtualMachineInstances
            name: Desired
            type: integer
          - JSONPath: .status.replicas
            description: Number of managed and not final or deleted VirtualMachineInstances
            name: Current
            type: integer
          - JSONPath: .status.readyReplicas
            description: Number of managed VirtualMachineInstances which are ready to receive
              traffic
            name: Ready
            type: integer
          - JSONPath: .metadata.creationTimestamp
            name: Age
            type: date
          group: kubevirt.io
          names:
            kind: VirtualMachineInstanceReplicaSet
            plural: virtualmachineinstancereplicasets
            shortNames:
            - vmirs
            - vmirss
            singular: virtualmachineinstancereplicaset
          scope: Namespaced
          version: v1alpha2

        ---
        apiVersion: apiextensions.k8s.io/v1beta1
        kind: CustomResourceDefinition
        metadata:
          creationTimestamp: null
          labels:
            kubevirt.io: ""
          name: virtualmachineinstancepresets.kubevirt.io
        spec:
          group: kubevirt.io
          names:
            kind: VirtualMachineInstancePreset
            plural: virtualmachineinstancepresets
            shortNames:
            - vmipreset
            - vmipresets
            singular: virtualmachineinstancepreset
          scope: Namespaced
          version: v1alpha2

        ---
        apiVersion: apiextensions.k8s.io/v1beta1
        kind: CustomResourceDefinition
        metadata:
          creationTimestamp: null
          labels:
            kubevirt.io: ""
          name: virtualmachines.kubevirt.io
        spec:
          additionalPrinterColumns:
          - JSONPath: .metadata.creationTimestamp
            name: Age
            type: date
          - JSONPath: .spec.running
            name: Running
            type: boolean
          - JSONPath: .spec.volumes[0].name
            description: Primary Volume
            name: Volume
            type: string
          group: kubevirt.io
          names:
            kind: VirtualMachine
            plural: virtualmachines
            shortNames:
            - vm
            - vms
            singular: virtualmachine
          scope: Namespaced
          version: v1alpha2

        ---
        apiVersion: apiextensions.k8s.io/v1beta1
        kind: CustomResourceDefinition
        metadata:
          creationTimestamp: null
          labels:
            kubevirt.io: ""
          name: virtualmachineinstancemigrations.kubevirt.io
        spec:
          group: kubevirt.io
          names:
            kind: VirtualMachineInstanceMigration
            plural: virtualmachineinstancemigrations
            shortNames:
            - vmim
            - vmims
            singular: virtualmachineinstancemigration
          scope: Namespaced
          version: v1alpha2
      Description: ""
      Endpoint: ""
      Errors: []
      ID: kubevirt.cfg.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    logging-fluent-bit.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # Kubernetes Rebar Integrated Boot (KRIB) Kubeadm Installer
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        echo "Configure fluent-bit logging the master (skip for minions)..."

        {{if .ParamExists "krib/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "krib/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "krib/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing krib/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        MASTER_INDEX=$(find_me $KRIB_MASTERS_PARAM "Uuid" $RS_UUID)
        echo "My Master index is $MASTER_INDEX"
        if [[ $MASTER_INDEX != notme ]] ; then

          if [[ $MASTER_INDEX == 0 ]] ; then

            echo "I am the elected leader - install fluent-bit and helm init"

            # needed to apply manifests
            export KUBECONFIG="/etc/kubernetes/admin.conf"

            ##### Optionally replace containers in distributed YAML manifests with operator-specified
            ##### containers from a private registry
            {{ if .ParamExists "krib/fluent-bit-container-image" }}
            # Replace default container image with the one specified in "krib/fluent-bit-container-image"
            sed -i 's|fluent/fluent-bit|{{ .Param "krib/fluent-bit-container-image" }}|' /tmp/logging-fluent-bit.yaml
            {{ end -}}

            echo "Applying fluent-bit manifests"
            kubectl apply -f /tmp/logging-fluent-bit.yaml
            mkdir -p /tmp/cleanup
            mv /tmp/logging-fluent-bit.yaml /tmp/cleanup

          else

            echo "I was not the leader, skipping fluent-bit init"

          fi

        else

          echo "I am a worker - no fluent-bit actions"

        fi

        echo "Finished successfully"
        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: logging-fluent-bit.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    logging-fluent-bit.yaml.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        apiVersion: v1
        kind: Namespace
        metadata:
          name: logging
        ---
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: fluent-bit
          namespace: logging
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRole
        metadata:
          name: fluent-bit-read
        rules:
        - apiGroups: [""]
          resources:
          - namespaces
          - pods
          verbs: ["get", "list", "watch"]
        ---
        apiVersion: rbac.authorization.k8s.io/v1beta1
        kind: ClusterRoleBinding
        metadata:
          name: fluent-bit-read
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: fluent-bit-read
        subjects:
        - kind: ServiceAccount
          name: fluent-bit
          namespace: logging
        ---
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: fluent-bit-config
          namespace: logging
          labels:
            k8s-app: fluent-bit
        data:
          # Configuration files: server, input, filters and output
          # ======================================================
          fluent-bit.conf: |
            [SERVICE]
                Flush         1
                Log_Level     info
                Daemon        off
                Parsers_File  parsers.conf
                HTTP_Server   On
                HTTP_Listen   0.0.0.0
                HTTP_Port     2020

            @INCLUDE input-kubernetes.conf
            @INCLUDE filter-kubernetes.conf
            @INCLUDE output-graylog.conf

          input-kubernetes.conf: |
            [INPUT]
                Name              tail
                Tag               kube.*
                Path              /var/log/containers/*.log
                Parser            docker
                DB                /var/log/flb_kube.db
                Mem_Buf_Limit     5MB
                Skip_Long_Lines   On
                Refresh_Interval  10

          filter-kubernetes.conf: |
            [FILTER]
                Name                kubernetes
                Match               kube.*
                Kube_URL            https://kubernetes.default.svc.cluster.local:443
                Merge_Log           On
                K8S-Logging.Parser  On

            # ${HOSTNAME} returns the host name.
            # But Fluentbit runs in a container. So, it is not meaningful.
            # Instead, copy the host name from the Kubernetes object.
            [FILTER]
                Name nest
                Match *
                Operation lift
                Nested_under kubernetes

            # Remove offending fields, see: https://github.com/fluent/fluent-bit/issues/1291
            [FILTER]
                Name record_modifier
                Match *
                Remove_key annotations
                Remove_key labels

          output-graylog.conf: |
            [OUTPUT]
                Name          gelf
                Match         *
                Host          {{ .Param "krib/log-target-gelf" }}
                Port          12201
                Mode          udp
                Gelf_Short_Message_Key log

          parsers.conf: |
            [PARSER]
                Name   json
                Format json
                Time_Key time
                Time_Format %d/%b/%Y:%H:%M:%S %z

            [PARSER]
                Name        docker
                Format      json
                Time_Key    time
                Time_Format %Y-%m-%dT%H:%M:%S.%L
                # Command      |  Decoder | Field | Optional Action
                # =============|==================|=================
                Decode_Field_As   escaped    log

            [PARSER]
                Name        syslog
                Format      regex
                Regex       ^\<(?<pri>[0-9]+)\>(?<time>[^ ]* {1,2}[^ ]* [^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? *(?<message>.*)$
                Time_Key    time
                Time_Format %b %d %H:%M:%S
        ---
        apiVersion: extensions/v1beta1
        kind: DaemonSet
        metadata:
          name: fluent-bit
          namespace: logging
          labels:
            k8s-app: fluent-bit-logging
            version: v1
            kubernetes.io/cluster-service: "true"
        spec:
          template:
            metadata:
              labels:
                k8s-app: fluent-bit-logging
                version: v1
                kubernetes.io/cluster-service: "true"
              annotations:
                prometheus.io/scrape: "true"
                prometheus.io/port: "2020"
                prometheus.io/path: /api/v1/metrics/prometheus
            spec:
              containers:
              - name: fluent-bit
                image: fluent/fluent-bit:1.2.1
                imagePullPolicy: Always
                ports:
                  - containerPort: 2020
                volumeMounts:
                - name: varlog
                  mountPath: /var/log
                - name: varlibdockercontainers
                  mountPath: /var/lib/docker/containers
                  readOnly: true
                - name: fluent-bit-config
                  mountPath: /fluent-bit/etc/
                - name: mnt
                  mountPath: /mnt
                  readOnly: true
              terminationGracePeriodSeconds: 10
              volumes:
              - name: varlog
                hostPath:
                  path: /var/log
              - name: varlibdockercontainers
                hostPath:
                  path: /var/lib/docker/containers
              - name: fluent-bit-config
                configMap:
                  name: fluent-bit-config
              - name: mnt
                hostPath:
                  path: /mnt
              serviceAccountName: fluent-bit
              tolerations:
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
      Description: ""
      Endpoint: ""
      Errors: []
      ID: logging-fluent-bit.yaml.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    mount-disks.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |+
        #!/usr/bin/env bash
        # Kubernetes Rebar Immutable Boot (KRIB) Mount Disks
        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}
        [[ $RS_UUID ]] || export RS_UUID="{{.Machine.UUID}}"

        # Set machine icon and color for KRIB cluster building
        drpcli machines update $RS_UUID "{\"Meta\":{\"color\":\"yellow\", \"icon\": \"ship\"}}" | jq .Meta

        # This will test for sledgehammer and if found use the first disk
        # as docker filesystem.

        BE=$(drpcli machines show $RS_UUID | jq -r .BootEnv)
        case $BE in
          sledgehammer)
            ;;
          coreos-*-live)
            ;;
          *)
            echo "Not in valid operating system - do not do anything"
            exit 0
            ;;
        esac

        # Umount disk if already mounted
        export MOUNT=/mnt/hdd

        if grep -qs $MOUNT /proc/mounts; then
          umount /dev/sda1
        else
            echo "/dev/sda1 is not mounted"
        fi

        GETDISK=$(lsblk | grep "disk" | awk '{ print $1 }' | head -1)
        echo "Found /dev/$GETDISK - using ..."

        echo "Making partitions"
        echo "
        n




        w
        " | fdisk /dev/$GETDISK || true

        partprobe

        echo "Make filesystem - xfs - docker likes it"

        mkfs.xfs -f /dev/${GETDISK}1

        echo "Mount filesystem - put it in place, put not permanently"

        mkdir -p /mnt/hdd
        mount /dev/${GETDISK}1 /mnt/hdd
        mkdir -p /mnt/hdd/docker
        mkdir -p /mnt/hdd/kubectl

        echo "Record docker working directory for future users"
        set_param docker/working-dir /mnt/hdd/docker

        echo "Record kubectl working directory for future users"
        set_param kubectl/working-dir /mnt/hdd/kubectl

        echo "Mounted directory for docker and kubectl successful."
        exit 0

      Description: ""
      Endpoint: ""
      Errors: []
      ID: mount-disks.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    rook-ceph-dashboard-ingress.yaml.tmpl:
      Available: false
      Bundle: ""
      Contents: "apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name:
        rook-ceph-mgr-dashboard\n  namespace: rook-ceph\n  annotations:\n    kubernetes.io/ingress.class:
        \"nginx\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n    nginx.ingress.kubernetes.io/server-snippet:
        |\n      proxy_ssl_verify off;\n{{- if .ParamExists \"certmanager/default-issuer-name\"
        }}    \n    certmanager.k8s.io/cluster-issuer: \"{{ .Param \"certmanager/default-issuer-name\"
        }}\"\n{{ end -}}\nspec:\n  tls:\n   - hosts:\n     - ceph.{{ .Param \"certmanager/dns-domain\"
        }}\n     secretName: ceph.{{ .Param \"certmanager/dns-domain\" }}\n  rules:\n
        \ - host: ceph.{{ .Param \"certmanager/dns-domain\" }}\n    http:\n      paths:\n
        \     - path: /\n        backend:\n          serviceName: rook-ceph-mgr-dashboard\n
        \         servicePort: https-dashboard"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: rook-ceph-dashboard-ingress.yaml.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    rook-ceph-override.yaml.tmpl:
      Available: false
      Bundle: ""
      Contents: "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: rook-config-override\n
        \ namespace: rook-ceph\ndata:\n  config: |\n    [global]\n    public network
        = {{ .Param \"rook/ceph-public-network\" }}\n    cluster network = {{ .Param
        \"rook/ceph-cluster-network\" }}\n    public addr = \"\"\n    cluster addr
        = \"\"    \n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: rook-ceph-override.yaml.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    rook-ceph-toolbox.yaml.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: rook-ceph-tools
          namespace: rook-ceph
          labels:
            app: rook-ceph-tools
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: rook-ceph-tools
          template:
            metadata:
              labels:
                app: rook-ceph-tools
            spec:
              dnsPolicy: ClusterFirstWithHostNet
              containers:
              - name: rook-ceph-tools
                image: rook/ceph:master
                command: ["/tini"]
                args: ["-g", "--", "/usr/local/bin/toolbox.sh"]
                imagePullPolicy: IfNotPresent
                env:
                  - name: ROOK_ADMIN_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: rook-ceph-mon
                        key: admin-secret
                securityContext:
                  privileged: true
                volumeMounts:
                  - mountPath: /dev
                    name: dev
                  - mountPath: /sys/bus
                    name: sysbus
                  - mountPath: /lib/modules
                    name: libmodules
                  - name: mon-endpoint-volume
                    mountPath: /etc/rook
              # if hostNetwork: false, the "rbd map" command hangs, see https://github.com/rook/rook/issues/2021
              hostNetwork: true
              volumes:
                - name: dev
                  hostPath:
                    path: /dev
                - name: sysbus
                  hostPath:
                    path: /sys/bus
                - name: libmodules
                  hostPath:
                    path: /lib/modules
                - name: mon-endpoint-volume
                  configMap:
                    name: rook-ceph-mon-endpoints
                    items:
                    - key: data
                      path: mon-endpoints
      Description: ""
      Endpoint: ""
      Errors: []
      ID: rook-ceph-toolbox.yaml.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    vault-config.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: "#!/usr/bin/env bash\n\n\n# Build a vault cluster\nset -e\n\n# Get
        access and who we are.\n{{template \"setup.tmpl\" .}}\n\n# Skip the remainder
        of this template if this host is not a master in a selective-master deployment\n{{template
        \"krib-skip-if-not-master.tmpl\" .}}\n\nexport RS_UUID=\"{{.Machine.UUID}}\"\nexport
        RS_IP=\"{{.Machine.Address}}\"\n\nVAULT_VERSION=\"{{ .Param \"vault/version\"
        }}\"\n\n# these need to be before krib-lib template\n{{if .ParamExists \"vault/cluster-profile\"
        -}}\nCLUSTER_PROFILE={{.Param \"vault/cluster-profile\"}}\nPROFILE_TOKEN={{.GenerateProfileToken
        (.Param \"vault/cluster-profile\") 7200}}\n{{else -}}\nxiterr 1 \"Missing
        vault/cluster-profile on the machine!\"\n{{end -}}\n\n{{template \"krib-lib.sh.tmpl\"
        .}}\n\n# Ensure that the vault systemd service specifies all vault nodes using
        retry-join\nsetup_seal() {\n  echo \"Setting up seal...\"\n\n  if [[ ! `grep
        awskms /etc/vault/vault.hcl` ]]; then\n  {{ if and (.ParamExists \"vault/awskms-region\")
        (.ParamExists \"vault/awskms-access-key\") (.ParamExists \"vault/awskms-secret-key\")
        (.ParamExists \"vault/awskms-kms-key-id\") }}\n    cat <<EOF >> /etc/vault/vault.hcl\n
        \   \nseal \"awskms\" {\n    region     = \"{{ .Param \"vault/awskms-region\"
        }}\"\n    access_key = \"{{ .Param \"vault/awskms-access-key\" }}\"\n    secret_key
        = \"{{ .Param \"vault/awskms-secret-key\" }}\"\n    kms_key_id = \"{{ .Param
        \"vault/awskms-kms-key-id\" }}\"\n}\nEOF\n  {{ else -}}\n    echo \"awskms\"
        specified  as \"vault/seal\", but requisite parameters vault/awskms-region,
        vault/awskms-access-key, vault/awskms-secret-key, or vault/awskms-kms-key-id
        not set. Skipping seal.\n  {{ end -}}\n  fi\n\n}\n\n\necho \"Configure the
        vault cluster\"\n\nsetup_seal\n\nsystemctl daemon-reload\nsystemctl restart
        vault\n\n# Wait around until consul has elected a leader\nuntil consul operator
        raft list-peers | grep -q leader;\ndo \n  SLEEP=$[ ( $RANDOM % 5 ) ]; \ndone\n\n#
        Only initialize vault on the consul leader, to avoid races\nset +e\n$(consul
        operator raft list-peers | grep `hostname -s` | grep -q leader)\nif [[ $?
        -eq 0 ]] ; then\n  set -e\n  # Before attempting to init vault, sleep 5s to
        give it time to wake up\n  sleep 5s\n\n  export VAULT_CACERT=/etc/vault/pki/server-ca.pem
        \n  vault operator init -recovery-shares=1 -recovery-threshold=1 -key-shares=1
        -key-threshold=1 -format json > /tmp/vault.json\n\n  # Get our recovery key
        and token, then delete the local record\n  export VAULT_UNSEAL_KEY=`cat /tmp/vault.json
        | jq -r '[.recovery_keys_b64][0][0]'`\n  export VAULT_TOKEN=`cat /tmp/vault.json
        | jq -r '[.root_token][0]'`\n\n  # Insert recovery key and token into params\n
        \ drpcli -T \"$PROFILE_TOKEN\" profiles add \"$CLUSTER_PROFILE\" param \"vault/unseal-key\"
        to \"$VAULT_UNSEAL_KEY\"\n  drpcli -T \"$PROFILE_TOKEN\" profiles add \"$CLUSTER_PROFILE\"
        param \"vault/root-token\" to \"$VAULT_TOKEN\"\n\n  # enable the transit secret
        backend\n  vault secrets enable transit\n\n  # create a policy for the transit
        backend\n  vault policy write transit-only /etc/vault/policy-transit-only.hcl\n\n
        \ # Create a token for kubernetes\n  echo \"Setting vault/kms-plugin-token\"\n
        \ vault token create -policy transit-only -format json > /tmp/token.json\n
        \ VAULT_KMS_TOKEN=`cat /tmp/token.json | jq -r '[.auth.client_token][0]'`\n
        \ \n  drpcli -T \"$PROFILE_TOKEN\" profiles add \"$CLUSTER_PROFILE\" param
        \"vault/kms-plugin-token\" to \"$VAULT_KMS_TOKEN\"\n\n  # Destroy the evidence\n
        \ mkdir -p /tmp/cleanup\n  mv /tmp/vault.json /tmp/cleanup\n  mv /tmp/token.json
        /tmp/cleanup\nfi\n\nset -e\n\nVAULT_SERVER_COUNT={{.Param \"vault/server-count\"}}\n\necho
        \"Vault installed and setup, let's go\"\nadd_me_if_not_count \"vault/servers-done\"
        $VAULT_SERVER_COUNT $VAULT_IP\n\necho \"Waiting for ${VAULT_SERVER_COUNT}
        servers to complete vault install\"\nwait_for_count \"vault/servers-done\"
        $VAULT_SERVER_COUNT\n\n\nexit 0\n"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: vault-config.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    vault-env.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        #!/bin/bash
        # Set defaults for Vault CLI.
        export VAULT_CACERT=/etc/kubernetes/pki/vault/server-ca.pem
      Description: ""
      Endpoint: ""
      Errors: []
      ID: vault-env.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    vault-install.sh.tmpl:
      Available: false
      Bundle: ""
      Contents: |
        #!/usr/bin/env bash
        # This script installs vault, but doesn't configure it

        set -e

        # Get access and who we are.
        {{template "setup.tmpl" .}}

        # Skip the remainder of this template if this host is not a master in a selective-master deployment
        {{template "krib-skip-if-not-master.tmpl" .}}

        export RS_UUID="{{.Machine.UUID}}"
        export RS_IP="{{.Machine.Address}}"

        VAULT_VERSION="{{ .Param "vault/version" }}"

        # these need to be before krib-lib template
        {{if .ParamExists "vault/cluster-profile" -}}
        CLUSTER_PROFILE={{.Param "vault/cluster-profile"}}
        PROFILE_TOKEN={{.GenerateProfileToken (.Param "vault/cluster-profile") 7200}}
        {{else -}}
        xiterr 1 "Missing vault/cluster-profile on the machine!"
        {{end -}}

        {{template "krib-lib.sh.tmpl" .}}

        build_cert() {
          local profile=$1
          local ca_name=$2
          local ca_pw=$3
          local myname=$4
          local myip=$5

          echo "Generating certificate for ${profile} with my name ${myname} and my IP ${myip}"
          drpcli machines runaction $RS_UUID getca certs/root $ca_name | jq -r . > /etc/vault/pki/${profile}-ca.pem
          drpcli certs csr $ca_name $myname $RS_IP $myip $(hostname) localhost 127.0.0.1 {{if .ParamExists "krib/cluster-master-vip" }}{{ .Param "krib/cluster-master-vip" }}{{end}} > tmp.csr
          drpcli machines runaction $RS_UUID signcert certs/root $ca_name certs/root-pw $ca_pw certs/csr "$(jq .CSR tmp.csr)" certs/profile $profile | jq -r . > /etc/vault/pki/$profile.pem
          jq -r .Key tmp.csr > /etc/vault/pki/$profile-key.pem
          rm tmp.csr
        }


        echo "Prepare the vault cluster"

        VAULT_CLUSTER_NAME={{.Param "vault/name"}}
        {{ if .ParamExists "vault/ip" -}}
        VAULT_IP={{ .Param "vault/ip" }}
        {{ else -}}
        VAULT_IP={{ .Machine.Address }}
        {{ end -}}

        {{if eq (.ParamExists "vault/servers") false -}}
        # add server management params if missing
        echo "Add initial variables to track members."
        drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "vault/servers" to "[]" || true
        {{ end -}}

        {{if eq (.ParamExists "vault/servers-done") false -}}
        drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "vault/servers-done" to "[]" || true
        {{ end -}}

        # Get the number of servers to create
        VAULT_SERVER_COUNT={{.Param "vault/server-count"}}
        echo "Creating $VAULT_SERVER_COUNT servers"

        echo "Electing vault members to cluster profile: $CLUSTER_PROFILE"
        VAULT_INDEX=$(add_me_if_not_count "vault/servers" $VAULT_SERVER_COUNT $VAULT_IP)

        echo "Added myself to cluster profile, my index is ${VAULT_INDEX}"

        if [[ $VAULT_INDEX == notme ]] ; then
          echo "I am not a VAULT server.  Move on."
          wait_for_count "vault/servers-done" $VAULT_SERVER_COUNT
          exit 0
        fi

        SERVER_CA=${VAULT_CLUSTER_NAME}-ca

        EXISTING_INDEX=$(find_me "vault/servers-done" "Uuid" $RS_UUID)
        if [[ $EXISTING_INDEX == notme ]] ; then
          {{if .ParamExists "certs/root" -}}

          echo "Certs plugin detected....setting up CA"
          # If we are INDEX=0, let's setup the root certs for building keys
          if [[ $VAULT_INDEX == "0" ]] ; then
            echo "We are first machine in cluster, setting up the root certs..."
            # Are certs built yet?
            if ! drpcli machines runaction $RS_UUID getca certs/root $SERVER_CA 2>/dev/null >/dev/null ; then
              SERVER_CA_PW=$(drpcli machines runaction $RS_UUID makeroot certs/root $SERVER_CA | jq -r .)
              drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "vault/server-ca-name" to "$SERVER_CA" || true
              drpcli -T "$PROFILE_TOKEN" profiles add "$CLUSTER_PROFILE" param "vault/server-ca-pw" to "$SERVER_CA_PW" || true
            else
              if [[ $(get_param "vault/server-ca-pw") == null ]] ; then
                xiterr 1 "SERVER CA Exists, but we did not set password.  Need to reset data in certs-data profile!!"
              fi
            fi
          fi
          {{else -}}
          xiterr 1 "STAGE REQUIRES CERT PLUGIN!!  It is freely available, download from RackN SaaS."
          {{end}}
        fi



        echo "Waiting for ${VAULT_SERVER_COUNT} servers to be ready"
        wait_for_count "vault/servers" $VAULT_SERVER_COUNT

        echo "${VAULT_SERVER_COUNT} servers ready!"


        # Add the vault user if it doesn't exist
        id -u vault &>/dev/null || ( echo "Creating vault user"; useradd vault -d /var/lib/vault )

        SERVER_CA_PW=$(wait_for_variable "vault/server-ca-pw")

        # Add the vault user if it doesn't exist
        id -u vault &>/dev/null || ( echo "Creating vault user"; useradd vault -d /var/lib/vault )

        mkdir -p /etc/vault/pki/

        build_cert "server" $SERVER_CA $SERVER_CA_PW {{.Machine.Name}} $VAULT_IP


        TMP_DIR=/tmp/vault-tmp
        INSTALL_DIR=/usr/local/bin
        if [[ $OS_FAMILY == coreos ]] ; then
          INSTALL_DIR=/opt/bin
        fi

        mkdir -p ${TMP_DIR}

        echo "Download vault version: v${VAULT_VERSION}"
        # Allow for a local repository for installation files
        {{if .ParamExists "krib/package-repository" -}}
        KRIB_REPO={{.Param "krib/package-repository"}}
        {{end -}}

        if [[ ! -z "$KRIB_REPO" ]] ; then
          download -L ${KRIB_REPO}/vault_${VAULT_VERSION}_linux_amd64.zip  -o ${TMP_DIR}/vault_${VAULT_VERSION}_linux_amd64.zip
        else
          download -L https://releases.hashicorp.com/vault/${VAULT_VERSION}/vault_${VAULT_VERSION}_linux_amd64.zip -o ${TMP_DIR}/vault_${VAULT_VERSION}_linux_amd64.zip
        fi

        echo "Install vault version: ${VAULT_VERSION}"
        yum -y install unzip || echo "Unzip already installed"
        unzip -o ${TMP_DIR}/vault_${VAULT_VERSION}_linux_amd64.zip -d ${INSTALL_DIR}


        systemctl daemon-reload
        systemctl enable vault
        systemctl restart vault
        systemctl status vault

        rm -rf ${TMP_DIR}

        exit 0
      Description: ""
      Endpoint: ""
      Errors: []
      ID: vault-install.sh.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    vault-kms-plugin.service.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        ### BEGIN INIT INFO
        # Provides:          vault-kms-plugin
        # Required-Start:    $local_fs $remote_fs
        # Required-Stop:     $local_fs $remote_fs
        # Default-Start:     2 3 4 5
        # Default-Stop:      0 1 6
        # Short-Description: Kubernetes encryption KMS plugin for Vault
        # Description:       Kubernetes encryption KMS plugin for Vault
        ### END INIT INFO

        [Unit]
        Description=Kubernetes encryption KMS plugin for Vault
        Requires=network-online.target
        After=network-online.target

        [Service]
        User=vault
        Group=vault
        PIDFile=/var/run/vault/kubernetes-vault-kms-plugin.pid
        ExecStart=/usr/local/bin/kubernetes-vault-kms-plugin -vaultConfig=/etc/vault/vault-kms-plugin.yaml -socketFile=/etc/kubernetes/pki/vault-kms-plugin/vault-kms-plugin.sock
        ExecReload=/bin/kill -HUP \$MAINPID
        KillMode=process
        KillSignal=SIGTERM
        Restart=on-failure
        RestartSec=42s
        LimitMEMLOCK=infinity

        [Install]
        WantedBy=multi-user.target
      Description: ""
      Endpoint: ""
      Errors: []
      ID: vault-kms-plugin.service.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    vault-policy-transit-only.hcl.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        path "transit/*" {
          capabilities = [ "create", "read", "update", "delete", "list" ]
        }
      Description: ""
      Endpoint: ""
      Errors: []
      ID: vault-policy-transit-only.hcl.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    vault.hcl.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        listener "tcp" {
          address          = "0.0.0.0:8200"
          cluster_address  = "{{.Machine.Address}}:8201"
          tls_cert_file = "/etc/vault/pki/server.pem"
          tls_key_file  = "/etc/vault/pki/server-key.pem"
        }

        storage "consul" {
          address = "127.0.0.1:8500"
          path    = "vault/"
        }

        api_addr = "http://{{.Machine.Address}}:8200"
        cluster_addr = "https://{{.Machine.Address}}:8201"
      Description: ""
      Endpoint: ""
      Errors: []
      ID: vault.hcl.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
    vault.service.tmpl:
      Available: false
      Bundle: ""
      Contents: |-
        ### BEGIN INIT INFO
        # Provides:          vault
        # Required-Start:    $local_fs $remote_fs
        # Required-Stop:     $local_fs $remote_fs
        # Default-Start:     2 3 4 5
        # Default-Stop:      0 1 6
        # Short-Description: Vault server
        # Description:       Vault secret management tool
        ### END INIT INFO

        [Unit]
        Description=Vault secret management tool
        Requires=network-online.target
        After=network-online.target

        [Service]
        User=vault
        Group=vault
        PIDFile=/var/run/vault/vault.pid
        ExecStart=/usr/local/bin/vault server -config=/etc/vault/vault.hcl -log-level=debug
        ExecReload=/bin/kill -HUP \$MAINPID
        KillMode=process
        KillSignal=SIGTERM
        Restart=on-failure
        RestartSec=42s
        LimitMEMLOCK=infinity

        [Install]
        WantedBy=multi-user.target
      Description: ""
      Endpoint: ""
      Errors: []
      ID: vault.service.tmpl
      Meta: {}
      ReadOnly: false
      Validated: false
  workflows:
    k3s-cluster:
      Available: false
      Bundle: ""
      Description: KRIB built K3s cluster
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        copyright: RackN 2019
        icon: ship
        k3s: "true"
        title: k3s cluster
      Name: k3s-cluster
      ReadOnly: false
      Stages:
      - k3s-config
      - krib-live-wait
      Validated: false
    krib-install-cluster:
      Available: false
      Bundle: ""
      Description: KRIB built kubernetes install-to-disk demo cluster
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: kubernetes install-to-disk demo cluster
      Name: krib-install-cluster
      ReadOnly: false
      Stages:
      - centos-7-install
      - runner-service
      - finish-install
      - krib-runtime-install
      - kubernetes-install
      - etcd-config
      - krib-config
      - krib-helm
      - krib-install-complete
      Validated: false
    krib-live-cluster:
      Available: false
      Bundle: ""
      Description: KRIB built kubernetes live boot demo cluster
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: kubernetes live boot demo cluster
      Name: krib-live-cluster
      ReadOnly: false
      Stages:
      - ssh-access
      - mount-local-disks
      - krib-runtime-install
      - kubernetes-install
      - etcd-config
      - krib-config
      - krib-helm
      - krib-live-wait
      Validated: false
    krib-operate-cordon:
      Available: false
      Bundle: ""
      Description: Run 'kubectl' operate to Cordin KRIB built Kubernetes node.
      Documentation: ""
      Endpoint: ""
      Errors: null
      Meta:
        color: yellow
        icon: ship
        title: krib operate cordon
      Name: krib-operate-cordon
      ReadOnly: false
      Stages:
      - krib-operate-cordon
      Validated: false
    krib-operate-delete:
      Available: false
      Bundle: ""
      Description: Run 'kubectl' operate to Delete KRIB built Kubernetes node. (DESTROYS
        NODE!!)
      Documentation: ""
      Endpoint: ""
      Errors: null
      Meta:
        color: yellow
        icon: ship
        title: krib operate delete
      Name: krib-operate-delete
      ReadOnly: false
      Stages:
      - krib-operate-delete
      Validated: false
    krib-operate-drain:
      Available: false
      Bundle: ""
      Description: Run 'kubectl' operate to Drain KRIB built Kubernetes node.
      Documentation: ""
      Endpoint: ""
      Errors: null
      Meta:
        color: yellow
        icon: ship
        title: krib operate drain
      Name: krib-operate-drain
      ReadOnly: false
      Stages:
      - krib-operate-drain
      Validated: false
    krib-operate-uncordon:
      Available: false
      Bundle: ""
      Description: Run 'kubectl' operate to Uncordon on KRIB built Kubernetes node.
      Documentation: ""
      Endpoint: ""
      Errors: null
      Meta:
        color: yellow
        icon: ship
        title: krib operate uncordon
      Name: krib-operate-uncordon
      ReadOnly: false
      Stages:
      - krib-operate-uncordon
      Validated: false
    krib-reset-cluster:
      Available: false
      Bundle: ""
      Description: KRIB Developer Reset Cluster Profile
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: red
        icon: ship
        title: kubernetes reset cluster
      Name: krib-reset-cluster
      ReadOnly: false
      Stages:
      - krib-dev-reset
      - krib-live-wait
      Validated: false
    krib-reset-cluster-hard:
      Available: false
      Bundle: ""
      Description: KRIB Developer HARD Reset Cluster Profile
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: red
        icon: ship
        title: kubernetes reset cluster hard (flush out errors on a regular reset)
      Name: krib-reset-cluster-hard
      ReadOnly: false
      Stages:
      - krib-dev-hard-reset
      - krib-live-wait
      Validated: false
    krib-soft-install-cluster:
      Available: false
      Bundle: ""
      Description: Install KRIB cluster on existing machine OS (post-reset)
      Documentation: ""
      Endpoint: ""
      Errors: []
      Meta:
        color: yellow
        icon: ship
        title: kubernetes install on existing machine OS
      Name: krib-soft-install-cluster
      ReadOnly: false
      Stages:
      - krib-runtime-install
      - kubernetes-install
      - etcd-config
      - krib-config
      - krib-helm
      - krib-install-complete
      Validated: false
